
@article{oliver_workflow_2019,
	title = {Workflow {Automation} for {Cycling} {Systems}: {The} {Cylc} {Workflow} {Engine}},
	issn = {1521-9615},
	shorttitle = {Workflow {Automation} for {Cycling} {Systems}},
	doi = {10.1109/MCSE.2019.2906593},
	abstract = {Complex cycling workflows are fundamental to Numerical Weather Prediction (NWP) and related environmental forecasting systems. Large numbers of jobs are executed at regular intervals to process new data and generate new forecasts. Dependence between these forecast cycles creates a single never-ending workflow, but NWP workflow schedulers have traditionally ignored this-at the cost of efficiency when running "off the clock"-by enforcing a simpler non-overlapping sequence of single-cycle workflows. Cylc ("Silk") (1)(2)(3) is designed to manage infinite cycling workflows efficiently even after delays in real-time operation, or in historical runs, when cycles can typically interleave for much-increased throughput. Cylc is not actually specialized to environmental forecasting, however, and cycling workflows may also be useful in other contexts. In this article we describe the origins and major features of Cylc, future plans for the project, and our experience of Open Source development and community engagement.},
	journal = {Computing in Science Engineering},
	author = {Oliver, H. and Shin, M. and Matthews, D. and Sanders, O. and Bartholomew, S. and Clark, A. and Fitzpatrick, B. and Haren, R. v and Hut, R. and Drost, N.},
	year = {2019},
	note = {00000},
	keywords = {Computational modeling, Delays, Predictive models, Real-time systems, Task analysis, Weather forecasting},
	pages = {1--1},
}

@article{de_la_garza_desktop_2016,
	title = {From the desktop to the grid: scalable bioinformatics via workflow conversion},
	volume = {17},
	issn = {1471-2105},
	shorttitle = {From the desktop to the grid},
	url = {https://doi.org/10.1186/s12859-016-0978-9},
	doi = {10.1186/s12859-016-0978-9},
	abstract = {Reproducibility is one of the tenets of the scientific method. Scientific experiments often comprise complex data flows, selection of adequate parameters, and analysis and visualization of intermediate and end results. Breaking down the complexity of such experiments into the joint collaboration of small, repeatable, well defined tasks, each with well defined inputs, parameters, and outputs, offers the immediate benefit of identifying bottlenecks, pinpoint sections which could benefit from parallelization, among others. Workflows rest upon the notion of splitting complex work into the joint effort of several manageable tasks.},
	number = {1},
	urldate = {2020-11-18},
	journal = {BMC Bioinformatics},
	author = {de la Garza, Luis and Veit, Johannes and Szolek, Andras and Röttig, Marc and Aiche, Stephan and Gesing, Sandra and Reinert, Knut and Kohlbacher, Oliver},
	month = mar,
	year = {2016},
	pages = {127},
}

@article{lau_cancer_2017,
	title = {The {Cancer} {Genomics} {Cloud}: {Collaborative}, {Reproducible}, and {Democratized}—{A} {New} {Paradigm} in {Large}-{Scale} {Computational} {Research}},
	volume = {77},
	issn = {0008-5472},
	url = {http://dx.doi.org/10.1158/0008-5472.can-17-0387},
	doi = {10.1158/0008-5472.can-17-0387},
	abstract = {The Seven Bridges Cancer Genomics Cloud (CGC; www.cancergenomicscloud.org) enables researchers to rapidly access and collaborate on massive public cancer genomic datasets, including The Cancer Genome Atlas. It provides secure on- demand access to data, analysis tools, and computing resources. Researchers from diverse backgrounds can easily visualize, query, and explore cancer genomic datasets visually or programmatically. Data of interest can be immediately analyzed in the cloud using more than 200 preinstalled, curated bioinformatics tools and workflows. Researchers can also extend the functionality of the platform by adding their own data and tools via an intuitive software development kit. By colocalizing these resources in the cloud, the CGC enables scalable, reproducible analyses. Researchers worldwide can use the CGC to investigate key questions in cancer genomics.},
	number = {21},
	journal = {Cancer Research},
	author = {Lau, Jessica W. and Lehnert, Erik and Sethi, Anurag and Malhotra, Raunaq and Kaushik, Gaurav and Onder, Zeynep and Groves-Kirkby, Nick and Mihajlovic, Aleksandar and DiGiovanna, Jack and Srdic, Mladen and Bajcic, Dragan and Radenkovic, Jelena and Mladenovic, Vladimir and Krstanovic, Damir and Arsenijevic, Vladan and Klisic, Djordje and Mitrovic, Milan and Bogicevic, Igor and Kural, Deniz and Davis-Dusenbery, Brandi},
	month = oct,
	year = {2017},
	pages = {e3--e6},
}

@techreport{noauthor_ieee_2020,
	title = {{IEEE} {Standard} for {Bioinformatics} {Analyses} {Generated} by {High}-{Throughput} {Sequencing} ({HTS}) to {Facilitate} {Communication}},
	url = {https://doi.org/10.1109/IEEESTD.2020.9094416},
	abstract = {A major goal of this standard is to improve communication of bioinformatics protocols and data in order to facilitate bioinformatics workflow related exchange and communication between regulatory agencies, pharmaceutical companies, bioinformatics platform providers and researchers. A detailed communication helps ensure responsibility, reproducibility, verify bioinformatics protocol, track provenance information and promote interoperability. In addition, this standard also defines the assurance program for evaluating and certifying products against those requirements.},
	number = {2791-2020},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Std 2791-2020},
	keywords = {Bioinformatics, HTS, IEEE 2791, IEEE Standards, IEEE standard, IEEE standards, MPS, NGS, Next generation networking, Sequential analysis, Throughput, analysis, bioinformatics, bioinformatics platform providers, bioinformatics protocol, detailed communication, genomics, high-throughput sequencing, interoperability, massively parallel sequencing, next generation sequencing, open systems, pharmaceutical companies, pharmaceutical industry, pipeline, provenance information tracking, regulatory, regulatory agencies, researchers, workflow},
	pages = {1--16},
}

@techreport{van_wezenbeek_nationaal_2017,
	title = {Nationaal plan open science},
	url = {http://dx.doi.org/10.4233/uuid:9e9fa82e-06c1-4d0d-9e20-5620259a6c65},
	abstract = {This National Plan Open Science sets out what the Dutch parties involved in creating this Plan are already doing and what they plan to do to grasp the opportunities and at the same time make science even more accessible to others. A major boost is required if these initiatives are to be coordinated and the great ambition realised. That is why this Plan lists the ambitions and provides details of the parties intending to take action, as well as the timeframes within which they believe they can realise their objectives. The key ambitions are: (1) Full open access to publications in 2020: Continue the Dutch approach for all Dutch research organisations and research areas whilst recognising their differences and similarities; (2) To make research data optimally suited for reuse: To set clear and agreed technical and policy-related preconditions to facilitate reuse of research data, including provision of the necessary expertise and support; (3) Recognition and rewards: To examine together how open science can be an element of the evaluation and reward system for researchers, research groups and research proposals; and (4) To promote and support: To establish a 'clearing house' for all information regarding all available research support. With the ambitions set out in this plan the Netherlands is responding to the Amsterdam Call for Action on Open Science published in 2016, the conclusions of the Competitiveness Council in May 2016, and to the in the letter to Parliament concerning open science confirmed question by the State Secretary for Education, Culture and Science (January 2017). Open access to publications and optimal reuse of research data are becoming the standard for all knowledge institutes and research areas. The motto here is as open as possible, as closed as necessary.},
	institution = {Ministerie van Onderwijs, Cultuur en Wetenschap},
	author = {van Wezenbeek, W. J. S. M. and Touwen, H. J. J. and Versteeg, A. M. C. and van Wesenbeeck, A. J. M.},
	month = feb,
	year = {2017},
	doi = {10.4233/uuid:9e9fa82e-06c1-4d0d-9e20-5620259a6c65},
}

@article{lee_rrnaselector_2011,
	title = {{rRNASelector}: {A} computer program for selecting ribosomal {RNA} encoding sequences from metagenomic and metatranscriptomic shotgun libraries},
	volume = {49},
	issn = {1976-3794},
	shorttitle = {{rRNASelector}},
	url = {https://doi.org/10.1007/s12275-011-1213-z},
	doi = {10.1007/s12275-011-1213-z},
	abstract = {Metagenomic and metatranscriptomic shotgun sequencing techniques are gaining popularity as more cost-effective next-generation sequencing technologies become commercially available. The initial stage of bioinfor-matic analysis generally involves the identification of phylogenetic markers such as ribosomal RNA genes. The sequencing reads that do not code for rRNA can then be used for protein-based analysis. Hidden Markov model is a well-known method for pattern recognition. Hidden Markov models that are trained on well-curated rRNA sequence databases have been successfully used to identify DNA sequence coding for rRNAs in pro-karyotes. Here, we introduce rRNASelector, which is a computer program for selecting rRNA genes from massive metagenomic and metatranscriptomic sequences using hidden Markov models. The program successfully identified prokaryotic 5S, 26S, and 23S rRNA genes from Roche 454 FLX Titanium-based metagenomic and metatranscriptomic libraries. The rRNASelector program is available at http://sw.ezbiocloud.net/rrnaselector.},
	language = {en},
	number = {4},
	urldate = {2021-07-24},
	journal = {The Journal of Microbiology},
	author = {Lee, Jae-Hak and Yi, Hana and Chun, Jongsik},
	month = sep,
	year = {2011},
	pages = {689},
}

@misc{guarracino_covid-19_2020,
	title = {{COVID}-19 {PubSeq}: {Public} {SARS}-{CoV}-2 {Sequence} {Resource}},
	url = {https://bcc2020.sched.com/event/coLw/covid-19-pubseq-public-sars-cov-2-sequence-resource},
	abstract = {As part of the COVID-19 Virtual Biohackathon 2020 we formed a working group to create a
COVID-19 Public Sequence Resource (COVID-19 PubSeq) for SARS-CoV-2 virus sequences. Our goal
was to create a repository that had a low barrier to entry for uploading and analyzing sequence
data. We followed FAIR data practices: data are published with public domain (CC0) or creative
commons 4.0 (CC-BY-4.0) license, structured metadata is validated against standard ontologies,
and, most importantly, reproducible workflows are executed after the upload in order to provide
up-to-date results rapidly and in standardized data formats.

Existing data repositories for viral data include GISAID, EBI ENA and NCBI. These repositories allow
for free sharing data, but do not enforce strict quality control on submitted data or metadata, and
do not add value in terms of running additional analysis. In addition, some databases have a
restricted license which prevents data from being used in online web services and on-the-fly
computation, hindering research.

We created a prototype sequence resource within one week by leveraging existing technologies,
such as the Arvados Cloud platform (http://arvados.org), Common Workflow Language (CWL)
(http://commonwl.org), and the many free and open source software packages that are available
for bioinformatics. Pipelines developed by several teams were combined into an omnibus
pangenome analysis workflow. Computing resources for this project were generously donated by
Amazon Web Services.},
	author = {Guarracino, Andrea and Amstutz, Peter and Liener, Thomas and Crusoe, Michael and Novak, Adam and Garrison, Erik and Ohta, Tazro and Munyoki, Bonface and Welter, Danielle and Wait Zaranek, Sarah and Wait Zaranek, Alexander (Sasha) and Prins, Pjotr},
	month = jul,
	year = {2020},
}

@techreport{kunze_bagit_2018,
	type = {{RFC}},
	title = {The {BagIt} {File} {Packaging} {Format} ({V1}.0)},
	url = {https://www.rfc-editor.org/info/rfc8493},
	number = {8493},
	institution = {RFC Editor},
	author = {Kunze, J. and Littman, J. and Madden, E. and Scancella, J. and Adams, C.},
	month = oct,
	year = {2018},
	note = {ISSN: 2070-1721},
}

@article{gryk_workflows_2017,
	title = {Workflows and {Provenance}: {Toward} {Information} {Science} {Solutions} for the {Natural} {Sciences}},
	volume = {65},
	issn = {0024-2594},
	shorttitle = {Workflows and {Provenance}},
	url = {https://muse.jhu.edu/article/669457},
	doi = {10.1353/lib.2017.0018},
	abstract = {The era of big data and ubiquitous computation has brought with it concerns about ensuring reproducibility in this new research environment. It is easy to assume computational methods self-document by their very nature of being exact, deterministic processes. However, similar to laboratory experiments, ensuring reproducibility in the computational realm requires the documentation of both the protocols used (workflows) as well as a detailed description of the computational environment: algorithms, implementations, software environments as well as the data ingested and execution logs of the computation. These two aspects of computational reproducibility (workflows and execution details) are discussed in the context of biomolecular Nuclear Magnetic Resonance spectroscopy (bioNMR) as well as the PRIMAD model for computational reproducibility.},
	number = {4},
	urldate = {2021-04-16},
	journal = {Library trends},
	author = {Gryk, Michael R. and Ludäscher, Bertram},
	year = {2017},
	pmid = {29375158},
	pmcid = {PMC5779102},
	pages = {555--562},
}

@article{cuevas-vicenttin_scientific_2012,
	title = {Scientific {Workflows} and {Provenance}: {Introduction} and {Research} {Opportunities}},
	volume = {12},
	issn = {1610-1995},
	shorttitle = {Scientific {Workflows} and {Provenance}},
	url = {https://doi.org/10.1007/s13222-012-0100-z},
	doi = {10.1007/s13222-012-0100-z},
	abstract = {Scientific workflows are becoming increasingly popular for compute-intensive and data-intensive scientific applications. The vision and promise of scientific workflows includes rapid, easy workflow design, reuse, scalable execution, and other advantages, e.g., to facilitate “reproducible science” through provenance (e.g., data lineage) support. However, as described in the paper, important research challenges remain. While the database community has studied (business) workflow technologies extensively in the past, most current work in scientific workflows seems to be done outside of the database community, e.g., by practitioners and researchers in the computational sciences and eScience. We provide a brief introduction to scientific workflows and provenance, and identify areas and problems that suggest new opportunities for database research.},
	language = {en},
	number = {3},
	urldate = {2020-07-03},
	journal = {Datenbank-Spektrum},
	author = {Cuevas-Vicenttín, Víctor and Dey, Saumen and Köhler, Sven and Riddle, Sean and Ludäscher, Bertram},
	month = nov,
	year = {2012},
	pages = {193--203},
}

@article{crusoe_walking_2015,
	title = {Walking the talk: adopting and adapting sustainable scientiﬁc software development processes in a small biology lab},
	url = {http://doi.org/10.6084/m9.figshare.791567},
	doi = {10.6084/m9.figshare.791567},
	author = {Crusoe, Michael R.},
	year = {2015},
}

@article{crusoe_walking_2016,
	title = {Walking the {Talk}: {Adopting} and {Adapting} {Sustainable} {Scientific} {Software} {Development} processes in a {Small} {Biology} {Lab}},
	volume = {4},
	url = {http://dx.doi.org/10.5334/jors.35},
	doi = {10.5334/jors.35},
	journal = {Journal of Open Research Software},
	author = {Crusoe, Michael R. and Brown, C. Titus},
	month = nov,
	year = {2016},
}

@techreport{jiang_tr-19-01_2019,
	type = {Technical {Report}},
	title = {{TR}-19-01: {A} {Cloud}-{Agnostic} {Framework} for {Geo}-{Distributed} {Data}-{Intensive} {Applications}},
	url = {https://renci.org/technical-reports/tr-19-01/},
	abstract = {As the demand for Cloud computing trends up, valuable datasets are stored in the Cloud across various geographical regions and Cloud platforms and providers. The distribution of data across cloud providers imposes three major challenges for data-driven analysis and applications: the heterogeneity of cloud resources across clouds; low network throughput over the widearea network; and high monetary cost resulting from moving data in/out cloud regions. In this work we propose a cloudagnostic framework named PIVOT that builds on open-source technologies and abstraction principles to create the illusion of one single computer for applications and users. We have deployed a prototype across AWS and GCP and investigated its effectiveness against synthetic workloads. Using a combination of advanced middleware techniques and data-locality and cost aware scheduling strategies we show that PIVOT is able to achieve up to 4x improvement in network throughput and reduce {\textgreater} 60\% monetary cost.},
	language = {en},
	number = {TR-19-01},
	institution = {RENCI, University of North Carolina at Chapel Hill},
	author = {Jiang, Fan and Castillo, Claris and Ahalt, Stan},
	month = feb,
	year = {2019},
	pages = {10},
}

@techreport{jiang_pivot_2019,
	type = {Technical {Report}},
	title = {{PIVOT}: {Cost}-{Aware} {Scheduling} of {Data}-{Intensive} {Applications} in a {Cloud}-{Agnostic} {System}},
	url = {https://renci.org/technical-reports/tr-19-02/},
	abstract = {We have witnessed a surge in big data applications being hosted by assorted cloud vendors, and the astronomical amount of data they produce and consume on a daily basis. Traditional cluster computing frameworks can hardly cope with the unprecedented data volume and the geo-distributed, cross-cloud data distribution due to their limited scalability and adaptability across the heterogeneous clouds. Moreover, running data-intensive applications across clouds at will is extremely cost-inefﬁcient and likely to incur outrageous expenses. Hence, we introduce our cloud-agnostic system PIVOT with the novel cost-aware scheduling algorithm, which enables dataintensive applications to run and scale across clouds instantly in a cost-efﬁcient manner. We evaluate our system and scheduling algorithm extensively with simulation, and real-world big data applications on a deployment across 11 regions on AWS and GCP. The experimental results show that PIVOT achieves over 55\% saving in expense for VM subscription and up to 92\% for egress network trafﬁc compared to the state-of-the-art baselines. Notably, the cost-aware scheduling also achieves up to a 10x speedup in data transfers for data-intensive applications.},
	language = {en},
	number = {TR-19-02},
	institution = {RENCI, University of North Carolina at Chapel Hill},
	author = {Jiang, Fan and Ferriter, Kyle and Castillo, Claris},
	month = feb,
	year = {2019},
	pages = {8},
}

@techreport{bell_web-based_2017,
	address = {Geneva, Switzerland},
	title = {Web-based {Analysis} {Services} {Report}},
	url = {http://cds.cern.ch/record/2315331/},
	abstract = {Web-based services (cloud services) is an important trend to innovate end-user services while optimising the service operational costs. CERN users are constantly proposing new approaches (inspired from services existing on the web, tools used in education or other science or based on their experience in using existing computing services). In addition, industry and open source communities have recently made available a large number of powerful and attractive tools and platforms that enable large scale data processing. “Big Data” software stacks notably provide solutions for scalable storage, distributed compute and data analysis engines, data streaming, web-based interfaces (notebooks). Some of those platforms and tools, typically available as open source products, are experiencing a very fast adoption in industry and science such that they are becoming “de facto” references in several areas of data engineering, data science and machine learning. In parallel to users' requests, WLCG is considering to consolidate its deployment model into a relatively reduced number of resource sites (while benefiting from potentially ephemeral resources like public cloud and large installations like experiment filter farms, HPC facilities, analysis centres). In this schema, analysis facilities could be provided as web-based services located in strategic WLCG centres or spawned on relatively short-lived farms},
	language = {eng},
	number = {CERN-IT-Note-2018-004},
	institution = {CERN},
	author = {Bell, Tim and Canali, Luca and Grancher, Eric and Lamanna, Massimo and McCance, Gavin and Mato Vila, Pere and Piparo, Danilo and Moscicki, Jakub and Pace, Alberto and Brito Da Rocha, Ricardo and Simko, Tibor and Smith, Tim and Tejedor Saavedra, Enric},
	collaborator = {CERN. Geneva. IT Department},
	month = nov,
	year = {2017},
	keywords = {CERNBOX, CVMFS, Computing and Computers, Particle Physics - Experiment, REANA, ROOT, SWAN},
}

@techreport{the_austin_group_posix1-2008_2008,
	title = {{POSIX}.1-2008 ({IEEE} {Std} 1003.1™-2008 and {The} {Open} {Group} {Technical} {Standard} {Base} {Specifications}, {Issue} 7)},
	url = {https://pubs.opengroup.org/onlinepubs/9699919799.2008edition/},
	institution = {Austin Group},
	author = {The Austin Group},
	year = {2008},
}

@incollection{hutchison_taverna_2010,
	address = {Berlin, Heidelberg},
	title = {Taverna, {Reloaded}},
	volume = {6187},
	isbn = {978-3-642-13817-1 978-3-642-13818-8},
	url = {http://link.springer.com/10.1007/978-3-642-13818-8_33},
	abstract = {The Taverna workﬂow management system is an open source project with a history of widespread adoption within multiple experimental science communities, and a longterm ambition of eﬀectively supporting the evolving need of those communities for complex, data-intensive, service-based experimental pipelines. This paper describes how the recently overhauled technical architecture of Taverna addresses issues of eﬃciency, scalability, and extensibility, and presents performance results based on a collection of synthetic workﬂows, as well as a concrete case study involving a production workﬂow in the area of cancer research.},
	language = {en},
	urldate = {2020-01-21},
	booktitle = {Scientific and {Statistical} {Database} {Management}},
	publisher = {Springer Berlin Heidelberg},
	author = {Missier, Paolo and Soiland-Reyes, Stian and Owen, Stuart and Tan, Wei and Nenadic, Alexandra and Dunlop, Ian and Williams, Alan and Oinn, Tom and Goble, Carole},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gertz, Michael and Ludäscher, Bertram},
	year = {2010},
	doi = {10.1007/978-3-642-13818-8_33},
	keywords = {Concurrent Thread, Execution Model, Execution Time, Input Port, Memory Usage},
	pages = {471--481},
}

@article{brandies_ten_2021,
	title = {Ten simple rules for getting started with command-line bioinformatics},
	volume = {17},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008645},
	doi = {10.1371/journal.pcbi.1008645},
	language = {en},
	number = {2},
	urldate = {2021-03-02},
	journal = {PLOS Computational Biology},
	author = {Brandies, Parice A. and Hogg, Carolyn J.},
	month = feb,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Bioinformatics, Cloud computing, Computational pipelines, Computer software, Operating systems, Programming languages, Software tools, Source code},
	pages = {e1008645},
}

@article{mitchell_mgnify_2020,
	title = {{MGnify}: the microbiome analysis resource in 2020},
	volume = {48},
	issn = {0305-1048},
	shorttitle = {{MGnify}},
	url = {https://doi.org/10.1093/nar/gkz1035},
	doi = {10.1093/nar/gkz1035},
	abstract = {MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.},
	number = {D1},
	urldate = {2021-04-16},
	journal = {Nucleic Acids Research},
	author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},
	month = jan,
	year = {2020},
	pages = {D570--D578},
}

@article{seemann_ten_2013,
	title = {Ten recommendations for creating usable bioinformatics command line software},
	volume = {2},
	issn = {2047-217X},
	url = {https://doi.org/10.1186/2047-217X-2-15},
	doi = {10.1186/2047-217X-2-15},
	abstract = {Bioinformatics software varies greatly in quality. In terms of usability, the command line interface is the first experience a user will have of a tool. Unfortunately, this is often also the last time a tool will be used. Here I present ten recommendations for command line software author’s tools to follow, which I believe would greatly improve the uptake and usability of their products, waste less user’s time, and improve the quality of scientific analyses.},
	number = {2047-217X-2-15},
	urldate = {2021-03-02},
	journal = {GigaScience},
	author = {Seemann, Torsten},
	month = dec,
	year = {2013},
}

@inproceedings{jiang_cloud-agnostic_2020,
	title = {A {Cloud}-{Agnostic} {Framework} to {Enable} {Cost}-{Aware} {Scheduling} of {Applications} in a {Multi}-{Cloud} {Environment}},
	doi = {10.1109/NOMS47738.2020.9110325},
	abstract = {We have witnessed a surge in both the big data applications being hosted by an assortment of cloud vendors, and in the astronomical amount of data they produce and consume on a daily basis. Traditional cluster computing frameworks can hardly cope with the unprecedented data volume and the geo-distributed, cross-cloud data distribution due to their limited scalability and adaptability across the heterogeneous clouds. Moreover, running data-intensive applications across clouds at will is extremely cost-inefficient and likely to incur outrageous expenses. Hence, we introduce our cloud-agnostic system PIVOT with the novel cost-aware scheduling algorithm, which enables data-intensive applications to run and scale across clouds instantly in a cost-efficient manner. We evaluate our system and scheduling algorithm extensively with the Alibaba production cluster trace, as well as real-world big data applications on a 100-node deployment across 11 regions (31 availability zones) on AWS and GCP. The experimental results show that PIVOT achieves up to 90.8\% saving in expense for VM subscription and 99.2\% for egress network traffic compared to the state-of-the-art baselines. Notably, the cost-aware scheduling also achieves over 4x speedup in data transfers for data-intensive applications.},
	booktitle = {{NOMS} 2020 - 2020 {IEEE}/{IFIP} {Network} {Operations} and {Management} {Symposium}},
	author = {Jiang, Fan and Ferriter, Kyle and Castillo, Claris},
	month = apr,
	year = {2020},
	note = {ISSN: 2374-9709},
	pages = {1--9},
}

@incollection{couvares_workflow_2007,
	address = {London},
	title = {Workflow {Management} in {Condor}},
	isbn = {978-1-84628-757-2},
	url = {https://doi.org/10.1007/978-1-84628-757-2_22},
	abstract = {The Condor project began in 1988 and has evolved into a feature-rich batch system that targets high-throughput computing; that is, Condor ([262], [414]) focuses on providing reliable access to computing over long periods of time instead of highly tuned, high-performance computing for short periods of time or a small number of applications.},
	language = {en},
	urldate = {2021-03-02},
	booktitle = {Workflows for e-{Science}: {Scientific} {Workflows} for {Grids}},
	publisher = {Springer},
	author = {Couvares, Peter and Kosar, Tevfik and Roy, Alain and Weber, Jeff and Wenger, Kent},
	editor = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
	year = {2007},
	doi = {10.1007/978-1-84628-757-2_22},
	keywords = {Basic Local Alignment Search Tool, Batch System, Data Placement, Grid Environment, State Diagram},
	pages = {357--375},
}

@article{deelman_pegasus_2015,
	title = {Pegasus, a workflow management system for science automation},
	volume = {46},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X14002015},
	doi = {10.1016/j.future.2014.10.008},
	abstract = {Modern science often requires the execution of large-scale, multi-stage simulation and data analysis pipelines to enable the study of complex systems. The amount of computation and data involved in these pipelines requires scalable workflow management systems that are able to reliably and efficiently coordinate and automate data movement and task execution on distributed computational resources: campus clusters, national cyberinfrastructures, and commercial and academic clouds. This paper describes the design, development and evolution of the Pegasus Workflow Management System, which maps abstract workflow descriptions onto distributed computing infrastructures. Pegasus has been used for more than twelve years by scientists in a wide variety of domains, including astronomy, seismology, bioinformatics, physics and others. This paper provides an integrated view of the Pegasus system, showing its capabilities that have been developed over time in response to application needs and to the evolution of the scientific computing platforms. The paper describes how Pegasus achieves reliable, scalable workflow execution across a wide variety of computing infrastructures.},
	language = {en},
	urldate = {2021-03-02},
	journal = {Future Generation Computer Systems},
	author = {Deelman, Ewa and Vahi, Karan and Juve, Gideon and Rynge, Mats and Callaghan, Scott and Maechling, Philip J. and Mayani, Rajiv and Chen, Weiwei and Ferreira da Silva, Rafael and Livny, Miron and Wenger, Kent},
	month = may,
	year = {2015},
	keywords = {Pegasus, Scientific workflows, Workflow management system},
	pages = {17--35},
}

@article{perkel_workflow_2019,
	title = {Workflow systems turn raw data into scientific knowledge},
	volume = {573},
	copyright = {2019 Nature},
	url = {http://www.nature.com/articles/d41586-019-02619-z},
	doi = {10.1038/d41586-019-02619-z},
	abstract = {How workflow tools can make your computational methods portable, maintainable, reproducible and shareable.},
	language = {en},
	urldate = {2019-09-03},
	journal = {Nature},
	author = {Perkel, Jeffrey M.},
	month = sep,
	year = {2019},
	pages = {149--150},
}

@article{afgan_galaxy_2018,
	title = {The {Galaxy} platform for accessible, reproducible and collaborative biomedical analyses: 2018 update},
	volume = {46},
	issn = {0305-1048},
	shorttitle = {The {Galaxy} platform for accessible, reproducible and collaborative biomedical analyses},
	url = {https://doi.org/10.1093/nar/gky379},
	doi = {10.1093/nar/gky379},
	abstract = {Galaxy (homepage: https://galaxyproject.org, main public server: https://usegalaxy.org) is a web-based scientific analysis platform used by tens of thousands of scientists across the world to analyze large biomedical datasets such as those found in genomics, proteomics, metabolomics and imaging. Started in 2005, Galaxy continues to focus on three key challenges of data-driven biomedical science: making analyses accessible to all researchers, ensuring analyses are completely reproducible, and making it simple to communicate analyses so that they can be reused and extended. During the last two years, the Galaxy team and the open-source community around Galaxy have made substantial improvements to Galaxy's core framework, user interface, tools, and training materials. Framework and user interface improvements now enable Galaxy to be used for analyzing tens of thousands of datasets, and \&gt;5500 tools are now available from the Galaxy ToolShed. The Galaxy community has led an effort to create numerous high-quality tutorials focused on common types of genomic analyses. The Galaxy developer and user communities continue to grow and be integral to Galaxy's development. The number of Galaxy public servers, developers contributing to the Galaxy framework and its tools, and users of the main Galaxy server have all increased substantially.},
	number = {W1},
	urldate = {2021-03-02},
	journal = {Nucleic Acids Research},
	author = {Afgan, Enis and Baker, Dannon and Batut, Bérénice and van den Beek, Marius and Bouvier, Dave and Čech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Grüning, Björn A and Guerler, Aysam and Hillman-Jackson, Jennifer and Hiltemann, Saskia and Jalili, Vahid and Rasche, Helena and Soranzo, Nicola and Goecks, Jeremy and Taylor, James and Nekrutenko, Anton and Blankenberg, Daniel},
	month = jul,
	year = {2018},
	pages = {W537--W544},
}

@techreport{goncalves_ogc_2020,
	type = {{OGC} {Public} {Engineering} {Report}},
	title = {{OGC} {Earth} {Observations} {Applications} {Pilot}: {Terradue} {Engineering} {Report}},
	url = {http://docs.opengeospatial.org/per/20-042.html},
	abstract = {The availability of a growing volume of environmental data from space represents a unique opportunity for science, general R\&D, and applications (apps), but it also poses a major challenge to achieve its full potential in terms of data exploitation. Firstly, because the emergence of large volumes of data (Petabytes era) raises new issues in terms of discovery, access, exploitation, and visualization of “Big Data”, with profound implications on how users do “data-intensive” Earth Science. Secondly, because the inherent growing diversity and complexity of data and users, whereby different communities – having different needs, skills, methods, languages and protocols – need to cooperate to make sense of a wealth of data of different nature (e.g. EO, in-situ, model), structure and format.

Responding to these technological and community challenges requires the development of new ways of working, capitalizing on Information and Communication Technology (ICT) developments to facilitate the exploitation, analysis, sharing, mining and visualization of massive EO data sets and high-level products within Europe and beyond. Evolution in information technology and the consequent shifts in user behavior and expectations provide new opportunities to provide more significant support to EO data exploitation.

Earth Observation Platforms provide customers with an environment allowing them to focus on their core business and outsource other aspects to a platform that supplies services to a large number of customers with similar needs. The success of platforms in the business world is based on their ability to minimize cost and time to market for their customers, thereby also reducing the risk of exploring unproven business cases. In recent years, Platforms for the Exploitation of Earth Observation data have been developed by public and private companies in order to foster the usage of EO data and expand the market of Earth Observation-derived information. The domain is composed of platform providers, service providers who use the platform to deliver a service to their users, and data providers. The availability of free and open data (e.g. Copernicus Sentinel), together with the availability of affordable computing resources, creates an opportunity for the wide adoption and use of Earth Observation data in a growing number of fields in our society.

OGC activities in Testbed-13, Testbed-14, and Testbed-15 initiated the development of an architecture to allow the ad-hoc deployment and execution of applications close to the physical location of the source data with the goal to minimize data transfer between data repositories and application processes.

The activity described in this Engineering Report responds to the invitation for Earth observation platform operators to implement the OGC Earth Observation Applications Pilot architecture as it has been defined in those previous OGC Innovation Program (IP) initiatives. The goal of the pilot is to evaluate the maturity of those specifications in a real-world environment with several Earth Observation applications brought by several application developers that work with Earth observation satellites. These developers brought different views and requirements in terms of data discovery, data loading, data processing, and result delivery to which the platform readiness is challenged and evolutions are proposed to the architecture.

This Engineering Report initiates by introducing the Earth Observation (EO) Platform architecture and documents the encoding and interfaces needed for defining, deployment and execution of EO Applications brought by the different application developers. The ER concludes with a summary of the main challenges found during the pilot activities and provides further recommendations to advance the architecture, integration and implementation strategies taking in consideration the viewpoints of both EO platforms and EO application developers.},
	number = {OGC 20-042},
	urldate = {2020-11-18},
	institution = {Open Geospatial Consortium},
	author = {Gonçalves, Pedro},
	month = oct,
	year = {2020},
}

@techreport{simonis_ogc_2020,
	type = {{OGC} {Public} {Engineering} {Report}},
	title = {{OGC} {Earth} {Observation} {Applications} {Pilot}: {Summary} {Engineering} {Report}},
	url = {https://docs.ogc.org/per/20-073.html},
	abstract = {This Engineering Report (ER) summarizes the main achievements of the OGC Innovation Program initiative OGC Earth Observation Applications Pilot, conducted between December 2019 and July 2020. The pilot explored an Earth Observation Application software architecture that was developed in OGC Testbeds 13-15. The architecture allows the deployment and execution of externally developed applications on Earth Observation (EO) data and processing platforms. The architecture is essentially based on three major components:

    Execution Management Service (EMS): This component provides a RESTful interface (defined using OpenAPI) to register applications and build workflows from registered applications. The EMS selects the appropriate ADES platform to execute the processes based on the runtime input parameters (close to the data).

    Application Deployment and Execution Service (ADES): This component allows deployment, discovery, and execution of applications or processing of quoting requests.

    Applications are delivered in the form of Docker images along with corresponding metadata called an Application Package (AP). The application package provides all information for deployment and execution of an application.

The pilot demonstrated that the interoperability arrangements developed and documented in OGC Innovation Program initiatives Testbed 13, 14 and 15 provide a solid starting point for maturity tests within operational platforms. Additional arrangements and more detailed definitions have been developed during the pilot that now allow deployment and execution of an application on various platforms with minimal adaptions.

The pilot produced very valuable results. It confirmed the general approach to use Docker for application packaging and HTTP Web APIs or Web Services for application handling and execution. It defined application patterns based on data inputs/outputs, confirmed the role of the Common Workflow Language (CWL) for application description, execution, and workflow building; and recommends the usage of the SpatioTemporal Asset Catalog (STAC) as a data manifest for application inputs and outputs.},
	number = {OGC 20-073},
	urldate = {2020-11-18},
	institution = {Open Geospatial Consortium},
	author = {Simonis, Ingo},
	month = oct,
	year = {2020},
}

@techreport{landry_ogc_2020,
	type = {{OGC} {Public} {Engineering} {Report}},
	title = {{OGC} {Earth} {Observation} {Applications} {Pilot}: {CRIM} {Engineering} {Report}},
	url = {http://docs.opengeospatial.org/per/20-045.html},
	abstract = {CRIM’s key findings are as follows:

    From the application developer’s perspective, it was easier to first develop and test locally, and then use an OGC Application Programming Interface (API) to deploy and execute remotely. This is partly due to a smoother learning curve from application to packaging, than from package to platform.

    Conformance classes for an Application Deployment and Execution Service (ADES) API should be standardized. An offset in the support of API elements subsists in participants' implementations.

    Application packaging, deployment and execution are appropriately defined by combination of Common Workflow Language (CWL) and Docker images.

    More tests are required to better support application workflows running in multiple platforms. In order to build a federated cloud, these tests have to run regularly, in a structured and systemic fashion.

    Participants' findings should offer feedback into EOEPCA, for example for platforms intending to support machine learning (ML) services. From the application developer’s perspective, ML apps are well defined and should be deployable like any other app. It is not clear with the current EOEPCA use cases that the platforms will easily integrate ML services (annotations, trained models, access to GPU clusters, etc.).},
	number = {OGC 20-045},
	urldate = {2020-11-18},
	institution = {Open Geospatial Consortium},
	author = {Landry, Tom},
	month = oct,
	year = {2020},
}

@incollection{kaushik_building_2019,
	address = {New York, NY},
	series = {Methods in {Molecular} {Biology}},
	title = {Building {Portable} and {Reproducible} {Cancer} {Informatics} {Workflows}: {An} {RNA} {Sequencing} {Case} {Study}},
	volume = {1878},
	isbn = {978-1-4939-8868-6},
	shorttitle = {Building {Portable} and {Reproducible} {Cancer} {Informatics} {Workflows}},
	url = {https://doi.org/10.1007/978-1-4939-8868-6_2},
	abstract = {The Seven Bridges Cancer Genomics Cloud (CGC) is part of the National Cancer Institute Cloud Resource project, which was created to explore the paradigm of co-locating massive datasets with the computational resources to analyze them. The CGC was designed to allow researchers to easily find the data they need and analyze it with robust applications in a scalable and reproducible fashion. To enable this, individual tools are packaged within Docker containers and described by the Common Workflow Language (CWL), an emerging standard for enabling reproducible data analysis. On the CGC, researchers can deploy individual tools and customize massive workflows by chaining together tools. Here, we discuss a case study in which RNA sequencing data is analyzed with different methods and compared on the Seven Bridges CGC. We highlight best practices for designing command line tools, Docker containers, and CWL descriptions to enable massively parallelized and reproducible biomedical computation with cloud resources.},
	language = {en},
	urldate = {2020-12-06},
	booktitle = {Cancer {Bioinformatics}},
	publisher = {Springer},
	author = {Kaushik, Gaurav and Davis-Dusenbery, Brandi},
	editor = {Krasnitz, Alexander},
	year = {2019},
	doi = {10.1007/978-1-4939-8868-6_2},
	keywords = {AWS, Bioinformatics, Cancer informatics, Cloud, Docker, Reproducibility, Software design, TCGA},
	pages = {39--64},
}

@article{georgeson_bionitio_2019,
	title = {Bionitio: demonstrating and facilitating best practices for bioinformatics command-line software},
	volume = {8},
	issn = {2047-217X},
	shorttitle = {Bionitio},
	url = {https://doi.org/10.1093/gigascience/giz109},
	doi = {10.1093/gigascience/giz109},
	abstract = {Bioinformatics software tools are often created ad hoc, frequently by people without extensive training in software development. In particular, for beginners, the barrier to entry in bioinformatics software development is high, especially if they want to adopt good programming practices. Even experienced developers do not always follow best practices. This results in the proliferation of poorer-quality bioinformatics software, leading to limited scalability and inefficient use of resources; lack of reproducibility, usability, adaptability, and interoperability; and erroneous or inaccurate results.We have developed Bionitio, a tool that automates the process of starting new bioinformatics software projects following recommended best practices. With a single command, the user can create a new well-structured project in 1 of 12 programming languages. The resulting software is functional, carrying out a prototypical bioinformatics task, and thus serves as both a working example and a template for building new tools. Key features include command-line argument parsing, error handling, progress logging, defined exit status values, a test suite, a version number, standardized building and packaging, user documentation, code documentation, a standard open source software license, software revision control, and containerization.Bionitio serves as a learning aid for beginner-to-intermediate bioinformatics programmers and provides an excellent starting point for new projects. This helps developers adopt good programming practices from the beginning of a project and encourages high-quality tools to be developed more rapidly. This also benefits users because tools are more easily installed and consistent in their usage. Bionitio is released as open source software under the MIT License and is available at https://github.com/bionitio-team/bionitio.},
	number = {giz109},
	urldate = {2021-03-02},
	journal = {GigaScience},
	author = {Georgeson, Peter and Syme, Anna and Sloggett, Clare and Chung, Jessica and Dashnow, Harriet and Milton, Michael and Lonsdale, Andrew and Powell, David and Seemann, Torsten and Pope, Bernard},
	month = sep,
	year = {2019},
}

@article{taylor_overview_2010,
	title = {An overview of the {Hadoop}/{MapReduce}/{HBase} framework and its current applications in bioinformatics},
	volume = {11},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-11-S12-S1},
	doi = {10.1186/1471-2105-11-S12-S1},
	abstract = {Bioinformatics researchers are now confronted with analysis of ultra large-scale data sets, a problem that will only increase at an alarming rate in coming years. Recent developments in open source software, that is, the Hadoop project and associated software, provide a foundation for scaling to petabyte scale data warehouses on Linux clusters, providing fault-tolerant parallelized analysis on such data using a programming style named MapReduce.},
	number = {12},
	urldate = {2021-04-16},
	journal = {BMC Bioinformatics},
	author = {Taylor, Ronald C.},
	month = dec,
	year = {2010},
	keywords = {Cloud Computing, Hadoop Distribute File System, Message Passing Interface, Open Source Software Project, Pacific Northwest National Laboratory},
	pages = {S1},
}

@article{berthold_knime_2009,
	title = {{KNIME} - the {Konstanz} information miner: version 2.0 and beyond},
	volume = {11},
	issn = {1931-0145},
	shorttitle = {{KNIME} - the {Konstanz} information miner},
	url = {https://doi.org/10.1145/1656274.1656280},
	doi = {10.1145/1656274.1656280},
	abstract = {The Konstanz Information Miner is a modular environment, which enables easy visual assembly and interactive execution of a data pipeline. It is designed as a teaching, research and collaboration platform, which enables simple integration of new algorithms and tools as well as data manipulation or visualization methods in the form of new modules or nodes. In this paper we describe some of the design aspects of the underlying architecture, briey sketch how new nodes can be incorporated, and highlight some of the new features of version 2.0.},
	number = {1},
	urldate = {2021-03-02},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Berthold, Michael R. and Cebron, Nicolas and Dill, Fabian and Gabriel, Thomas R. and Kötter, Tobias and Meinl, Thorsten and Ohl, Peter and Thiel, Kilian and Wiswedel, Bernd},
	month = nov,
	year = {2009},
	pages = {26--31},
}

@article{feitelson_repeatability_2015,
	title = {From {Repeatability} to {Reproducibility} and {Corroboration}},
	volume = {49},
	issn = {0163-5980},
	url = {https://doi.org/10.1145/2723872.2723875},
	doi = {10.1145/2723872.2723875},
	abstract = {Being able to repeat experiments is considered a hallmark of the scientific method, used to confirm or refute hypotheses and previously obtained results. But this can take many forms, from precise repetition using the original experimental artifacts, to conceptual reproduction of the main experimental idea using new artifacts. Furthermore, the conclusions from previous work can also be corroborated using a different experimental methodology altogether. In order to promote a better understanding and use of such methodologies we propose precise definitions for different terms, and suggest when and why each should be used.},
	number = {1},
	urldate = {2021-01-13},
	journal = {ACM SIGOPS Operating Systems Review},
	author = {Feitelson, Dror G.},
	month = jan,
	year = {2015},
	pages = {3--11},
}

@article{ivie_reproducibility_2018,
	title = {Reproducibility in {Scientific} {Computing}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3186266},
	doi = {10.1145/3186266},
	abstract = {Reproducibility is widely considered to be an essential requirement of the scientific process. However, a number of serious concerns have been raised recently, questioning whether today’s computational work is adequately reproducible. In principle, it should be possible to specify a computation to sufficient detail that anyone should be able to reproduce it exactly. But in practice, there are fundamental, technical, and social barriers to doing so. The many objectives and meanings of reproducibility are discussed within the context of scientific computing. Technical barriers to reproducibility are described, extant approaches surveyed, and open areas of research are identified.},
	number = {3},
	urldate = {2021-01-13},
	journal = {ACM Computing Surveys},
	author = {Ivie, Peter and Thain, Douglas},
	month = jul,
	year = {2018},
	keywords = {Reproducibility, computational science, replicability, reproducible, scientific computing, scientific workflow, scientific workflows, workflow, workflows},
	pages = {63:1--63:36},
}

@misc{noauthor_lofar_nodate,
	title = {{LOFAR} and the radio astronomy community {\textbar} {EOSC} {Portal}},
	url = {https://www.eosc-portal.eu/lofar-and-radio-astronomy-community},
	urldate = {2021-01-13},
}

@misc{noauthor_eoscpilot_nodate,
	title = {{EOSCpilot} {Science} {Demonstrator}: {eWaterCycle} \& {SWITCH}-{ON} - {FAIR} data for hydrology {\textbar} {EOSC} {Portal}},
	url = {https://www.eosc-portal.eu/eoscpilot-science-demonstrator-ewatercycle-switch-fair-data-hydrology},
	urldate = {2021-01-13},
}

@book{chapman_common_2016,
	title = {Common {Workflow} {Language}, v1.0},
	url = {https://w3id.org/cwl/v1.0/},
	abstract = {The Common Workflow Language (CWL) is an informal, multi-vendor working group consisting of various organizations and individuals that have an interest in portability of data analysis workflows. Our goal is to create specifications that enable data scientists to describe analysis tools and workflows that are powerful, easy to use, portable, and support reproducibility.CWL builds on technologies such as JSON-LD and Avro for data modeling and Docker for portable runtime environments. CWL is designed to express workflows for data-intensive science, such as Bioinformatics, Medical Imaging, Chemistry, Physics, and Astronomy.This is v1.0 of the CWL tool and workflow specification, released on 2016-07-08},
	urldate = {2017-08-16},
	publisher = {figshare},
	author = {Chapman, Brad and Chilton, John and Heuer, Michael and Kartashov, Andrey and Leehr, Dan and Ménager, Hervé and Nedeljkovich, Maya and Scales, Matt and Soiland-Reyes, Stian and Stojanovic, Luka},
	editor = {Amstutz, Peter and Crusoe, Michael R. and Tijanić, Nebojša},
	month = jul,
	year = {2016},
	doi = {10.6084/M9.FIGSHARE.3115156.V2},
	note = {00030 },
}

@book{robinson_cwl_2017,
	title = {{CWL} {Viewer}: {The} {Common} {Workflow} {Language} {Viewer}},
	volume = {6},
	url = {https://www.open-bio.org/w/images/4/42/BOSC2017-complete-program-compressed.pdf},
	abstract = {The Common Workflow Language (CWL) project emerged from the BOSC 2014},
	urldate = {2017-09-07},
	publisher = {F1000Research},
	author = {Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R and Goble, Carole},
	month = jul,
	year = {2017},
	doi = {10.7490/f1000research.1114375.1},
	note = {00000 },
}

@article{de_la_garza_desktop_2016-1,
	title = {From the desktop to the grid: scalable bioinformatics via workflow conversion},
	volume = {17},
	issn = {1471-2105},
	shorttitle = {From the desktop to the grid},
	url = {https://doi.org/10.1186/s12859-016-0978-9},
	doi = {10.1186/s12859-016-0978-9},
	abstract = {Reproducibility is one of the tenets of the scientific method. Scientific experiments often comprise complex data flows, selection of adequate parameters, and analysis and visualization of intermediate and end results. Breaking down the complexity of such experiments into the joint collaboration of small, repeatable, well defined tasks, each with well defined inputs, parameters, and outputs, offers the immediate benefit of identifying bottlenecks, pinpoint sections which could benefit from parallelization, among others. Workflows rest upon the notion of splitting complex work into the joint effort of several manageable tasks.},
	number = {1},
	urldate = {2020-11-11},
	journal = {BMC Bioinformatics},
	author = {de la Garza, Luis and Veit, Johannes and Szolek, Andras and Röttig, Marc and Aiche, Stephan and Gesing, Sandra and Reinert, Knut and Kohlbacher, Oliver},
	month = mar,
	year = {2016},
	pages = {127},
}

@inproceedings{babuji_parsl_2019,
	address = {New York, NY, USA},
	series = {{HPDC} '19},
	title = {Parsl: {Pervasive} {Parallel} {Programming} in {Python}},
	isbn = {978-1-4503-6670-0},
	shorttitle = {Parsl},
	url = {https://doi.org/10.1145/3307681.3325400},
	doi = {10.1145/3307681.3325400},
	abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
	urldate = {2020-12-13},
	booktitle = {Proceedings of the 28th {International} {Symposium} on {High}-{Performance} {Parallel} and {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
	month = jun,
	year = {2019},
	keywords = {parallel programming, parsl, python},
	pages = {25--36},
}

@article{belhajjame_using_2015,
	title = {Using a suite of ontologies for preserving workflow-centric research objects},
	volume = {32},
	issn = {1570-8268},
	url = {http://www.sciencedirect.com/science/article/pii/S1570826815000049},
	doi = {10.1016/j.websem.2015.01.003},
	abstract = {Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions. In this article, we present a novel approach to the preservation of scientific workflows through the application of research objects—aggregations of data and metadata that enrich the workflow specifications. Our approach is realised as a suite of ontologies that support the creation of workflow-centric research objects. Their design was guided by requirements elicited from previous empirical analyses of workflow decay and repair. The ontologies developed make use of and extend existing well known ontologies, namely the Object Reuse and Exchange (ORE) vocabulary, the Annotation Ontology (AO) and the W3C PROV ontology (PROVO). We illustrate the application of the ontologies for building Workflow Research Objects with a case-study that investigates Huntington’s disease, performed in collaboration with a team from the Leiden University Medial Centre (HG-LUMC). Finally we present a number of tools developed for creating and managing workflow-centric research objects.},
	language = {en},
	urldate = {2021-01-13},
	journal = {Journal of Web Semantics},
	author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Gamble, Matthew and Hettne, Kristina and Palma, Raul and Mina, Eleni and Corcho, Oscar and Gómez-Pérez, José Manuel and Bechhofer, Sean and Klyne, Graham and Goble, Carole},
	month = may,
	year = {2015},
	keywords = {Annotation, Ontologies, Preservation, Provenance, Research object, Scientific workflow},
	pages = {16--42},
}

@inproceedings{missier_w3c_2013,
	address = {New York, NY, USA},
	series = {{EDBT} '13},
	title = {The {W3C} {PROV} family of specifications for modelling provenance metadata},
	isbn = {978-1-4503-1597-5},
	url = {https://doi.org/10.1145/2452376.2452478},
	doi = {10.1145/2452376.2452478},
	abstract = {Provenance, a form of structured metadata designed to record the origin or source of information, can be instrumental in deciding whether information is to be trusted, how it can be integrated with other diverse information sources, and how to establish attribution of information to authors throughout its history. The PROV set of specifications, produced by the World Wide Web Consortium (W3C), is designed to promote the publication of provenance information on the Web, and offers a basis for interoperability across diverse provenance management systems. The PROV provenance model is deliberately generic and domain-agnostic, but extension mechanisms are available and can be exploited for modelling specific domains. This tutorial provides an account of these specifications. Starting from intuitive and informal examples that present idiomatic provenance patterns, it progressively introduces the relational model of provenance along with the constraints model for validation of provenance documents, and concludes with example applications that show the extension points in use.},
	urldate = {2021-01-13},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Extending} {Database} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Missier, Paolo and Belhajjame, Khalid and Cheney, James},
	month = mar,
	year = {2013},
	pages = {773--776},
}

@article{tange_gnu_2011,
	title = {{GNU} {Parallel} - {The} {Command}-{Line} {Power} {Tool}},
	volume = {36},
	issn = {1044-6397},
	url = {https://www.usenix.org/system/files/login/articles/105438-Tange.pdf},
	journal = {;login: The USENIX Magazine},
	author = {Tange, O},
	month = feb,
	year = {2011},
	pages = {42--47},
}

@article{tejedor_pycompss_2017,
	title = {{PyCOMPSs}: {Parallel} computational workflows in {Python}},
	volume = {31},
	issn = {1094-3420},
	shorttitle = {{PyCOMPSs}},
	url = {https://doi.org/10.1177/1094342015594678},
	doi = {10.1177/1094342015594678},
	abstract = {The use of the Python programming language for scientific computing has been gaining momentum in the last years. The fact that it is compact and readable and its complete set of scientific libraries are two important characteristics that favour its adoption. Nevertheless, Python still lacks a solution for easily parallelizing generic scripts on distributed infrastructures, since the current alternatives mostly require the use of APIs for message passing or are restricted to embarrassingly parallel computations. In that sense, this paper presents PyCOMPSs, a framework that facilitates the development of parallel computational workflows in Python. In this approach, the user programs her script in a sequential fashion and decorates the functions to be run as asynchronous parallel tasks. A runtime system is in charge of exploiting the inherent concurrency of the script, detecting the data dependencies between tasks and spawning them to the available resources. Furthermore, we show how this programming model can be built on top of a Big Data storage architecture, where the data stored in the backend is abstracted and accessed from the application in the form of persistent objects.},
	language = {en},
	number = {1},
	urldate = {2020-12-13},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Tejedor, Enric and Becerra, Yolanda and Alomar, Guillem and Queralt, Anna and Badia, Rosa M and Torres, Jordi and Cortes, Toni and Labarta, Jesús},
	month = jan,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Big Data storage, Python, Scientic computing, parallel programming models},
	pages = {66--82},
}

@misc{noauthor_pycompss_nodate,
	title = {{PyCOMPSs}: {Parallel} computational workflows in {Python} - {Enric} {Tejedor}, {Yolanda} {Becerra}, {Guillem} {Alomar}, {Anna} {Queralt}, {Rosa} {M} {Badia}, {Jordi} {Torres}, {Toni} {Cortes}, {Jesús} {Labarta}, 2017},
	url = {https://journals-sagepub-com.vu-nl.idm.oclc.org/doi/10.1177/1094342015594678},
	urldate = {2020-12-13},
}

@article{mons_cloudy_2017,
	title = {Cloudy, increasingly {FAIR}; revisiting the {FAIR} {Data} guiding principles for the {European} {Open} {Science} {Cloud}},
	volume = {37},
	issn = {0167-5265},
	url = {http://content.iospress.com/articles/information-services-and-use/isu824},
	doi = {10.3233/ISU-170824},
	abstract = {The FAIR Data Principles propose that all scholarly output should be Findable, Accessible, Interoperable, and Reusable. As a set of guiding principles, expressing only the kinds of behaviours that researchers should expect from contemporary data reso},
	language = {en},
	number = {1},
	urldate = {2020-07-15},
	journal = {Information Services \& Use},
	author = {Mons, Barend and Neylon, Cameron and Velterop, Jan and Dumontier, Michel and da Silva Santos, Luiz Olavo Bonino and Wilkinson, Mark D.},
	month = jan,
	year = {2017},
	note = {Publisher: IOS Press},
	pages = {49--56},
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	language = {en},
	number = {1},
	urldate = {2020-07-15},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {160018},
}

@article{koster_snakemakescalable_2012,
	title = {Snakemake—a scalable bioinformatics workflow engine},
	volume = {28},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/28/19/2520/290322},
	doi = {10.1093/bioinformatics/bts480},
	abstract = {Abstract.  Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that},
	language = {en},
	number = {19},
	urldate = {2019-08-01},
	journal = {Bioinformatics},
	author = {Köster, Johannes and Rahmann, Sven},
	month = oct,
	year = {2012},
	pages = {2520--2522},
}

@article{deelman_future_2017,
	title = {The future of scientific workflows},
	volume = {32},
	issn = {1094-3420},
	url = {http://journals.sagepub.com/doi/10.1177/1094342017704893},
	doi = {10.1177/1094342017704893},
	abstract = {Today’s computational, experimental, and observational sciences rely on computations that involve many related tasks. The success of a scientific mission often hinges on the computer automation of these workflows. In April 2015, the US Department of Energy (DOE) invited a diverse group of domain and computer scientists from national laboratories supported by the Office of Science, the National Nuclear Security Administration, from industry, and from academia to review the workflow requirements of DOE’s science and national security missions, to assess the current state of the art in science workflows, to understand the impact of emerging extreme-scale computing systems on those workflows, and to develop requirements for automated workflow management in future and existing environments. This article is a summary of the opinions of over 50 leading researchers attending this workshop. We highlight use cases, computing systems, workflow needs and conclude by summarizing the remaining challenges this community...},
	number = {1},
	urldate = {2020-01-20},
	journal = {The international journal of high performance computing applications},
	author = {Deelman, Ewa and Peterka, Tom and Altintas, Ilkay and Carothers, Christopher D and Kleese van Dam, Kerstin and Moreland, Kenneth and Parashar, Manish and Ramakrishnan, Lavanya and Taufer, Michela and Vetter, Jeffrey},
	month = apr,
	year = {2017},
	pages = {109434201770489},
}

@article{schaduangrat_towards_2020,
	title = {Towards reproducible computational drug discovery},
	volume = {12},
	issn = {1758-2946},
	url = {https://doi.org/10.1186/s13321-020-0408-x},
	doi = {10.1186/s13321-020-0408-x},
	abstract = {The reproducibility of experiments has been a long standing impediment for further scientific progress. Computational methods have been instrumental in drug discovery efforts owing to its multifaceted utilization for data collection, pre-processing, analysis and inference. This article provides an in-depth coverage on the reproducibility of computational drug discovery. This review explores the following topics: (1) the current state-of-the-art on reproducible research, (2) research documentation (e.g. electronic laboratory notebook, Jupyter notebook, etc.), (3) science of reproducible research (i.e. comparison and contrast with related concepts as replicability, reusability and reliability), (4) model development in computational drug discovery, (5) computational issues on model development and deployment, (6) use case scenarios for streamlining the computational drug discovery protocol. In computational disciplines, it has become common practice to share data and programming codes used for numerical calculations as to not only facilitate reproducibility, but also to foster collaborations (i.e. to drive the project further by introducing new ideas, growing the data, augmenting the code, etc.). It is therefore inevitable that the field of computational drug design would adopt an open approach towards the collection, curation and sharing of data/code.},
	number = {1},
	urldate = {2020-01-30},
	journal = {Journal of Cheminformatics},
	author = {Schaduangrat, Nalini and Lampa, Samuel and Simeon, Saw and Gleeson, Matthew Paul and Spjuth, Ola and Nantasenamat, Chanin},
	month = jan,
	year = {2020},
	pages = {9},
}

@article{belhajjame_privacy-aware_2020,
	title = {On privacy-aware {eScience} workflows},
	issn = {1436-5057},
	url = {https://doi.org/10.1007/s00607-019-00783-8},
	doi = {10.1007/s00607-019-00783-8},
	abstract = {Computing-intensive experiments in modern sciences have become increasingly data-driven illustrating perfectly the Big-Data era. These experiments are usually specified and enacted in the form of workflows that would need to manage (i.e., read, write, store, and retrieve) highly-sensitive data like persons’ medical records. We assume for this work that the operations that constitute a workflow are 1-to-1 operations, in the sense that for each input data record they produce a single data record. While there is an active research body on how to protect sensitive data by, for instance, anonymizing datasets, there is a limited number of approaches that would assist scientists with identifying the datasets, generated by the workflows, that need to be anonymized along with setting the anonymization degree that must be met. We present in this paper a solution privacy requirements of datasets used and generated by a workflow execution. We also present a technique for anonymizing workflow data given an anonymity degree.},
	journal = {Computing},
	author = {Belhajjame, Khalid and Faci, Noura and Maamar, Zakaria and Burégio, Vanilson and Soares, Edvan and Barhamgi, Mahmoud},
	month = jan,
	year = {2020},
}

@article{gruning_bioconda_2018,
	title = {Bioconda: sustainable and comprehensive software distribution for the life sciences.},
	volume = {15},
	issn = {1548-7091},
	url = {http://www.nature.com/articles/s41592-018-0046-7},
	doi = {10.1038/s41592-018-0046-7},
	number = {7},
	urldate = {2018-07-13},
	journal = {Nature Methods},
	author = {Grüning, Björn and Dale, Ryan and Sjödin, Andreas and Chapman, Brad A and Rowe, Jillian and Tomkins-Tinch, Christopher H and Valieris, Renan and Köster, Johannes and Team, Bioconda},
	year = {2018},
	pmid = {29967506},
	pages = {475--476},
}

@article{papageorgiou_genomic_2018,
	title = {Genomic big data hitting the storage bottleneck.},
	volume = {24},
	url = {http://dx.doi.org/10.14806/ej.24.0.910},
	doi = {10.14806/ej.24.0.910},
	abstract = {During the last decades, there is a vast data explosion in bioinformatics. Big data centres are trying to face this data crisis, reaching high storage capacity levels. Although several scientific giants examine how to handle the enormous pile of information in their cupboards, the problem remains unsolved. On a daily basis, there is a massive quantity of permanent loss of extensive information due to infrastructure and storage space problems. The motivation for sequencing has fallen behind. Sometimes, the time that is spent to solve storage space problems is longer than the one dedicated to collect and analyse data. To bring sequencing to the foreground, scientists have to slide over such obstacles and find alternative ways to approach the issue of data volume. Scientific community experiences the data crisis era, where, out of the box solutions may ease the typical research workflow, until technological development meets the needs of Bioinformatics.},
	urldate = {2018-06-05},
	journal = {EMBnet.journal},
	author = {Papageorgiou, Louis and Eleni, Picasi and Raftopoulou, Sofia and Mantaiou, Meropi and Megalooikonomou, Vasileios and Vlachakis, Dimitrios},
	month = apr,
	year = {2018},
	pmid = {29782620},
	pmcid = {PMC5958914},
}

@article{glatard_boutiques_2018,
	title = {Boutiques: a flexible framework to integrate command-line applications in computing platforms.},
	volume = {7},
	issn = {2047-217X},
	url = {https://academic.oup.com/gigascience/advance-article/doi/10.1093/gigascience/giy016/4951979},
	doi = {10.1093/gigascience/giy016},
	abstract = {We present Boutiques, a system to automatically publish, integrate and execute command-line applications across computational platforms. Boutiques applications are installed through software containers described in a rich and flexible JSON language. A set of core tools facilitate the construction, validation, import, execution, and publishing of applications. Boutiques is currently supported by several distinct virtual research platforms, and it has been used to describe dozens of applications in the neuroinformatics domain. We expect Boutiques to improve the quality of application integration in computational platforms, to reduce redundancy of effort, to contribute to computational reproducibility, and to foster Open Science.},
	number = {5},
	urldate = {2018-05-29},
	journal = {GigaScience},
	author = {Glatard, Tristan and Kiar, Gregory and Aumentado-Armstrong, Tristan and Beck, Natacha and Bellec, Pierre and Bernard, Rémi and Bonnet, Axel and Brown, Shawn T and Camarasu-Pop, Sorina and Cervenansky, Frédéric and Das, Samir and Ferreira da Silva, Rafael and Flandin, Guillaume and Girard, Pascal and Gorgolewski, Krzysztof J and Guttmann, Charles R G and Hayot-Sasson, Valérie and Quirion, Pierre-Olivier and Rioux, Pierre and Rousseau, Marc-Étienne and Evans, Alan C},
	month = mar,
	year = {2018},
	pmid = {29718199},
	pmcid = {PMC6007562},
}

@article{van_mourik_porcupine_2018,
	title = {Porcupine: {A} visual pipeline tool for neuroimaging analysis.},
	volume = {14},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1006064},
	doi = {10.1371/journal.pcbi.1006064},
	abstract = {The field of neuroimaging is rapidly adopting a more reproducible approach to data acquisition and analysis. Data structures and formats are being standardised and data analyses are getting more automated. However, as data analysis becomes more complicated, researchers often have to write longer analysis scripts, spanning different tools across multiple programming languages. This makes it more difficult to share or recreate code, reducing the reproducibility of the analysis. We present a tool, Porcupine, that constructs one's analysis visually and automatically produces analysis code. The graphical representation improves understanding of the performed analysis, while retaining the flexibility of modifying the produced code manually to custom needs. Not only does Porcupine produce the analysis code, it also creates a shareable environment for running the code in the form of a Docker image. Together, this forms a reproducible way of constructing, visualising and sharing one's analysis. Currently, Porcupine links to Nipype functionalities, which in turn accesses most standard neuroimaging analysis tools. Our goal is to release researchers from the constraints of specific implementation details, thereby freeing them to think about novel and creative ways to solve a given problem. Porcupine improves the overview researchers have of their processing pipelines, and facilitates both the development and communication of their work. This will reduce the threshold at which less expert users can generate reusable pipelines. With Porcupine, we bridge the gap between a conceptual and an implementational level of analysis and make it easier for researchers to create reproducible and shareable science. We provide a wide range of examples and documentation, as well as installer files for all platforms on our website: https://timvanmourik.github.io/Porcupine. Porcupine is free, open source, and released under the GNU General Public License v3.0.},
	number = {5},
	urldate = {2018-06-05},
	journal = {PLoS Computational Biology},
	author = {van Mourik, Tim and Snoek, Lukas and Knapen, Tomas and Norris, David G},
	month = may,
	year = {2018},
	pmid = {29746461},
	pmcid = {PMC5963801},
	pages = {e1006064},
}

@article{ko_closha_2018,
	title = {Closha: bioinformatics workflow system for the analysis of massive sequencing data.},
	volume = {19},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2019-3},
	doi = {10.1186/s12859-018-2019-3},
	abstract = {BACKGROUND: While next-generation sequencing (NGS) costs have fallen in recent years, the cost and complexity of computation remain substantial obstacles to the use of NGS in bio-medical care and genomic research. The rapidly increasing amounts of data available from the new high-throughput methods have made data processing infeasible without automated pipelines. The integration of data and analytic resources into workflow systems provides a solution to the problem by simplifying the task of data analysis. RESULTS: To address this challenge, we developed a cloud-based workflow management system, Closha, to provide fast and cost-effective analysis of massive genomic data. We implemented complex workflows making optimal use of high-performance computing clusters. Closha allows users to create multi-step analyses using drag and drop functionality and to modify the parameters of pipeline tools. Users can also import the Galaxy pipelines into Closha. Closha is a hybrid system that enables users to use both analysis programs providing traditional tools and MapReduce-based big data analysis programs simultaneously in a single pipeline. Thus, the execution of analytics algorithms can be parallelized, speeding up the whole process. We also developed a high-speed data transmission solution, KoDS, to transmit a large amount of data at a fast rate. KoDS has a file transfer speed of up to 10 times that of normal FTP and HTTP. The computer hardware for Closha is 660 CPU cores and 800 TB of disk storage, enabling 500 jobs to run at the same time. CONCLUSIONS: Closha is a scalable, cost-effective, and publicly available web service for large-scale genomic data analysis. Closha supports the reliable and highly scalable execution of sequencing analysis workflows in a fully automated manner. Closha provides a user-friendly interface to all genomic scientists to try to derive accurate results from NGS platform data. The Closha cloud server is freely available for use from http://closha.kobic.re.kr/ .},
	number = {Suppl 1},
	urldate = {2018-05-29},
	journal = {BMC Bioinformatics},
	author = {Ko, GunHwan and Kim, Pan-Gyu and Yoon, Jongcheol and Han, Gukhee and Park, Seong-Jin and Song, Wangho and Lee, Byungwook},
	month = feb,
	year = {2018},
	pmid = {29504905},
	pmcid = {PMC5836837},
	pages = {43},
}

@article{digles_accessing_2018,
	title = {Accessing the {Open} {PHACTS} {Discovery} {Platform} with {Workflow} {Tools}.},
	volume = {1787},
	url = {http://dx.doi.org/10.1007/978-1-4939-7847-2_14},
	doi = {10.1007/978-1-4939-7847-2_14},
	abstract = {The Open PHACTS Discovery Platform integrates several public databases, which can be of interest when annotating the results of a phenotypic screening campaign. Workflow tools provide easy-to-customize possibilities to access the platform. Here, we describe how to create such workflows for two different workflow tools (KNIME and Pipeline Pilot), including a protocol to annotate compounds (e.g., phenotypic screening hits) with compound classification, known protein targets, and classifications of the targets.},
	urldate = {2018-05-29},
	journal = {Methods in Molecular Biology},
	author = {Digles, Daniela and Caracoti, Andrei and Jacoby, Edgar},
	year = {2018},
	pmid = {29736719},
	pages = {183--193},
}

@article{wimalaratne_uniform_2018,
	title = {Uniform resolution of compact identifiers for biomedical data.},
	volume = {5},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata201829},
	doi = {10.1038/sdata.2018.29},
	abstract = {Most biomedical data repositories issue locally-unique accessions numbers, but do not provide globally unique, machine-resolvable, persistent identifiers for their datasets, as required by publishers wishing to implement data citation in accordance with widely accepted principles. Local accessions may however be prefixed with a namespace identifier, providing global uniqueness. Such "compact identifiers" have been widely used in biomedical informatics to support global resource identification with local identifier assignment. We report here on our project to provide robust support for machine-resolvable, persistent compact identifiers in biomedical data citation, by harmonizing the Identifiers.org and N2T.net (Name-To-Thing) meta-resolvers and extending their capabilities. Identifiers.org services hosted at the European Molecular Biology Laboratory - European Bioinformatics Institute (EMBL-EBI), and N2T.net services hosted at the California Digital Library (CDL), can now resolve any given identifier from over 600 source databases to its original source on the Web, using a common registry of prefix-based redirection rules. We believe these services will be of significant help to publishers and others implementing persistent, machine-resolvable citation of research data.},
	urldate = {2018-05-29},
	journal = {Scientific data},
	author = {Wimalaratne, Sarala M and Juty, Nick and Kunze, John and Janée, Greg and McMurry, Julie A and Beard, Niall and Jimenez, Rafael and Grethe, Jeffrey S and Hermjakob, Henning and Martone, Maryann E and Clark, Tim},
	month = may,
	year = {2018},
	pmid = {29737976},
	pmcid = {PMC5944906},
	pages = {180029},
}

@article{miksa_using_2017,
	title = {Using ontologies for verification and validation of workflow-based experiments},
	volume = {43},
	issn = {15708268},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1570826817300112},
	doi = {10.1016/j.websem.2017.01.002},
	abstract = {Scientific experiments performed in the eScience domain require special tooling, software, and workflows that allow researchers to link, transform, visualize and interpret data. Recent studies report that such experiments often cannot be replicated due to dierences in the underlying infrastructure. The provenance collection mechanisms were built into workflow engines to increase research replicability. However, the traces do not contain the execution context that consists of software, hardware and external services used to produce the result which may change between executions. The problem thus remains on how to identify such context and how to store such data. To address this challenge we propose the context model that integrates ontologies which describe workflow and its environment. It includes not only high level description of workflow steps and services but also low level technical details on infrastructure, including hardware, software, and files. In this paper we discuss which ontologies that compose the context model must be instantiated to enable verification of a workflow re-execution. We use a tool that monitors a workflow execution and automatically creates the context model. We also authored the VPlan ontology that enables modelling validation requirements. It contains a controlled vocabulary of metrics that can be used for quantification of requirements. We evaluate the proposed ontologies on five Taverna workflows that dier in the degree on which they depend on additional software and services. The results show that the proposed ontologies are necessary and can be used for verification and validation of scientific workflows re-executions in dierent environments without the necessity of accessing the original environment at the same time. Thus the scientists can state whether the scientific experiment is replicable.},
	number = {0},
	urldate = {2018-04-30},
	journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
	author = {Miksa, Tomasz and Rauber, Andreas},
	month = mar,
	year = {2017},
	pages = {25--45},
}

@article{peng_sos_2018,
	title = {{SoS} {Notebook}: {An} {Interactive} {Multi}-{Language} {Data} {Analysis} {Environment}.},
	volume = {34},
	url = {http://dx.doi.org/10.1093/bioinformatics/bty405},
	doi = {10.1093/bioinformatics/bty405},
	abstract = {Motivation: Complex bioinformatic data analysis workflows involving multiple scripts in different languages can be difficult to consolidate, share, and reproduce. An environment that streamlines the entire processes of data collection, analysis, visualization and reporting of such multi-language analyses is currently lacking. Results: We developed Script of Scripts (SoS) Notebook, a web-based notebook environment that allows the use of multiple scripting language in a single notebook, with data flowing freely within and across languages. SoS Notebook enables researchers to perform sophisticated bioinformatic analysis using the most suitable tools for different parts of the workflow, without the limitations of a particular language or complications of cross-language communications. Availability: SoS Notebook is hosted at http://vatlab.github.io/SoS/ and is distributed under a BSD license. Contact: bpeng@mdanderson.org.},
	number = {21},
	urldate = {2018-05-29},
	journal = {Bioinformatics},
	author = {Peng, Bo and Wang, Gao and Ma, Jun and Leong, Man Chong and Wakefield, Chris and Melott, James and Chiu, Yulun and Du, Di and Weinstein, John N},
	month = may,
	year = {2018},
	pmid = {29790910},
	pmcid = {PMC6198852},
	pages = {3768--3770},
}

@article{koster_snakemake-scalable_2018,
	title = {Snakemake-a scalable bioinformatics workflow engine.},
	volume = {34},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty350/4996910},
	doi = {10.1093/bioinformatics/bty350},
	number = {20},
	urldate = {2018-05-29},
	journal = {Bioinformatics},
	author = {Köster, Johannes and Rahmann, Sven},
	month = oct,
	year = {2018},
	pmid = {29788404},
	pages = {3600},
}

@article{kluge_watchdog_2018,
	title = {Watchdog - a workflow management system for the distributed analysis of large-scale experimental data.},
	volume = {19},
	url = {http://dx.doi.org/10.1186/s12859-018-2107-4},
	doi = {10.1186/s12859-018-2107-4},
	abstract = {BACKGROUND: The development of high-throughput experimental technologies, such as next-generation sequencing, have led to new challenges for handling, analyzing and integrating the resulting large and diverse datasets. Bioinformatical analysis of these data commonly requires a number of mutually dependent steps applied to numerous samples for multiple conditions and replicates. To support these analyses, a number of workflow management systems (WMSs) have been developed to allow automated execution of corresponding analysis workflows. Major advantages of WMSs are the easy reproducibility of results as well as the reusability of workflows or their components. RESULTS: In this article, we present Watchdog, a WMS for the automated analysis of large-scale experimental data. Main features include straightforward processing of replicate data, support for distributed computer systems, customizable error detection and manual intervention into workflow execution. Watchdog is implemented in Java and thus platform-independent and allows easy sharing of workflows and corresponding program modules. It provides a graphical user interface (GUI) for workflow construction using pre-defined modules as well as a helper script for creating new module definitions. Execution of workflows is possible using either the GUI or a command-line interface and a web-interface is provided for monitoring the execution status and intervening in case of errors. To illustrate its potentials on a real-life example, a comprehensive workflow and modules for the analysis of RNA-seq experiments were implemented and are provided with the software in addition to simple test examples. CONCLUSIONS: Watchdog is a powerful and flexible WMS for the analysis of large-scale high-throughput experiments. We believe it will greatly benefit both users with and without programming skills who want to develop and apply bioinformatical workflows with reasonable overhead. The software, example workflows and a comprehensive documentation are freely available at www.bio.ifi.lmu.de/watchdog.},
	number = {1},
	urldate = {2018-04-28},
	journal = {BMC Bioinformatics},
	author = {Kluge, Michael and Friedel, Caroline C},
	month = mar,
	year = {2018},
	pmid = {29534677},
	pmcid = {PMC5850912},
	pages = {97},
}

@inproceedings{singh_deep_2017,
	title = {Deep {Learning} on {Operational} {Facility} {Data} {Related} to {Large}-{Scale} {Distributed} {Area} {Scientific} {Workflows}},
	isbn = {978-1-5386-2686-3},
	url = {http://ieeexplore.ieee.org/document/8109199/},
	doi = {10.1109/eScience.2017.94},
	abstract = {Distributed computing platforms provide a robust mechanism to perform large-scale computations by splitting the task and data among multiple locations, possibly located thousands of miles apart geographically. Although such distribution of resources can lead to benefits, it also comes with its associated problems such as rampant duplication of file transfers increasing congestion, long job completion times, unexpected site crashing, suboptimal data transfer rates, unpredictable reliability in a time range, and suboptimal usage of storage elements. In addition, each sub-system becomes a potential failure node that can trigger system wide disruptions. In this vision paper, we outline our approach to leveraging Deep Learning algorithms to discover solutions to unique problems that arise in a system with computational infrastructure that is spread over a wide area. The presented vision, motivated by a real scientific use case from Belle II experiments, is to develop multilayer neural networks to tackle forecasting, anomaly detection and optimization challenges in a complex and distributed data movement environment. Through this vision based on Deep Learning principles, we aim to achieve reduced congestion events, faster file transfer rates, and enhanced site reliability.},
	urldate = {2018-04-28},
	booktitle = {2017 {IEEE} 13th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Singh, Alok and Stephan, Eric and Schram, Malachi and Altintas, Ilkay},
	month = oct,
	year = {2017},
	pages = {586--591},
}

@inproceedings{zamani_supporting_2017,
	title = {Supporting {Data}-{Driven} {Workflows} {Enabled} by {Large} {Scale} {Observatories}},
	isbn = {978-1-5386-2686-3},
	url = {http://ieeexplore.ieee.org/document/8109200/},
	doi = {10.1109/eScience.2017.95},
	abstract = {Large scale observatories are shared-use resources that provide open access to data from geographically distributed sensors and instruments. This data has the potential to accelerate scientific discovery. However, seamlessly integrating the data into scientific workflows remains a challenge. In this paper, we summarize our ongoing work in supporting data-driven and data-intensive workflows and outline our vision for how these observatories can improve large-scale science. Specifically, we present programming abstractions and runtime management services to enable the automatic integration of data in scientific workflows. Further, we show how approximation techniques can be used to address network and processing variations by studying constraint limitations and their associated latencies. We use the Ocean Observatories Initiative (OOI) as a driving use case for this work.},
	urldate = {2018-04-28},
	booktitle = {2017 {IEEE} 13th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Zamani, Ali Reza and AbdelBaky, Moustafa and Balouek-Thomert, Daniel and Rodero, Ivan and Parashar, Manish},
	month = oct,
	year = {2017},
	pages = {592--595},
}

@inproceedings{mandal_toward_2017,
	title = {Toward prioritization of data flows for scientific workflows using virtual software defined exchanges},
	isbn = {978-1-5386-2686-3},
	url = {http://ieeexplore.ieee.org/document/8109197/},
	doi = {10.1109/eScience.2017.92},
	abstract = {Recent advances in cloud systems, on-demand circuits and software-defined networking have created new opportunities to enable complex, data-intensive scientific applications to run on dynamic networked cloud infrastructures. In this work, we present an end-to-end framework for autonomic adaptation for scientific workflows on networked cloud systems, which leverages novel network provisioning technologies. We present an application-independent controller framework called Mobius++ that includes dynamic network adaptation capabilities using Software-Defined Networking (SDN) mechanisms, which enables workflow management systems to address competing priorities of workflow operations, data movements in particular. We use a representative, data-intensive bioinformatics workflow as a driving use case to showcase the above capabilities. Experimental results show that the Mobius++ framework, in conjunction with a novel virtual Software Defined Exchange (SDX) platform, is able to dynamically prioritize bandwidths between different end-points, on-demand, and being driven by priority directives from a workflow management system. We show that data transfer jobs from two workflows with different priorities are accurately arbitrated as the relative priorities change.},
	urldate = {2018-04-28},
	booktitle = {2017 {IEEE} 13th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Mandal, Anirban and Ruth, Paul and Baldin, Ilya and Da Silva, Rafael Ferreira and Deelman, Ewa},
	month = oct,
	year = {2017},
	pages = {566--575},
}

@inproceedings{diaz_workflowhunt_2017,
	title = {Workflowhunt: combining keyword and semantic search in scientific workflow repositories},
	isbn = {978-1-5386-2686-3},
	url = {http://ieeexplore.ieee.org/document/8109131/},
	doi = {10.1109/eScience.2017.26},
	abstract = {Scientific datasets and the experiments that analyze them are growing in size and complexity, and scientists are facing difficulties to share such resources. Some initiatives have emerged to try to solve this problem. One of them involves the use of scientific workflows to represent and enact experiment execution. There is an increasing number of workflows that are potentially relevant for more than one scientific domain. However, it is hard to find workflows suitable for reuse given an experiment. Creating a workflow takes time and resources, and their reuse helps scientists to build new workflows faster and in a more reliable way. Search mechanisms in workflow repositories should provide different options for workflow discovery, but it is difficult for generic repositories to provide multiple mechanisms. This paper presents WorkflowHunt, a hybrid architecture for workflow search and discovery for generic repositories, which combines keyword and semantic search to allow finding relevant workflows using different search methods. We validated our architecture creating a prototype that uses real workflows and metadata from myExperiment, and compare search results via WorkflowHunt and via myExperiment's search interface.},
	urldate = {2018-04-28},
	booktitle = {2017 {IEEE} 13th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Diaz, Juan Sebastian Beleno and Medeiros, Claudia Bauzer},
	month = oct,
	year = {2017},
	pages = {138--147},
}

@article{penders_ten_2018,
	title = {Ten simple rules for responsible referencing.},
	volume = {14},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1006036},
	doi = {10.1371/journal.pcbi.1006036},
	number = {4},
	urldate = {2018-05-29},
	journal = {PLoS Computational Biology},
	author = {Penders, Bart},
	month = apr,
	year = {2018},
	pmid = {29649210},
	pmcid = {PMC5896885},
	pages = {e1006036},
}

@article{devenyi_ten_2018,
	title = {Ten simple rules for collaborative lesson development.},
	volume = {14},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1005963},
	doi = {10.1371/journal.pcbi.1005963},
	number = {3},
	urldate = {2018-05-29},
	journal = {PLoS Computational Biology},
	author = {Devenyi, Gabriel A and Emonet, Rémi and Harris, Rayna M and Hertweck, Kate L and Irving, Damien and Milligan, Ian and Wilson, Greg},
	month = mar,
	year = {2018},
	pmid = {29494585},
	pmcid = {PMC5832188},
	pages = {e1005963},
}

@article{atkinson_scientific_2017,
	title = {Scientific workflows: {Past}, present and future},
	volume = {75},
	issn = {0167739X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X17311202},
	doi = {10.1016/j.future.2017.05.041},
	abstract = {This special issue and our editorial celebrate 10 years of progress with data-intensive or scientific workflows. There have been very substantial advances in the representation of workflows and in the engineering of workflow management systems (WMS). The creation and refinement stages are now well supported, with a significant improvement in usability. Improved abstraction supports cross-fertilisation between different workflow communities and consistent interpretation as WMS evolve. Through such re-engineering the WMS deliver much improved performance, significantly increased scale and sophisticated reliability mechanisms. Further improvement is anticipated from substantial advances in optimisation. We invited papers from those who have delivered these advances and selected 14 to represent today’s achievements and representative plans for future progress. This editorial introduces those contributions with an overview and categorisation of the papers. Furthermore, it elucidates responses from a survey of major workflow systems, which provides evidence of substantial progress and a structured index of related papers. We conclude with suggestions on areas where further research and development is needed and offer a vision of future research directions.},
	urldate = {2018-07-13},
	journal = {Future Generation Computer Systems},
	author = {Atkinson, Malcolm and Gesing, Sandra and Montagnat, Johan and Taylor, Ian},
	month = oct,
	year = {2017},
	pages = {216--227},
}

@incollection{christensen_store_1995,
	address = {Berlin, Heidelberg},
	series = {Lecture notes in computer science},
	title = {Store — a system for handling third-party applications in a heterogeneous computer environment},
	volume = {1005},
	isbn = {978-3-540-47768-6},
	url = {http://link.springer.com/10.1007/3-540-60578-9_22},
	urldate = {2017-11-03},
	booktitle = {Software {Configuration} {Management}},
	publisher = {Springer Berlin Heidelberg},
	author = {Christensen, Anders and Egge, Tor},
	editor = {Estublier, Jacky and Goos, Gerhard and Hartmanis, Juris and Leeuwen, Jan},
	year = {1995},
	doi = {10.1007/3-540-60578-9_22},
	pages = {263--276},
}

@article{ferreira_da_silva_characterization_2017,
	title = {A characterization of workflow management systems for extreme-scale applications},
	volume = {75},
	issn = {0167739X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X17302510},
	doi = {10.1016/j.future.2017.02.026},
	abstract = {Automation of the execution of computational tasks is at the heart of improving scientific productivity. Over the last years, scientific workflows have been established as an important abstraction that captures data processing and computation of large and complex scientific applications. By allowing scientists to model and express entire data processing steps and their dependencies, workflow management systems relieve scientists from the details of an application and manage its execution on a computational infrastructure. As the resource requirements of today’s computational and data science applications that process vast amounts of data keep increasing, there is a compelling case for a new generation of advances in high-performance computing, commonly termed as extreme-scale computing, which will bring forth multiple challenges for the design of workflow applications and management systems. This paper presents a novel characterization of workflow management systems using features commonly associated with extreme-scale computing applications. We classify 15 popular workflow management systems in terms of workflow execution models, heterogeneous computing environments, and data access methods. The paper also surveys workflow applications and identifies gaps for future research on the road to extreme-scale workflows and management systems.},
	urldate = {2020-01-20},
	journal = {Future Generation Computer Systems},
	author = {Ferreira da Silva, Rafael and Filgueira, Rosa and Pietri, Ilia and Jiang, Ming and Sakellariou, Rizos and Deelman, Ewa},
	month = oct,
	year = {2017},
	pages = {228--238},
}

@article{carey_ten_2018,
	title = {Ten simple rules for biologists learning to program.},
	volume = {14},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1005871},
	doi = {10.1371/journal.pcbi.1005871},
	number = {1},
	urldate = {2018-05-29},
	journal = {PLoS Computational Biology},
	author = {Carey, Maureen A and Papin, Jason A},
	month = jan,
	year = {2018},
	pmid = {29300745},
	pmcid = {PMC5754048},
	pages = {e1005871},
}

@phdthesis{klingstrom_data_2017,
	type = {{THESIS}.{DOCTORAL}},
	title = {Data integration and handling},
	url = {https://pub.epsilon.slu.se/14669/},
	abstract = {Modern technology allows researchers to generate data at an ever increasing rate,outpacing the capacity of researchers to analyse it. Developing automated supportsystems for the collection, management and distribution of information is therefore animportant step to reduce error rates and accelerate progress to enable high-qualityresearch based on big data volumes. This thesis encompasses five articles, describingstrategies for the creation of technical research platforms, as well as descriptions of thetechnical platforms themselves.The key conclusion of the thesis is that technical solutions for many issues have beenavailable for a long time. These technical solutions are however overlooked, or simplyignored, if they fail to recognise the social dimensions of the issues they try to solve.The Molecular Methods database is an example of a technically sound but onlypartially successful solution in regards to social viability. Thousands of researchershave used the website to access protocols, but only a handful have shared their ownwork on MolMeth. Experiences from the Molecular Methods database and otherprojects have provided a foundation for studies supporting the development of theeB3KitThe eB3Kit is a portable, robust and scalable informatics platform for structured datamanagement. Deploying the platform enables research groups to carry out advancedresearch projects with very limited means. With the eB3Kit researchers can integratedata from a wide variety of sources, including the local laboratory informationmanagement system and analyse it using the Galaksio interface. Galaksio provides userfriendly access to the Galaxy workflow management system and provides eB3Kit userswith access to tools developed by a far larger user community than the one activelydeveloping the eB3Kit. Using a workflow management system improvesreproducibility and enables bioinformaticians to prepare workflows without directlyaccessing ethically or commercially sensitive data. Therefore, it is especially well-suited for applications where researchers are worried about privacy and during diseaseoutbreaks where persistent storage and analysis capacity must be established quickly.},
	urldate = {2017-11-08},
	school = {Uppsala : Sveriges lantbruksuniv},
	author = {Klingström, Tomas},
	month = oct,
	year = {2017},
}

@article{perens_debians_1997,
	title = {Debian’s “{Social} {Contract}” with the {Free} {Software} {Community}},
	url = {https://lists.debian.org/debian-announce/1997/msg00017.html},
	number = {msg00017},
	urldate = {2017-11-03},
	journal = {debian-announce@lists.debian.org},
	author = {Perens, Bruce},
	month = jul,
	year = {1997},
	keywords = {⛔ No DOI found},
}

@article{murdock_debian_1994,
	title = {The {Debian} {Linux} {Manifesto}},
	url = {http://www.ibiblio.org/pub/historic-linux/distributions/debian-0.91/info/Manifesto},
	urldate = {2017-11-03},
	author = {Murdock, Ian A.},
	month = jun,
	year = {1994},
	keywords = {⛔ No DOI found},
}

@article{gruning_bioconda_2017,
	title = {Bioconda: {A} sustainable and comprehensive software distribution for the life sciences},
	url = {http://biorxiv.org/lookup/doi/10.1101/207092},
	doi = {10.1101/207092},
	abstract = {We present Bioconda (https://bioconda.github.io), a distribution of bioinformatics software for the lightweight, multi-platform and language-agnostic package manager Conda. Currently, Bioconda offers a collection of over 3000 software packages, which is continuously maintained, updated, and extended by a growing global community of more than 200 contributors. Bioconda improves analysis reproducibility by allowing users to define isolated environments with defined software versions, all of which are easily installed and managed without administrative privileges.},
	urldate = {2017-11-03},
	journal = {BioRxiv},
	author = {Grüning, Björn and Dale, Ryan and Sjödin, Andreas and Rowe, Jillian and Chapman, Brad A. and Tomkins-Tinch, Christopher H. and Valieris, Renan and Team, The Bioconda and Köster, Johannes},
	month = oct,
	year = {2017},
}

@article{krabbenhoft_integrating_2008,
	title = {Integrating {ARC} grid middleware with {Taverna} workflows.},
	volume = {24},
	url = {http://dx.doi.org/10.1093/bioinformatics/btn095},
	doi = {10.1093/bioinformatics/btn095},
	abstract = {SUMMARY: This work presents two independent approaches for a seamless integration of computational grids with the bioinformatics workflow suite Taverna. These are supported by a unique relational database to link applications with grid resources and presents those as workflow elements. A web portal facilitates its collaborative maintenance. The first approach implements a gateway service to handle authentication certificates and all communication with the grid. It reads the database to spawn web services for workflow elements which are in turn used by Taverna. The second approach lets Taverna communicate with the grid on its own, by means of a newly developed plug-in. It reads the database and executes the needed tasks directly on the grid. While the gateway service is non-intrusive, the plug-in has technical advantages, e.g. by allowing data to remain on the grid while being passed between workflow elements. AVAILABILITY: http://grid.inb.uni-luebeck.de/},
	number = {9},
	urldate = {2017-11-03},
	journal = {Bioinformatics},
	author = {Krabbenhöft, Hajo N and Möller, Steffen and Bayer, Daniel},
	month = may,
	year = {2008},
	pmid = {18353787},
	pages = {1221--1222},
}

@article{laurie_wet-lab_2016,
	title = {From {Wet}-{Lab} to {Variations}: {Concordance} and {Speed} of {Bioinformatics} {Pipelines} for {Whole} {Genome} and {Whole} {Exome} {Sequencing}.},
	volume = {37},
	url = {http://dx.doi.org/10.1002/humu.23114},
	doi = {10.1002/humu.23114},
	abstract = {As whole genome sequencing becomes cheaper and faster, it will progressively substitute targeted next-generation sequencing as standard practice in research and diagnostics. However, computing cost-performance ratio is not advancing at an equivalent rate. Therefore, it is essential to evaluate the robustness of the variant detection process taking into account the computing resources required. We have benchmarked six combinations of state-of-the-art read aligners (BWA-MEM and GEM3) and variant callers (FreeBayes, GATK HaplotypeCaller, SAMtools) on whole genome and whole exome sequencing data from the NA12878 human sample. Results have been compared between them and against the NIST Genome in a Bottle (GIAB) variants reference dataset. We report differences in speed of up to 20 times in some steps of the process and have observed that SNV, and to a lesser extent InDel, detection is highly consistent in 70\% of the genome. SNV, and especially InDel, detection is less reliable in 20\% of the genome, and almost unfeasible in the remaining 10\%. These findings will aid in choosing the appropriate tools bearing in mind objectives, workload, and computing infrastructure available. {\textbackslash}copyright 2016 The Authors. **Human Mutation published by Wiley Periodicals, Inc.},
	number = {12},
	urldate = {2017-11-03},
	journal = {Human Mutation},
	author = {Laurie, Steve and Fernandez-Callejo, Marcos and Marco-Sola, Santiago and Trotta, Jean-Remi and Camps, Jordi and Chacón, Alejandro and Espinosa, Antonio and Gut, Marta and Gut, Ivo and Heath, Simon and Beltran, Sergi},
	month = sep,
	year = {2016},
	pmid = {27604516},
	pmcid = {PMC5129537},
	pages = {1263--1271},
}

@article{schubert_immunonodes_2017,
	title = {{ImmunoNodes} - graphical development of complex immunoinformatics workflows.},
	volume = {18},
	url = {http://dx.doi.org/10.1186/s12859-017-1667-z},
	doi = {10.1186/s12859-017-1667-z},
	abstract = {BACKGROUND: Immunoinformatics has become a crucial part in biomedical research. Yet many immunoinformatics tools have command line interfaces only and can be difficult to install. Web-based immunoinformatics tools, on the other hand, are difficult to integrate with other tools, which is typically required for the complex analysis and prediction pipelines required for advanced applications. RESULT: We present ImmunoNodes, an immunoinformatics toolbox that is fully integrated into the visual workflow environment KNIME. By dragging and dropping tools and connecting them to indicate the data flow through the pipeline, it is possible to construct very complex workflows without the need for coding. CONCLUSION: ImmunoNodes allows users to build complex workflows with an easy to use and intuitive interface with a few clicks on any desktop computer.},
	number = {1},
	urldate = {2017-11-03},
	journal = {BMC Bioinformatics},
	author = {Schubert, Benjamin and de la Garza, Luis and Mohr, Christopher and Walzer, Mathias and Kohlbacher, Oliver},
	month = may,
	year = {2017},
	pmid = {28482806},
	pmcid = {PMC5422934},
	pages = {242},
}

@article{hastreiter_knime4ngs_2017,
	title = {{KNIME4NGS}: a comprehensive toolbox for {Next} {Generation} {Sequencing} analysis.},
	url = {http://dx.doi.org/10.1093/bioinformatics/btx003},
	doi = {10.1093/bioinformatics/btx003},
	abstract = {Analysis of Next Generation Sequencing (NGS) data requires the processing of large datasets by chaining various tools with complex input and output formats. In order to automate data analysis, we propose to standardize NGS tasks into modular workflows. This simplifies reliable handling and processing of NGS data, and corresponding solutions become substantially more reproducible and easier to maintain. Here, we present a documented, linux-based, toolbox of 42 processing modules that are combined to construct workflows facilitating a variety of tasks such as DNAseq and RNAseq analysis. We also describe important technical extensions. The high throughput executor (HTE) helps to increase the reliability and to reduce manual interventions when processing complex datasets. We also provide a dedicated binary manager that assists users in obtaining the modules' executables and keeping them up to date. As basis for this actively developed toolbox we use the workflow management software KNIME. AVAILABILITY: See http://ibisngs.github.io/knime4ngs for nodes and user manual (GPLv3 license) CONTACT: robert.kueffner@helmholtz-muenchen.de. {\textbackslash}copyright The Author (2017). Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.},
	urldate = {2017-11-03},
	journal = {Bioinformatics},
	author = {Hastreiter, Maximilian and Jeske, Tim and Hoser, Jonathan and Kluge, Michael and Ahomaa, Kaarin and Friedl, Marie-Sophie and Kopetzky, Sebastian J and Quell, Jan-Dominik and Werner Mewes, H- and Küffner, Robert},
	month = jan,
	year = {2017},
	pmid = {28069593},
}

@article{amadio_portage_2016,
	title = {Portage: {Bringing} {Hackers}' {Wisdom} to {Science}},
	volume = {abs/1610.02742},
	url = {http://arxiv.org/abs/1610.02742},
	urldate = {2017-11-03},
	journal = {CoRR},
	author = {Amadio, Guilherme and Xu, Benda},
	year = {2016},
	keywords = {⛔ No DOI found},
}

@phdthesis{kalas_efforts_2015,
	type = {{THESIS}.{DOCTORAL}},
	title = {Efforts towards accessible and reliable bioinformatics},
	url = {http://hdl.handle.net/1956/10658},
	urldate = {2017-11-03},
	school = {University of Bergen, Norway},
	author = {Kalaš, Matúš},
	month = nov,
	year = {2015},
}

@article{kent_assembly_2001,
	title = {Assembly of the working draft of the human genome with {GigAssembler}.},
	volume = {11},
	issn = {1088-9051},
	url = {http://dx.doi.org/10.1101/gr.183201},
	doi = {10.1101/gr.183201},
	abstract = {The data for the public working draft of the human genome contains roughly 400,000 initial sequence contigs in approximately 30,000 large insert clones. Many of these initial sequence contigs overlap. A program, GigAssembler, was built to merge them and to order and orient the resulting larger sequence contigs based on mRNA, paired plasmid ends, EST, BAC end pairs, and other information. This program produced the first publicly available assembly of the human genome, a working draft containing roughly 2.7 billion base pairs and covering an estimated 88\% of the genome that has been used for several recent studies of the genome. Here we describe the algorithm used by GigAssembler.},
	number = {9},
	urldate = {2017-11-03},
	journal = {Genome Research},
	author = {Kent, W J and Haussler, D},
	month = sep,
	year = {2001},
	pmid = {11544197},
	pmcid = {PMC311095},
	pages = {1541--1548},
}

@article{yung_large-scale_2017,
	title = {Large-{Scale} {Uniform} {Analysis} of {Cancer} {Whole} {Genomes} in {Multiple} {Computing}},
	url = {http://www.biorxiv.org/content/early/2017/07/10/161638.abstract},
	abstract = {The International Cancer Genome Consortium (ICGC) 9s Pan-Cancer Analysis},
	urldate = {2017-11-03},
	journal = {bioRxiv},
	author = {Yung, C K and O'Connor, B D and Yakneen, S and Zhang, J and Ellrott, K and others},
	year = {2017},
	keywords = {⛔ No DOI found},
}

@article{van_neste_forensic_2015,
	title = {Forensic massively parallel sequencing data analysis tool: {Implementation} of {MyFLq} as a standalone web- and {Illumina} {BaseSpace}(®)-application.},
	volume = {15},
	url = {http://dx.doi.org/10.1016/j.fsigen.2014.10.006},
	doi = {10.1016/j.fsigen.2014.10.006},
	abstract = {Routine use of massively parallel sequencing (MPS) for forensic genomics is on the horizon. The last few years, several algorithms and workflows have been developed to analyze forensic MPS data. However, none have yet been tailored to the needs of the forensic analyst who does not possess an extensive bioinformatics background. We developed our previously published forensic MPS data analysis framework MyFLq (My-Forensic-Loci-queries) into an open-source, user-friendly, web-based application. It can be installed as a standalone web application, or run directly from the Illumina BaseSpace environment. In the former, laboratories can keep their data on-site, while in the latter, data from forensic samples that are sequenced on an Illumina sequencer can be uploaded to Basespace during acquisition, and can subsequently be analyzed using the published MyFLq BaseSpace application. Additional features were implemented such as an interactive graphical report of the results, an interactive threshold selection bar, and an allele length-based analysis in addition to the sequenced-based analysis. Practical use of the application is demonstrated through the analysis of four 16-plex short tandem repeat (STR) samples, showing the complementarity between the sequence- and length-based analysis of the same MPS data. Copyright {\textbackslash}copyright 2014 The Authors. Published by Elsevier Ireland Ltd.. All rights reserved.},
	urldate = {2017-11-03},
	journal = {Forensic science international. Genetics},
	author = {Van Neste, Christophe and Gansemans, Yannick and De Coninck, Dieter and Van Hoofstat, David and Van Criekinge, Wim and Deforce, Dieter and Van Nieuwerburgh, Filip},
	month = mar,
	year = {2015},
	pmid = {25457631},
	pages = {2--7},
}

@article{marcus_credibility_2015,
	title = {Credibility and reproducibility.},
	volume = {22},
	url = {http://dx.doi.org/10.1016/j.chembiol.2014.12.008},
	doi = {10.1016/j.chembiol.2014.12.008},
	number = {1},
	urldate = {2017-11-03},
	journal = {Chemistry \& Biology},
	author = {Marcus, Emilie},
	month = jan,
	year = {2015},
	pmid = {25615949},
	pages = {3--4},
}

@book{nedeljkovic_cwl-svg_nodate,
	title = {{CWL}-svg: an open-source workflow visualization library for the {Common}},
	url = {https://github.com/rabix/cwl-svg},
	urldate = {2017-11-03},
	author = {Nedeljkovic, Maja and Pajic, Boban and Batic, Ivan and Sharma, Adrian and Kaushik, Gaurav and Davis-Dusenbery, Brandi},
}

@article{amstutz_portable_2015,
	title = {Portable, {Reproducible} {Analysis} with {Arvados}},
	volume = {4},
	urldate = {2017-09-07},
	journal = {F1000Research},
	author = {Amstutz, Peter},
	month = jul,
	year = {2015},
	keywords = {⛔ No DOI found},
}

@article{xu_fdas_2016,
	title = {The {FDA}'s {Experience} with {Emerging} {Genomics} {Technologies}-{Past}, {Present}, and {Future}.},
	volume = {18},
	url = {http://dx.doi.org/10.1208/s12248-016-9917-y},
	doi = {10.1208/s12248-016-9917-y},
	abstract = {The rapid advancement of emerging genomics technologies and their application for assessing safety and efficacy of FDA-regulated products require a high standard of reliability and robustness supporting regulatory decision-making in the FDA. To facilitate the regulatory application, the FDA implemented a novel data submission program, Voluntary Genomics Data Submission (VGDS), and also to engage the stakeholders. As part of the endeavor, for the past 10 years, the FDA has led an international consortium of regulatory agencies, academia, pharmaceutical companies, and genomics platform providers, which was named MicroArray Quality Control Consortium (MAQC), to address issues such as reproducibility, precision, specificity/sensitivity, and data interpretation. Three projects have been completed so far assessing these genomics technologies: gene expression microarrays, whole genome genotyping arrays, and whole transcriptome sequencing (i.e., RNA-seq). The resultant studies provide the basic parameters for fit-for-purpose application of these new data streams in regulatory environments, and the solutions have been made available to the public through peer-reviewed publications. The latest MAQC project is also called the SEquencing Quality Control (SEQC) project focused on next-generation sequencing. Using reference samples with built-in controls, SEQC studies have demonstrated that relative gene expression can be measured accurately and reliably across laboratories and RNA-seq platforms. Besides prediction performance comparable to microarrays in clinical settings and safety assessments, RNA-seq is shown to have better sensitivity for low expression and reveal novel transcriptomic features. Future effort of MAQC will be focused on quality control of whole genome sequencing and targeted sequencing.},
	number = {4},
	urldate = {2017-11-03},
	journal = {The AAPS Journal},
	author = {Xu, Joshua and Thakkar, Shraddha and Gong, Binsheng and Tong, Weida},
	month = apr,
	year = {2016},
	pmid = {27116022},
	pmcid = {PMC4973466},
	pages = {814--818},
}

@book{simonovic_rabix_nodate,
	title = {Rabix {Executor}: an open-source executor supporting the recomputability and},
	url = {https://github.com/rabix/bunny},
	urldate = {2017-11-03},
	author = {Simonovic, Janko and Kaushik, Gaurav and Ivkovic, Sinisa and Stojanovic, Luka and Tijanic, Nebojsa and Sharma, Adrian and Davis-Dusenbery, Brandi},
}

@book{amstutz_common-workflow-languagecwltool_2017,
	address = {GitHub},
	title = {common-workflow-language/cwltool},
	url = {https://github.com/common-workflow-language/cwltool/releases/tag/1.0.20170828135420},
	urldate = {2017-08-28},
	publisher = {GitHub},
	author = {Amstutz, Peter and Crusoe, Michael R and Singh, Manvendra and Kumar, Kapil and Chilton, John and Boysha and Soiland-Reyes, Stian and Chapman, Brad and Kotliar, Michael and Leehr, Dan and Carrasco, Guillermo and Kartashov, Andrey and Tijanic, Nebojsa and Ménager, Hervé and Safont, Pau Ruiz and Porter, James J and Molenaar, Gijs and Yuen, Denis and Barrera, Alejandro and Ivkovic, Sinisa and Spangler, Ryan and psaffrey-illumina and Tanjo, Tomoya and Vandewege, Ward and Randall, Joshua C and Kern, John and Bradley, John and Li, Jiayong and der Zwaan, Janneke van and Connelly, Abram},
	year = {2017},
}

@book{preston-werner_semantic_2013,
	title = {Semantic {Versioning} 2.0.0},
	url = {http://semver.org/spec/v2.0.0.html},
	urldate = {2017-09-07},
	author = {Preston-Werner, Tom},
	month = jun,
	year = {2013},
}

@book{noauthor_stellaris_nodate,
	title = {Stellaris on {Steam}},
	url = {http://store.steampowered.com/app/281990/Stellaris/},
	abstract = {Explore a vast galaxy full of wonder! Paradox Development Studio, makers},
	urldate = {2017-09-07},
}

@book{ssi_top_2016,
	title = {Top tips},
	url = {https://www.software.ac.uk/resources/top-tips},
	urldate = {2017-09-07},
	author = {{\textbackslash}SSI},
	year = {2016},
}

@article{amstutz_using_2016,
	title = {Using the {Common} {Workflow} {Language} ({CWL}) to run portable workflows with},
	volume = {5},
	url = {http://dx.doi.org/10.7490/f1000research.1112523.1},
	abstract = {Read this work by Amstutz P, at F1000Research.},
	urldate = {2017-09-07},
	journal = {F1000Res.},
	author = {Amstutz, Peter and Zaranek, Alexander Wait and O'Connor, Brian and Paten, Benedict},
	month = jul,
	year = {2016},
	keywords = {⛔ No DOI found},
}

@article{kanwal_investigating_2017,
	title = {Investigating reproducibility and tracking provenance - {A} genomic workflow case study.},
	volume = {18},
	url = {http://dx.doi.org/10.1186/s12859-017-1747-0},
	doi = {10.1186/s12859-017-1747-0},
	abstract = {BACKGROUND: Computational bioinformatics workflows are extensively used to analyse genomics data, with different approaches available to support implementation and execution of these workflows. Reproducibility is one of the core principles for any scientific workflow and remains a challenge, which is not fully addressed. This is due to incomplete understanding of reproducibility requirements and assumptions of workflow definition approaches. Provenance information should be tracked and used to capture all these requirements supporting reusability of existing workflows. RESULTS: We have implemented a complex but widely deployed bioinformatics workflow using three representative approaches to workflow definition and execution. Through implementation, we identified assumptions implicit in these approaches that ultimately produce insufficient documentation of workflow requirements resulting in failed execution of the workflow. This study proposes a set of recommendations that aims to mitigate these assumptions and guides the scientific community to accomplish reproducible science, hence addressing reproducibility crisis. CONCLUSIONS: Reproducing, adapting or even repeating a bioinformatics workflow in any environment requires substantial technical knowledge of the workflow execution environment, resolving analysis assumptions and rigorous compliance with reproducibility requirements. Towards these goals, we propose conclusive recommendations that along with an explicit declaration of workflow specification would result in enhanced reproducibility of computational genomic analyses.},
	number = {1},
	urldate = {2017-11-03},
	journal = {BMC Bioinformatics},
	author = {Kanwal, Sehrish and Khan, Farah Zaib and Lonie, Andrew and Sinnott, Richard O},
	month = jul,
	year = {2017},
	pmid = {28701218},
	pmcid = {PMC5508699},
	pages = {337},
}

@book{van_der_zwaan_flexible_nodate,
	title = {Flexible {NLP} {Pipelines} for {Digital} {Humanities} {Research}},
	url = {https://dh2017.adho.org/abstracts/215/215.pdf},
	urldate = {2017-11-03},
	author = {van der Zwaan, Janneke M and Smink, Wouter and Sools, Anneke and Westerhof, Gerben and Veldkamp, Bernard and Wiegersma, Sytske},
}

@book{batic_rabix_nodate,
	title = {Rabix {Composer}: an open-source integrated development environment for the},
	url = {https://github.com/rabix/composer},
	urldate = {2017-11-03},
	author = {Batic, Ivan and Nedeljkovic, Maja and Pajic, Boban and Tijanic, Nebojsa and Stojanovic, Luka and Lekic, Marijan and Sharma, Adrian and Kaushik, Gaurav and Davis-Dusenbery, Brandi Davis-Dusenbery},
}

@article{prins_developing_2015,
	title = {Developing an {Arvados} {BWA}-{GATK} pipeline},
	volume = {4},
	url = {http://dx.doi.org/10.7490/f1000research.1110026.1},
	abstract = {Read this work by Prins P, at F1000Research.},
	urldate = {2017-09-07},
	journal = {F1000Res.},
	author = {Prins, Pjotr and de Ligt, Joep and Nijman, Isaac J and Amstutz, Peter and Cosca, Bryan and Vandewege, Ward},
	month = jul,
	year = {2015},
	keywords = {⛔ No DOI found},
}

@article{williams_running_2014,
	title = {Running {Taverna} {Workflows} within {IPython} {Notebook}},
	url = {https://zenodo.org/record/11360#.WcokBsiGNPY},
	urldate = {2017-09-26},
	author = {Williams and Pawlik and Goble},
	month = jul,
	year = {2014},
	keywords = {⛔ No DOI found},
}

@book{robinson_common-workflow-languagecwlviewer_2017,
	title = {common-workflow-language/cwlviewer: {CWL} {Viewer}},
	url = {https://doi.org/10.5281/zenodo.848163},
	urldate = {2017-11-03},
	author = {Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R},
	month = aug,
	year = {2017},
}

@article{fjukstad_review_2017,
	title = {A review of scalable bioinformatics pipelines},
	volume = {2},
	issn = {2364-1185},
	url = {http://link.springer.com/10.1007/s41019-017-0047-z},
	doi = {10.1007/s41019-017-0047-z},
	abstract = {Scalability is increasingly important for bioinformatics analysis services, since these must handle larger datasets, more jobs, and more users. The pipelines used to implement analyses must therefore scale with respect to the resources on a single compute node, the number of nodes on a cluster, and also to cost-performance. Here, we survey several scalable bioinformatics pipelines and compare their design and their use of underlying frameworks and infrastructures. We also discuss current trends for bioinformatics pipeline development.},
	number = {3},
	urldate = {2017-10-30},
	journal = {Data Science and Engineering},
	author = {Fjukstad, Bjørn and Bongo, Lars Ailo},
	month = oct,
	year = {2017},
	pages = {245--251},
}

@article{schulz_use_2016,
	title = {Use of application containers and workflows for genomic data analysis.},
	volume = {7},
	url = {http://dx.doi.org/10.4103/2153-3539.197197},
	doi = {10.4103/2153-3539.197197},
	abstract = {BACKGROUND: The rapid acquisition of biological data and development of computationally intensive analyses has led to a need for novel approaches to software deployment. In particular, the complexity of common analytic tools for genomics makes them difficult to deploy and decreases the reproducibility of computational experiments. METHODS: Recent technologies that allow for application virtualization, such as Docker, allow developers and bioinformaticians to isolate these applications and deploy secure, scalable platforms that have the potential to dramatically increase the efficiency of big data processing. RESULTS: While limitations exist, this study demonstrates a successful implementation of a pipeline with several discrete software applications for the analysis of next-generation sequencing (NGS) data. CONCLUSIONS: With this approach, we significantly reduced the amount of time needed to perform clonal analysis from NGS data in acute myeloid leukemia.},
	urldate = {2017-11-03},
	journal = {Journal of pathology informatics},
	author = {Schulz, Wade L and Durant, Thomas J S and Siddon, Alexa J and Torres, Richard},
	month = dec,
	year = {2016},
	pmid = {28163975},
	pmcid = {PMC5248400},
	pages = {53},
}

@book{robinson_reproducible_2017,
	title = {Reproducible {Research} using {Research} {Objects}},
	url = {https://doi.org/10.5281/zenodo.823295},
	abstract = {Scientific workflows play a crucial role in conducting large scale scientific experiments and enabling them to be easily reproducible. However, it is vital that these are specified well with useful metadata for consumption by applications in order to make sharing them simple and convenient.This project focuses on two up-and-coming standards - the Common Workflow Language and Research Objects - which complement each other to achieve this goal and thus aid in reproducible and transparent research. In order to unite these standards, this report concerns the planning, development and evaluation process of a web application, ‘CWL Viewer’, to allow the sharing of workflows written in the Common Workflow Language.This is accomplished by providing visualisation and summary of important details as well as a downloadable Research Object Bundle designed to provide metadata relevant to workflow management and other applications.},
	urldate = {2017-09-18},
	publisher = {The University of Manchester},
	author = {Robinson, Mark},
	month = may,
	year = {2017},
	keywords = {cwl},
}

@article{yakneen_enabling_2017,
	title = {Enabling rapid cloud-based analysis of thousands of human genomes via {Butler}},
	url = {http://biorxiv.org/lookup/doi/10.1101/185736},
	doi = {10.1101/185736},
	abstract = {We present Butler, a computational framework developed in the context of the international Pan-cancer Analysis of Whole Genomes (PCAWG) project to overcome the challenges of orchestrating analyses of thousands of human genomes on the cloud. Butler operates equally well on public and academic clouds. This highly flexible framework facilitates management of virtual cloud infrastructure, software configuration, genomics workflow development, and provides unique capabilities in workflow execution management. By comprehensively collecting and analysing metrics and logs, performing anomaly detection as well as notification and cluster self-healing, Butler enables large-scale analytical processing of human genomes with 43\% increased throughput compared to prior setups. Butler was key for delivering the germline genetic variant call-sets in 2,834 cancer genomes analysed by PCAWG.},
	urldate = {2017-09-08},
	journal = {BioRxiv},
	author = {Yakneen, Sergei and Waszak, Sebastian and Gertz, Michael and Korbel, Jan O. and Group, PCAWG Germline Cancer Genome Working and Group, PCAWG Technical Working and Net, ICGC/TCGA Pan-Cancer Analysis of Whole Genomes},
	month = sep,
	year = {2017},
}

@article{cohen-boulakia_scientific_2017,
	title = {Scientific workflows for computational reproducibility in the life sciences: {Status}, challenges and opportunities},
	volume = {75},
	issn = {0167739X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X17300316},
	doi = {10.1016/j.future.2017.01.012},
	abstract = {With the development of new experimental technologies, biologists are faced with an avalanche of data to be computationally analyzed for scientific advancements and discoveries to emerge. Faced with the complexity of analysis pipelines, the large number of computational tools, and the enormous amount of data to manage, there is compelling evidence that many if not most scientific discoveries will not stand the test of time: increasing the reproducibility of computed results is of paramount importance. The objective we set out in this paper is to place scientific workflows in the context of reproducibility. To do so, we define several kinds of repro-ducibility that can be reached when scientific workflows are used to perform experiments. We characterize and define the criteria that need to be catered for by reproducibility-friendly scientific workflow systems, and use such criteria to place several representative and widely used workflow systems and companion tools within such a framework. We also discuss the remaining challenges posed by reproducible scientific workflows in the life sciences. Our study was guided by three use cases from the life science domain involving in silico experiments.},
	urldate = {2018-07-13},
	journal = {Future Generation Computer Systems},
	author = {Cohen-Boulakia, Sarah and Belhajjame, Khalid and Collin, Olivier and Chopard, Jérôme and Froidevaux, Christine and Gaignard, Alban and Hinsen, Konrad and Larmande, Pierre and Bras, Yvan Le and Lemoine, Frédéric and Mareuil, Fabien and Ménager, Hervé and Pradal, Christophe and Blanchet, Christophe},
	month = oct,
	year = {2017},
	pages = {284--298},
}

@article{jongeneel_assessing_2017,
	title = {Assessing computational genomics skills: {Our} experience in the {H3ABioNet} {African} bioinformatics network.},
	volume = {13},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1005419},
	doi = {10.1371/journal.pcbi.1005419},
	abstract = {The H3ABioNet pan-African bioinformatics network, which is funded to support the Human Heredity and Health in Africa (H3Africa) program, has developed node-assessment exercises to gauge the ability of its participating research and service groups to analyze typical genome-wide datasets being generated by H3Africa research groups. We describe a framework for the assessment of computational genomics analysis skills, which includes standard operating procedures, training and test datasets, and a process for administering the exercise. We present the experiences of 3 research groups that have taken the exercise and the impact on their ability to manage complex projects. Finally, we discuss the reasons why many H3ABioNet nodes have declined so far to participate and potential strategies to encourage them to do so.},
	number = {6},
	urldate = {2017-11-03},
	journal = {PLoS Computational Biology},
	author = {Jongeneel, C Victor and Achinike-Oduaran, Ovokeraye and Adebiyi, Ezekiel and Adebiyi, Marion and Adeyemi, Seun and Akanle, Bola and Aron, Shaun and Ashano, Efejiro and Bendou, Hocine and Botha, Gerrit and Chimusa, Emile and Choudhury, Ananyo and Donthu, Ravikiran and Drnevich, Jenny and Falola, Oluwadamila and Fields, Christopher J and Hazelhurst, Scott and Hendry, Liesl and Isewon, Itunuoluwa and Khetani, Radhika S and Kumuthini, Judit and Kimuda, Magambo Phillip and Magosi, Lerato and Mainzer, Liudmila Sergeevna and Maslamoney, Suresh and Mbiyavanga, Mamana and Meintjes, Ayton and Mugutso, Danny and Mpangase, Phelelani and Munthali, Richard and Nembaware, Victoria and Ndhlovu, Andrew and Odia, Trust and Okafor, Adaobi and Oladipo, Olaleye and Panji, Sumir and Pillay, Venesa and Rendon, Gloria and Sengupta, Dhriti and Mulder, Nicola},
	month = jun,
	year = {2017},
	pmid = {28570565},
	pmcid = {PMC5453403},
	pages = {e1005419},
}

@article{moller_community-driven_2014,
	title = {Community-driven development for computational biology at {Sprints}, {Hackathons} and {Codefests}.},
	volume = {15 Suppl 14},
	url = {http://dx.doi.org/10.1186/1471-2105-15-S14-S7},
	doi = {10.1186/1471-2105-15-S14-S7},
	abstract = {BACKGROUND: Computational biology comprises a wide range of technologies and approaches. Multiple technologies can be combined to create more powerful workflows if the individuals contributing the data or providing tools for its interpretation can find mutual understanding and consensus. Much conversation and joint investigation are required in order to identify and implement the best approaches. Traditionally, scientific conferences feature talks presenting novel technologies or insights, followed up by informal discussions during coffee breaks. In multi-institution collaborations, in order to reach agreement on implementation details or to transfer deeper insights in a technology and practical skills, a representative of one group typically visits the other. However, this does not scale well when the number of technologies or research groups is large. Conferences have responded to this issue by introducing Birds-of-a-Feather (BoF) sessions, which offer an opportunity for individuals with common interests to intensify their interaction. However, parallel BoF sessions often make it hard for participants to join multiple BoFs and find common ground between the different technologies, and BoFs are generally too short to allow time for participants to program together. RESULTS: This report summarises our experience with computational biology Codefests, Hackathons and Sprints, which are interactive developer meetings. They are structured to reduce the limitations of traditional scientific meetings described above by strengthening the interaction among peers and letting the participants determine the schedule and topics. These meetings are commonly run as loosely scheduled "unconferences" (self-organized identification of participants and topics for meetings) over at least two days, with early introductory talks to welcome and organize contributors, followed by intensive collaborative coding sessions. We summarise some prominent achievements of those meetings and describe differences in how these are organised, how their audience is addressed, and their outreach to their respective communities. CONCLUSIONS: Hackathons, Codefests and Sprints share a stimulating atmosphere that encourages participants to jointly brainstorm and tackle problems of shared interest in a self-driven proactive environment, as well as providing an opportunity for new participants to get involved in collaborative projects.},
	urldate = {2017-08-16},
	journal = {BMC Bioinformatics},
	author = {Möller, Steffen and Afgan, Enis and Banck, Michael and Bonnal, Raoul J P and Booth, Timothy and Chilton, John and Cock, Peter J A and Gumbel, Markus and Harris, Nomi and Holland, Richard and Kalaš, Matúš and Kaján, László and Kibukawa, Eri and Powel, David R and Prins, Pjotr and Quinn, Jacqueline and Sallou, Olivier and Strozzi, Francesco and Seemann, Torsten and Sloggett, Clare and Soiland-Reyes, Stian and Spooner, William and Steinbiss, Sascha and Tille, Andreas and Travis, Anthony J and Guimera, Roman and Katayama, Toshiaki and Chapman, Brad A},
	month = nov,
	year = {2014},
	pmid = {25472764},
	pmcid = {PMC4255748},
	pages = {S7},
}

@article{kurtzer_singularity_2017,
	title = {Singularity: {Scientific} containers for mobility of compute.},
	volume = {12},
	url = {http://dx.doi.org/10.1371/journal.pone.0177459},
	doi = {10.1371/journal.pone.0177459},
	abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
	number = {5},
	urldate = {2017-11-03},
	journal = {Plos One},
	author = {Kurtzer, Gregory M and Sochat, Vanessa and Bauer, Michael W},
	month = may,
	year = {2017},
	pmid = {28494014},
	pmcid = {PMC5426675},
	pages = {e0177459},
}

@article{guimera_bcbio-nextgen_2012,
	title = {bcbio-nextgen: {Automated}, distributed next-gen sequencing pipeline},
	volume = {17},
	issn = {2226-6089},
	url = {http://journal.embnet.org/index.php/embnetjournal/article/view/286},
	doi = {10.14806/ej.17.B.286},
	number = {B},
	urldate = {2017-09-07},
	journal = {EMBnet.journal},
	author = {Guimera, Roman Valls},
	month = feb,
	year = {2012},
	pages = {30},
}

@article{taschuk_ten_2017,
	title = {Ten simple rules for making research software more robust.},
	volume = {13},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1005412},
	doi = {10.1371/journal.pcbi.1005412},
	abstract = {Software produced for research, published and otherwise, suffers from a number of common problems that make it difficult or impossible to run outside the original institution or even off the primary developer's computer. We present ten simple rules to make such software robust enough to be run by anyone, anywhere, and thereby delight your users and collaborators.},
	number = {4},
	urldate = {2017-11-03},
	journal = {PLoS Computational Biology},
	author = {Taschuk, Morgan and Wilson, Greg},
	month = apr,
	year = {2017},
	pmid = {28407023},
	pmcid = {PMC5390961},
	pages = {e1005412},
}

@article{henry_omictools_2014,
	title = {{OMICtools}: an informative directory for multi-omic data analysis.},
	volume = {2014},
	url = {http://dx.doi.org/10.1093/database/bau069},
	doi = {10.1093/database/bau069},
	abstract = {Recent advances in 'omic' technologies have created unprecedented opportunities for biological research, but current software and database resources are extremely fragmented. OMICtools is a manually curated metadatabase that provides an overview of more than 4400 web-accessible tools related to genomics, transcriptomics, proteomics and metabolomics. All tools have been classified by omic technologies (next-generation sequencing, microarray, mass spectrometry and nuclear magnetic resonance) associated with published evaluations of tool performance. Information about each tool is derived either from a diverse set of developers, the scientific literature or from spontaneous submissions. OMICtools is expected to serve as a useful didactic resource not only for bioinformaticians but also for experimental researchers and clinicians. Database URL: http://omictools.com/. {\textbackslash}copyright The Author(s) 2014. Published by Oxford University Press.},
	urldate = {2017-11-03},
	journal = {Database: the Journal of Biological Databases and Curation},
	author = {Henry, Vincent J and Bandrowski, Anita E and Pepin, Anne-Sophie and Gonzalez, Bruno J and Desfeux, Arnaud},
	month = jul,
	year = {2014},
	pmid = {25024350},
	pmcid = {PMC4095679},
}

@article{oconnor_dockstore_2017,
	title = {The {Dockstore}: enabling modular, community-focused sharing of {Docker}-based genomics tools and workflows.},
	volume = {6},
	url = {http://dx.doi.org/10.12688/f1000research.10137.1},
	doi = {10.12688/f1000research.10137.1},
	abstract = {As genomic datasets continue to grow, the feasibility of downloading data to a local organization and running analysis on a traditional compute environment is becoming increasingly problematic. Current large-scale projects, such as the ICGC PanCancer Analysis of Whole Genomes (PCAWG), the Data Platform for the U.S. Precision Medicine Initiative, and the NIH Big Data to Knowledge Center for Translational Genomics, are using cloud-based infrastructure to both host and perform analysis across large data sets. In PCAWG, over 5,800 whole human genomes were aligned and variant called across 14 cloud and HPC environments; the processed data was then made available on the cloud for further analysis and sharing. If run locally, an operation at this scale would have monopolized a typical academic data centre for many months, and would have presented major challenges for data storage and distribution. However, this scale is increasingly typical for genomics projects and necessitates a rethink of how analytical tools are packaged and moved to the data. For PCAWG, we embraced the use of highly portable Docker images for encapsulating and sharing complex alignment and variant calling workflows across highly variable environments. While successful, this endeavor revealed a limitation in Docker containers, namely the lack of a standardized way to describe and execute the tools encapsulated inside the container. As a result, we created the Dockstore ( https://dockstore.org), a project that brings together Docker images with standardized, machine-readable ways of describing and running the tools contained within. This service greatly improves the sharing and reuse of genomics tools and promotes interoperability with similar projects through emerging web service standards developed by the Global Alliance for Genomics and Health (GA4GH).},
	urldate = {2018-07-13},
	journal = {F1000Research},
	author = {O'Connor, Brian D and Yuen, Denis and Chung, Vincent and Duncan, Andrew G and Liu, Xiang Kun and Patricia, Janice and Paten, Benedict and Stein, Lincoln and Ferretti, Vincent},
	month = jan,
	year = {2017},
	pmid = {28344774},
	pmcid = {PMC5333608},
	pages = {52},
}

@article{di_tommaso_nextflow_2017,
	title = {Nextflow enables reproducible computational workflows.},
	volume = {35},
	url = {http://dx.doi.org/10.1038/nbt.3820},
	doi = {10.1038/nbt.3820},
	number = {4},
	urldate = {2018-07-13},
	journal = {Nature Biotechnology},
	author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
	month = apr,
	year = {2017},
	pmid = {28398311},
	pages = {316--319},
}

@article{vivian_toil_2017,
	title = {Toil enables reproducible, open source, big biomedical data analyses.},
	volume = {35},
	url = {http://dx.doi.org/10.1038/nbt.3772},
	doi = {10.1038/nbt.3772},
	number = {4},
	urldate = {2017-11-03},
	journal = {Nature Biotechnology},
	author = {Vivian, John and Rao, Arjun Arkal and Nothaft, Frank Austin and Ketchum, Christopher and Armstrong, Joel and Novak, Adam and Pfeil, Jacob and Narkizian, Jake and Deran, Alden D and Musselman-Brown, Audrey and Schmidt, Hannes and Amstutz, Peter and Craft, Brian and Goldman, Mary and Rosenbloom, Kate and Cline, Melissa and O'Connor, Brian and Hanna, Megan and Birger, Chet and Kent, W James and Patterson, David A and Joseph, Anthony D and Zhu, Jingchun and Zaranek, Sasha and Getz, Gad and Haussler, David and Paten, Benedict},
	month = apr,
	year = {2017},
	pmid = {28398314},
	pmcid = {PMC5546205},
	pages = {314--316},
}

@article{stodden_enhancing_2016,
	title = {Enhancing reproducibility for computational methods.},
	volume = {354},
	issn = {0036-8075},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aah6168},
	doi = {10.1126/science.aah6168},
	number = {6317},
	urldate = {2018-07-23},
	journal = {Science},
	author = {Stodden, Victoria and McNutt, Marcia and Bailey, David H and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A and Ioannidis, John P A and Taufer, Michela},
	month = dec,
	year = {2016},
	pmid = {27940837},
	pages = {1240--1241},
}

@article{artaza_top_2016,
	title = {Top 10 metrics for life science software good practices.},
	volume = {5},
	url = {http://dx.doi.org/10.12688/f1000research.9206.1},
	doi = {10.12688/f1000research.9206.1},
	abstract = {Metrics for assessing adoption of good development practices are a useful way to ensure that software is sustainable, reusable and functional. Sustainability means that the software used today will be available - and continue to be improved and supported - in the future. We report here an initial set of metrics that measure good practices in software development. This initiative differs from previously developed efforts in being a community-driven grassroots approach where experts from different organisations propose good software practices that have reasonable potential to be adopted by the communities they represent. We not only focus our efforts on understanding and prioritising good practices, we assess their feasibility for implementation and publish them here.},
	urldate = {2017-11-03},
	journal = {F1000Research},
	author = {Artaza, Haydee and Chue Hong, Neil and Corpas, Manuel and Corpuz, Angel and Hooft, Rob and Jimenez, Rafael C and Leskošek, Brane and Olivier, Brett G and Stourac, Jan and Svobodová Vařeková, Radka and Van Parys, Thomas and Vaughan, Daniel},
	month = aug,
	year = {2016},
	pmid = {27635232},
	pmcid = {PMC5007752},
}

@article{moreews_bioshadock_2015,
	title = {{BioShaDock}: a community driven bioinformatics shared {Docker}-based tools registry.},
	volume = {4},
	url = {http://dx.doi.org/10.12688/f1000research.7536.1},
	doi = {10.12688/f1000research.7536.1},
	abstract = {Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.},
	urldate = {2017-11-03},
	journal = {F1000Research},
	author = {Moreews, François and Sallou, Olivier and Ménager, Hervé and Le Bras, Yvan and Monjeaud, Cyril and Blanchet, Christophe and Collin, Olivier},
	month = dec,
	year = {2015},
	pmid = {26913191},
	pmcid = {PMC4743153},
	pages = {1443},
}

@article{kaushik_rabix_2017,
	title = {Rabix: an open-source workflow executor supporting recomputability and interoperability of workflow descriptions.},
	volume = {22},
	url = {http://dx.doi.org/10.1142/9789813207813_0016},
	doi = {10.1142/9789813207813_0016},
	abstract = {As biomedical data has become increasingly easy to generate in large quantities, the methods used to analyze it have proliferated rapidly. Reproducible and reusable methods are required to learn from large volumes of data reliably. To address this issue, numerous groups have developed workflow specifications or execution engines, which provide a framework with which to perform a sequence of analyses. One such specification is the Common Workflow Language, an emerging standard which provides a robust and flexible framework for describing data analysis tools and workflows. In addition, reproducibility can be furthered by executors or workflow engines which interpret the specification and enable additional features, such as error logging, file organization, optim1izations to computation and job scheduling, and allow for easy computing on large volumes of data. To this end, we have developed the Rabix Executor, an open-source workflow engine for the purposes of improving reproducibility through reusability and interoperability of workflow descriptions.},
	urldate = {2017-11-03},
	journal = {Pacific Symposium on Biocomputing},
	author = {Kaushik, Gaurav and Ivkovic, Sinisa and Simonovic, Janko and Tijanic, Nebojsa and Davis-Dusenbery, Brandi and Kural, Deniz},
	year = {2017},
	pmid = {27896971},
	pmcid = {PMC5166558},
	pages = {154--165},
}

@article{shanahan_bioinformatics_2014,
	title = {Bioinformatics on the cloud computing platform {Azure}.},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0102642},
	doi = {10.1371/journal.pone.0102642},
	abstract = {We discuss the applicability of the Microsoft cloud computing platform, Azure, for bioinformatics. We focus on the usability of the resource rather than its performance. We provide an example of how R can be used on Azure to analyse a large amount of microarray expression data deposited at the public database ArrayExpress. We provide a walk through to demonstrate explicitly how Azure can be used to perform these analyses in Appendix S1 and we offer a comparison with a local computation. We note that the use of the Platform as a Service (PaaS) offering of Azure can represent a steep learning curve for bioinformatics developers who will usually have a Linux and scripting language background. On the other hand, the presence of an additional set of libraries makes it easier to deploy software in a parallel (scalable) fashion and explicitly manage such a production run with only a few hundred lines of code, most of which can be incorporated from a template. We propose that this environment is best suited for running stable bioinformatics software by users not involved with its development.},
	number = {7},
	urldate = {2017-11-03},
	journal = {Plos One},
	author = {Shanahan, Hugh P and Owen, Anne M and Harrison, Andrew P},
	month = jul,
	year = {2014},
	pmid = {25050811},
	pmcid = {PMC4106841},
	pages = {e102642},
}

@article{irwin_community_2008,
	title = {Community benchmarks for virtual screening.},
	volume = {22},
	url = {http://dx.doi.org/10.1007/s10822-008-9189-4},
	doi = {10.1007/s10822-008-9189-4},
	abstract = {Ligand enrichment among top-ranking hits is a key metric of virtual screening. To avoid bias, decoys should resemble ligands physically, so that enrichment is not attributable to simple differences of gross features. We therefore created a directory of useful decoys (DUD) by selecting decoys that resembled annotated ligands physically but not topologically to benchmark docking performance. DUD has 2950 annotated ligands and 95,316 property-matched decoys for 40 targets. It is by far the largest and most comprehensive public data set for benchmarking virtual screening programs that I am aware of. This paper outlines several ways that DUD can be improved to provide better telemetry to investigators seeking to understand both the strengths and the weaknesses of current docking methods. I also highlight several pitfalls for the unwary: a risk of over-optimization, questions about chemical space, and the proper scope for using DUD. Careful attention to both the composition of benchmarks and how they are used is essential to avoid being misled by overfitting and bias.},
	number = {3-4},
	urldate = {2017-11-03},
	journal = {Journal of Computer-Aided Molecular Design},
	author = {Irwin, John J},
	month = apr,
	year = {2008},
	pmid = {18273555},
	pages = {193--199},
}

@article{perez-riverol_ten_2016,
	title = {Ten simple rules for taking advantage of git and github.},
	volume = {12},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1004947},
	doi = {10.1371/journal.pcbi.1004947},
	number = {7},
	urldate = {2017-11-03},
	journal = {PLoS Computational Biology},
	author = {Perez-Riverol, Yasset and Gatto, Laurent and Wang, Rui and Sachsenberg, Timo and Uszkoreit, Julian and Leprevost, Felipe da Veiga and Fufezan, Christian and Ternent, Tobias and Eglen, Stephen J and Katz, Daniel S and Pollard, Tom J and Konovalov, Alexander and Flight, Robert M and Blin, Kai and Vizcaíno, Juan Antonio},
	month = jul,
	year = {2016},
	pmid = {27415786},
	pmcid = {PMC4945047},
	pages = {e1004947},
}

@incollection{berthold_knime_2008,
	address = {Berlin, Heidelberg},
	series = {Studies in {Classification}, {Data} {Analysis}, and {Knowledge} {Organization}},
	title = {{KNIME}: {The} {Konstanz} {Information} {Miner}},
	isbn = {978-3-540-78239-1},
	url = {http://link.springer.com/10.1007/978-3-540-78246-9_38},
	abstract = {The Konstanz Information Miner is a modular environment, which enables easy visual assembly and interactive execution of a data pipeline. It is designed as a teaching, research and collaboration platform, which enables simple integration of new algorithms and tools as well as data manipulation or visualization methods in the form of new modules or nodes. In this paper we describe some of the design aspects of the underlying architecture and briefly sketch how new nodes can be incorporated.},
	urldate = {2017-11-03},
	booktitle = {Data {Analysis}, {Machine} {Learning} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Berthold, Michael R. and Cebron, Nicolas and Dill, Fabian and Gabriel, Thomas R. and Kötter, Tobias and Meinl, Thorsten and Ohl, Peter and Sieb, Christoph and Thiel, Kilian and Wiswedel, Bernd},
	editor = {Preisach, Christine and Burkhardt, Hans and Schmidt-Thieme, Lars and Decker, Reinhold},
	year = {2008},
	doi = {10.1007/978-3-540-78246-9_38},
	pages = {319--326},
}

@article{moller_community-driven_2010,
	title = {Community-driven computational biology with {Debian} {Linux}.},
	volume = {11 Suppl 12},
	url = {http://dx.doi.org/10.1186/1471-2105-11-S12-S5},
	doi = {10.1186/1471-2105-11-S12-S5},
	abstract = {BACKGROUND: The Open Source movement and its technologies are popular in the bioinformatics community because they provide freely available tools and resources for research. In order to feed the steady demand for updates on software and associated data, a service infrastructure is required for sharing and providing these tools to heterogeneous computing environments. RESULTS: The Debian Med initiative provides ready and coherent software packages for medical informatics and bioinformatics. These packages can be used together in Taverna workflows via the UseCase plugin to manage execution on local or remote machines. If such packages are available in cloud computing environments, the underlying hardware and the analysis pipelines can be shared along with the software. CONCLUSIONS: Debian Med closes the gap between developers and users. It provides a simple method for offering new releases of software and data resources, thus provisioning a local infrastructure for computational biology. For geographically distributed teams it can ensure they are working on the same versions of tools, in the same conditions. This contributes to the world-wide networking of researchers.},
	urldate = {2017-11-03},
	journal = {BMC Bioinformatics},
	author = {Möller, Steffen and Krabbenhöft, Hajo Nils and Tille, Andreas and Paleino, David and Williams, Alan and Wolstencroft, Katy and Goble, Carole and Holland, Richard and Belhachemi, Dominique and Plessy, Charles},
	month = dec,
	year = {2010},
	pmid = {21210984},
	pmcid = {PMC3040531},
	pages = {S5},
}

@article{afgan_galaxy_2016,
	title = {The {Galaxy} platform for accessible, reproducible and collaborative biomedical analyses: 2016 update.},
	volume = {44},
	url = {http://dx.doi.org/10.1093/nar/gkw343},
	doi = {10.1093/nar/gkw343},
	abstract = {High-throughput data production technologies, particularly 'next-generation' DNA sequencing, have ushered in widespread and disruptive changes to biomedical research. Making sense of the large datasets produced by these technologies requires sophisticated statistical and computational methods, as well as substantial computational power. This has led to an acute crisis in life sciences, as researchers without informatics training attempt to perform computation-dependent analyses. Since 2005, the Galaxy project has worked to address this problem by providing a framework that makes advanced computational tools usable by non experts. Galaxy seeks to make data-intensive research more accessible, transparent and reproducible by providing a Web-based environment in which users can perform computational analyses and have all of the details automatically tracked for later inspection, publication, or reuse. In this report we highlight recently added features enabling biomedical analyses on a large scale. {\textbackslash}copyright The Author(s) 2016. Published by Oxford University Press on behalf of Nucleic Acids Research.},
	number = {W1},
	urldate = {2017-11-03},
	journal = {Nucleic Acids Research},
	author = {Afgan, Enis and Baker, Dannon and van den Beek, Marius and Blankenberg, Daniel and Bouvier, Dave and Čech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Eberhard, Carl and Grüning, Björn and Guerler, Aysam and Hillman-Jackson, Jennifer and Von Kuster, Greg and Rasche, Eric and Soranzo, Nicola and Turaga, Nitesh and Taylor, James and Nekrutenko, Anton and Goecks, Jeremy},
	month = jul,
	year = {2016},
	pmid = {27137889},
	pmcid = {PMC4987906},
	pages = {W3--W10},
}

@article{ison_edam_2013,
	title = {{EDAM}: an ontology of bioinformatics operations, types of data and identifiers, topics and formats.},
	volume = {29},
	url = {http://dx.doi.org/10.1093/bioinformatics/btt113},
	doi = {10.1093/bioinformatics/btt113},
	abstract = {MOTIVATION: Advancing the search, publication and integration of bioinformatics tools and resources demands consistent machine-understandable descriptions. A comprehensive ontology allowing such descriptions is therefore required. RESULTS: EDAM is an ontology of bioinformatics operations (tool or workflow functions), types of data and identifiers, application domains and data formats. EDAM supports semantic annotation of diverse entities such as Web services, databases, programmatic libraries, standalone tools, interactive applications, data schemas, datasets and publications within bioinformatics. EDAM applies to organizing and finding suitable tools and data and to automating their integration into complex applications or workflows. It includes over 2200 defined concepts and has successfully been used for annotations and implementations. AVAILABILITY: The latest stable version of EDAM is available in OWL format from http://edamontology.org/EDAM.owl and in OBO format from http://edamontology.org/EDAM.obo. It can be viewed online at the NCBO BioPortal and the EBI Ontology Lookup Service. For documentation and license please refer to http://edamontology.org. This article describes version 1.2 available at http://edamontology.org/EDAM\_1.2.owl. CONTACT: jison@ebi.ac.uk.},
	number = {10},
	urldate = {2018-07-13},
	journal = {Bioinformatics},
	author = {Ison, Jon and Kalas, Matús and Jonassen, Inge and Bolser, Dan and Uludag, Mahmut and McWilliam, Hamish and Malone, James and Lopez, Rodrigo and Pettifer, Steve and Rice, Peter},
	month = may,
	year = {2013},
	pmid = {23479348},
	pmcid = {PMC3654706},
	pages = {1325--1332},
}

@article{wolstencroft_taverna_2013,
	title = {The {Taverna} workflow suite: designing and executing workflows of {Web} {Services} on the desktop, web or in the cloud.},
	volume = {41},
	url = {http://dx.doi.org/10.1093/nar/gkt328},
	doi = {10.1093/nar/gkt328},
	abstract = {The Taverna workflow tool suite (http://www.taverna.org.uk) is designed to combine distributed Web Services and/or local tools into complex analysis pipelines. These pipelines can be executed on local desktop machines or through larger infrastructure (such as supercomputers, Grids or cloud environments), using the Taverna Server. In bioinformatics, Taverna workflows are typically used in the areas of high-throughput omics analyses (for example, proteomics or transcriptomics), or for evidence gathering methods involving text mining or data mining. Through Taverna, scientists have access to several thousand different tools and resources that are freely available from a large range of life science institutions. Once constructed, the workflows are reusable, executable bioinformatics protocols that can be shared, reused and repurposed. A repository of public workflows is available at http://www.myexperiment.org. This article provides an update to the Taverna tool suite, highlighting new features and developments in the workbench and the Taverna Server.},
	number = {Web Server issue},
	urldate = {2018-07-13},
	journal = {Nucleic Acids Research},
	author = {Wolstencroft, Katherine and Haines, Robert and Fellows, Donal and Williams, Alan and Withers, David and Owen, Stuart and Soiland-Reyes, Stian and Dunlop, Ian and Nenadic, Aleksandra and Fisher, Paul and Bhagat, Jiten and Belhajjame, Khalid and Bacall, Finn and Hardisty, Alex and Nieva de la Hidalga, Abraham and Balcazar Vargas, Maria P and Sufi, Shoaib and Goble, Carole},
	month = jul,
	year = {2013},
	pmid = {23640334},
	pmcid = {PMC3692062},
	pages = {W557--61},
}

@article{ison_tools_2016,
	title = {Tools and data services registry: a community effort to document bioinformatics resources.},
	volume = {44},
	url = {http://dx.doi.org/10.1093/nar/gkv1116},
	doi = {10.1093/nar/gkv1116},
	abstract = {Life sciences are yielding huge data sets that underpin scientific discoveries fundamental to improvement in human health, agriculture and the environment. In support of these discoveries, a plethora of databases and tools are deployed, in technically complex and diverse implementations, across a spectrum of scientific disciplines. The corpus of documentation of these resources is fragmented across the Web, with much redundancy, and has lacked a common standard of information. The outcome is that scientists must often struggle to find, understand, compare and use the best resources for the task at hand.Here we present a community-driven curation effort, supported by ELIXIR-the European infrastructure for biological information-that aspires to a comprehensive and consistent registry of information about bioinformatics resources. The sustainable upkeep of this Tools and Data Services Registry is assured by a curation effort driven by and tailored to local needs, and shared amongst a network of engaged partners.As of November 2015, the registry includes 1785 resources, with depositions from 126 individual registrations including 52 institutional providers and 74 individuals. With community support, the registry can become a standard for dissemination of information about bioinformatics resources: we welcome everyone to join us in this common endeavour. The registry is freely available at https://bio.tools. {\textbackslash}copyright The Author(s) 2015. Published by Oxford University Press on behalf of Nucleic Acids Research.},
	number = {D1},
	urldate = {2017-11-03},
	journal = {Nucleic Acids Research},
	author = {Ison, Jon and Rapacki, Kristoffer and Ménager, Hervé and Kalaš, Matúš and Rydza, Emil and Chmura, Piotr and Anthon, Christian and Beard, Niall and Berka, Karel and Bolser, Dan and Booth, Tim and Bretaudeau, Anthony and Brezovsky, Jan and Casadio, Rita and Cesareni, Gianni and Coppens, Frederik and Cornell, Michael and Cuccuru, Gianmauro and Davidsen, Kristian and Vedova, Gianluca Della and Dogan, Tunca and Doppelt-Azeroual, Olivia and Emery, Laura and Gasteiger, Elisabeth and Gatter, Thomas and Goldberg, Tatyana and Grosjean, Marie and Grüning, Björn and Helmer-Citterich, Manuela and Ienasescu, Hans and Ioannidis, Vassilios and Jespersen, Martin Closter and Jimenez, Rafael and Juty, Nick and Juvan, Peter and Koch, Maximilian and Laibe, Camille and Li, Jing-Woei and Licata, Luana and Mareuil, Fabien and Mičetić, Ivan and Friborg, Rune Møllegaard and Moretti, Sebastien and Morris, Chris and Möller, Steffen and Nenadic, Aleksandra and Peterson, Hedi and Profiti, Giuseppe and Rice, Peter and Romano, Paolo and Roncaglia, Paola and Saidi, Rabie and Schafferhans, Andrea and Schwämmle, Veit and Smith, Callum and Sperotto, Maria Maddalena and Stockinger, Heinz and Vařeková, Radka Svobodová and Tosatto, Silvio C E and de la Torre, Victor and Uva, Paolo and Via, Allegra and Yachdav, Guy and Zambelli, Federico and Vriend, Gert and Rost, Burkhard and Parkinson, Helen and Løngreen, Peter and Brunak, Søren},
	month = jan,
	year = {2016},
	pmid = {26538599},
	pmcid = {PMC4702812},
	pages = {D38--47},
}

@article{leipzig_review_2017,
	title = {A review of bioinformatic pipeline frameworks.},
	volume = {18},
	url = {http://dx.doi.org/10.1093/bib/bbw020},
	doi = {10.1093/bib/bbw020},
	abstract = {High-throughput bioinformatic analyses increasingly rely on pipeline frameworks to process sequence and metadata. Modern implementations of these frameworks differ on three key dimensions: using an implicit or explicit syntax, using a configuration, convention or class-based design paradigm and offering a command line or workbench interface. Here I survey and compare the design philosophies of several current pipeline frameworks. I provide practical recommendations based on analysis requirements and the user base. {\textbackslash}copyright The Author 2016. Published by Oxford University Press.},
	number = {3},
	urldate = {2018-07-13},
	journal = {Briefings in Bioinformatics},
	author = {Leipzig, Jeremy},
	month = may,
	year = {2017},
	pmid = {27013646},
	pmcid = {PMC5429012},
	pages = {530--536},
}

@article{moller_evaluation_2001,
	title = {Evaluation of methods for the prediction of membrane spanning regions.},
	volume = {17},
	url = {http://dx.doi.org/10.1093/bioinformatics/18.1.218},
	doi = {10.1093/bioinformatics/18.1.218},
	abstract = {MOTIVATION: A variety of tools are available to predict the topology of transmembrane proteins. To date no independent evaluation of the performance of these tools has been published. A better understanding of the strengths and weaknesses of the different tools would guide both the biologist and the bioinformatician to make better predictions of membrane protein topology. RESULTS: Here we present an evaluation of the performance of the currently best known and most widely used methods for the prediction of transmembrane regions in proteins. Our results show that TMHMM is currently the best performing transmembrane prediction program.},
	number = {7},
	urldate = {2017-11-03},
	journal = {Bioinformatics},
	author = {Möller, S and Croning, M D and Apweiler, R},
	month = jul,
	year = {2001},
	pmid = {11448883},
	pages = {646--653},
}

@article{spjuth_experiences_2015,
	title = {Experiences with workflows for automating data-intensive bioinformatics.},
	volume = {10},
	issn = {1745-6150},
	url = {http://www.biologydirect.com/content/10/1/43},
	doi = {10.1186/s13062-015-0071-8},
	abstract = {High-throughput technologies, such as next-generation sequencing, have turned molecular biology into a data-intensive discipline, requiring bioinformaticians to use high-performance computing resources and carry out data management and analysis tasks on large scale. Workflow systems can be useful to simplify construction of analysis pipelines that automate tasks, support reproducibility and provide measures for fault-tolerance. However, workflow systems can incur significant development and administration overhead so bioinformatics pipelines are often still built without them. We present the experiences with workflows and workflow systems within the bioinformatics community participating in a series of hackathons and workshops of the EU COST action SeqAhead. The organizations are working on similar problems, but we have addressed them with different strategies and solutions. This fragmentation of efforts is inefficient and leads to redundant and incompatible solutions. Based on our experiences we define a set of recommendations for future systems to enable efficient yet simple bioinformatics workflow construction and execution.},
	urldate = {2017-11-03},
	journal = {Biology Direct},
	author = {Spjuth, Ola and Bongcam-Rudloff, Erik and Hernández, Guillermo Carrasco and Forer, Lukas and Giovacchini, Mario and Guimera, Roman Valls and Kallio, Aleksi and Korpelainen, Eija and Kańduła, Maciej M and Krachunov, Milko and Kreil, David P and Kulev, Ognyan and Łabaj, Paweł P and Lampa, Samuel and Pireddu, Luca and Schönherr, Sebastian and Siretskiy, Alexey and Vassilev, Dimitar},
	month = aug,
	year = {2015},
	pmid = {26282399},
	pmcid = {PMC4539931},
	pages = {43},
}

@article{zook_integrating_2014,
	title = {Integrating human sequence data sets provides a resource of benchmark {SNP} and indel genotype calls.},
	volume = {32},
	url = {http://dx.doi.org/10.1038/nbt.2835},
	doi = {10.1038/nbt.2835},
	abstract = {Clinical adoption of human genome sequencing requires methods that output genotypes with known accuracy at millions or billions of positions across a genome. Because of substantial discordance among calls made by existing sequencing methods and algorithms, there is a need for a highly accurate set of genotypes across a genome that can be used as a benchmark. Here we present methods to make high-confidence, single-nucleotide polymorphism (SNP), indel and homozygous reference genotype calls for NA12878, the pilot genome for the Genome in a Bottle Consortium. We minimize bias toward any method by integrating and arbitrating between 14 data sets from five sequencing technologies, seven read mappers and three variant callers. We identify regions for which no confident genotype call could be made, and classify them into different categories based on reasons for uncertainty. Our genotype calls are publicly available on the Genome Comparison and Analytic Testing website to enable real-time benchmarking of any method.},
	number = {3},
	urldate = {2018-06-12},
	journal = {Nature Biotechnology},
	author = {Zook, Justin M and Chapman, Brad and Wang, Jason and Mittelman, David and Hofmann, Oliver and Hide, Winston and Salit, Marc},
	month = mar,
	year = {2014},
	pmid = {24531798},
	pages = {246--251},
}

@article{prins_toward_2015,
	title = {Toward effective software solutions for big biology.},
	volume = {33},
	url = {http://dx.doi.org/10.1038/nbt.3240},
	doi = {10.1038/nbt.3240},
	number = {7},
	urldate = {2017-11-03},
	journal = {Nature Biotechnology},
	author = {Prins, Pjotr and de Ligt, Joep and Tarasov, Artem and Jansen, Ritsert C and Cuppen, Edwin and Bourne, Philip E},
	month = jul,
	year = {2015},
	pmid = {26154002},
	pages = {686--687},
}

@article{goecks_galaxy_2010,
	title = {Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences.},
	volume = {11},
	url = {http://dx.doi.org/10.1186/gb-2010-11-8-r86},
	doi = {10.1186/gb-2010-11-8-r86},
	abstract = {Increased reliance on computational approaches in the life sciences has revealed grave concerns about how accessible and reproducible computation-reliant results truly are. Galaxy http://usegalaxy.org, an open web-based platform for genomic research, addresses these problems. Galaxy automatically tracks and manages data provenance and provides support for capturing the context and intent of computational methods. Galaxy Pages are interactive, web-based documents that provide users with a medium to communicate a complete computational analysis.},
	number = {8},
	urldate = {2017-11-03},
	journal = {Genome Biology},
	author = {Goecks, Jeremy and Nekrutenko, Anton and Taylor, James and Team, Galaxy},
	month = aug,
	year = {2010},
	pmid = {20738864},
	pmcid = {PMC2945788},
	pages = {R86},
}

@article{garijo_common_2014,
	title = {Common motifs in scientific workflows: {An} empirical analysis},
	volume = {36},
	issn = {0167739X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X13001970},
	doi = {10.1016/j.future.2013.09.018},
	abstract = {Workflow technology continues to play an important role as a means for specifying and enacting computational experiments in modern science. Reusing and re-purposing workflows allow scientists to do new experiments faster, since the workflows capture useful expertise from others. As workflow libraries grow, scientists face the challenge of finding workflows appropriate for their task, understanding what each workflow does, and reusing relevant portions of a given workflow. We believe that workflows would be easier to understand and reuse if high-level views (abstractions) of their activities were available in workflow libraries. As a first step towards obtaining these abstractions, we report in this paper on the results of a manual analysis performed over a set of real-world scientific workflows from Taverna, Wings, Galaxy and Vistrails. Our analysis has resulted in a set of scientific workflow motifs that outline (i) the kinds of data-intensive activities that are observed in workflows (Data-Operation motifs), and (ii) the different manners in which activities are implemented within workflows (Workflow-Oriented motifs). These motifs are helpful to identify the functionality of the steps in a given workflow, to develop best practices for workflow design, and to develop approaches for automated generation of workflow abstractions.},
	urldate = {2020-01-20},
	journal = {Future Generation Computer Systems},
	author = {Garijo, Daniel and Alper, Pinar and Belhajjame, Khalid and Corcho, Oscar and Gil, Yolanda and Goble, Carole},
	month = jul,
	year = {2014},
	pages = {338--351},
}

@article{goble_myexperiment_2010,
	title = {{myExperiment}: a repository and social network for the sharing of bioinformatics workflows.},
	volume = {38},
	url = {http://dx.doi.org/10.1093/nar/gkq429},
	doi = {10.1093/nar/gkq429},
	abstract = {myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.},
	number = {Web Server issue},
	urldate = {2017-11-03},
	journal = {Nucleic Acids Research},
	author = {Goble, Carole A and Bhagat, Jiten and Aleksejevs, Sergejs and Cruickshank, Don and Michaelides, Danius and Newman, David and Borkum, Mark and Bechhofer, Sean and Roos, Marco and Li, Peter and De Roure, David},
	month = jul,
	year = {2010},
	pmid = {20501605},
	pmcid = {PMC2896080},
	pages = {W677--82},
}

@article{li_seqanswers_2012,
	title = {The {SEQanswers} wiki: a wiki database of tools for high-throughput sequencing analysis.},
	volume = {40},
	url = {http://dx.doi.org/10.1093/nar/gkr1058},
	doi = {10.1093/nar/gkr1058},
	abstract = {Recent advances in sequencing technology have created unprecedented opportunities for biological research. However, the increasing throughput of these technologies has created many challenges for data management and analysis. As the demand for sophisticated analyses increases, the development time of software and algorithms is outpacing the speed of traditional publication. As technologies continue to be developed, methods change rapidly, making publications less relevant for users. The SEQanswers wiki (SEQwiki) is a wiki database that is actively edited and updated by the members of the SEQanswers community (http://SEQanswers.com/). The wiki provides an extensive catalogue of tools, technologies and tutorials for high-throughput sequencing (HTS), including information about HTS service providers. It has been implemented in MediaWiki with the Semantic MediaWiki and Semantic Forms extensions to collect structured data, providing powerful navigation and reporting features. Within 2 years, the community has created pages for over 500 tools, with approximately 400 literature references and 600 web links. This collaborative effort has made SEQwiki the most comprehensive database of HTS tools anywhere on the web. The wiki includes task-focused mini-reviews of commonly used tools, and a growing collection of more than 100 HTS service providers. SEQwiki is available at: http://wiki.SEQanswers.com/.},
	number = {Database issue},
	urldate = {2017-11-03},
	journal = {Nucleic Acids Research},
	author = {Li, Jing-Woei and Robison, Keith and Martin, Marcel and Sjödin, Andreas and Usadel, Björn and Young, Matthew and Olivares, Eric C and Bolser, Dan M},
	month = jan,
	year = {2012},
	pmid = {22086956},
	pmcid = {PMC3245082},
	pages = {D1313--7},
}

@article{bandrowski_resource_2015,
	title = {The {Resource} {Identification} {Initiative}: {A} cultural shift in publishing.},
	volume = {4},
	url = {http://dx.doi.org/10.12688/f1000research.6555.2},
	doi = {10.12688/f1000research.6555.2},
	abstract = {A central tenet in support of research reproducibility is the ability to uniquely identify research resources, i.e., reagents, tools, and materials that are used to perform experiments. However, current reporting practices for research resources are insufficient to allow humans and algorithms to identify the exact resources that are reported or answer basic questions such as "What other studies used resource X?" To address this issue, the Resource Identification Initiative was launched as a pilot project to improve the reporting standards for research resources in the methods sections of papers and thereby improve identifiability and reproducibility. The pilot engaged over 25 biomedical journal editors from most major publishers, as well as scientists and funding officials. Authors were asked to include Research Resource Identifiers (RRIDs) in their manuscripts prior to publication for three resource types: antibodies, model organisms, and tools (including software and databases). RRIDs represent accession numbers assigned by an authoritative database, e.g., the model organism databases, for each type of resource. To make it easier for authors to obtain RRIDs, resources were aggregated from the appropriate databases and their RRIDs made available in a central web portal ( www.scicrunch.org/resources). RRIDs meet three key criteria: they are machine readable, free to generate and access, and are consistent across publishers and journals. The pilot was launched in February of 2014 and over 300 papers have appeared that report RRIDs. The number of journals participating has expanded from the original 25 to more than 40. Here, we present an overview of the pilot project and its outcomes to date. We show that authors are generally accurate in performing the task of identifying resources and supportive of the goals of the project. We also show that identifiability of the resources pre- and post-pilot showed a dramatic improvement for all three resource types, suggesting that the project has had a significant impact on reproducibility relating to research resources.},
	urldate = {2017-11-03},
	journal = {F1000Research},
	author = {Bandrowski, Anita and Brush, Matthew and Grethe, Jeffery S and Haendel, Melissa A and Kennedy, David N and Hill, Sean and Hof, Patrick R and Martone, Maryann E and Pols, Maaike and Tan, Serena and Washington, Nicole and Zudilova-Seinstra, Elena and Vasilevsky, Nicole and https://www.force11.org/node/4463/members, Resource Identification Initiative Members are listed here:},
	month = may,
	year = {2015},
	pmid = {26594330},
	pmcid = {PMC4648211},
	pages = {134},
}

@article{gentleman_bioconductor_2004,
	title = {Bioconductor: open software development for computational biology and bioinformatics.},
	volume = {5},
	url = {http://dx.doi.org/10.1186/gb-2004-5-10-r80},
	doi = {10.1186/gb-2004-5-10-r80},
	abstract = {The Bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics. The goals of the project include: fostering collaborative development and widespread use of innovative software, reducing barriers to entry into interdisciplinary scientific research, and promoting the achievement of remote reproducibility of research results. We describe details of our aims and methods, identify current challenges, compare Bioconductor to other open bioinformatics projects, and provide working examples.},
	number = {10},
	urldate = {2017-02-20},
	journal = {Genome Biology},
	author = {Gentleman, Robert C and Carey, Vincent J and Bates, Douglas M and Bolstad, Ben and Dettling, Marcel and Dudoit, Sandrine and Ellis, Byron and Gautier, Laurent and Ge, Yongchao and Gentry, Jeff and Hornik, Kurt and Hothorn, Torsten and Huber, Wolfgang and Iacus, Stefano and Irizarry, Rafael and Leisch, Friedrich and Li, Cheng and Maechler, Martin and Rossini, Anthony J and Sawitzki, Gunther and Smith, Colin and Smyth, Gordon and Tierney, Luke and Yang, Jean Y H and Zhang, Jianhua},
	month = sep,
	year = {2004},
	pmid = {15461798},
	pmcid = {PMC545600},
	pages = {R80},
}

@article{sroka_formal_2010,
	series = {Special {Issue}: {Scientific} {Workflow} 2009},
	title = {A formal semantics for the {Taverna} 2 workflow model},
	volume = {76},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S0022000009001251},
	doi = {10.1016/j.jcss.2009.11.009},
	abstract = {This paper presents a formal semantics for the Taverna 2 scientific workflow system. Taverna 2 is a successor to Taverna, an open-source workflow system broadly adopted within the e-science community worldwide. The new version improves upon the existing model in two main ways: (i) by adding support for data pipelining, which in turns enables input streams of indefinite length to be processed efficiently; and (ii) by providing new extensibility points that make it possible to add new operators to the workflow model. Consistent with previous work by some of the authors, we use trace semantics to describe the effect of workflow computations, and we show how they can be used to describe the new features in the Taverna 2 model.},
	language = {en},
	number = {6},
	urldate = {2020-01-21},
	journal = {Journal of Computer and System Sciences},
	author = {Sroka, Jacek and Hidders, Jan and Missier, Paolo and Goble, Carole},
	month = sep,
	year = {2010},
	keywords = {Formal semantics, Scientific workflows, Taverna 2, Trace semantics},
	pages = {490--508},
}

@article{khan_sharing_2019,
	title = {Sharing interoperable workflow provenance: {A} review of best practices and their practical application in {CWLProv}},
	volume = {8},
	shorttitle = {Sharing interoperable workflow provenance},
	url = {https://academic.oup.com/gigascience/article/8/11/giz095/5611001},
	doi = {10.1093/gigascience/giz095},
	abstract = {AbstractBackground.  The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Comput},
	language = {en},
	number = {11},
	urldate = {2019-11-04},
	journal = {GigaScience},
	author = {Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O. and Lonie, Andrew and Goble, Carole and Crusoe, Michael R.},
	month = nov,
	year = {2019},
}

@misc{noauthor_schema_salad_nodate,
	title = {schema\_salad {\textbar} {Read} the {Docs}},
	url = {https://readthedocs.org/projects/schema-salad/builds/9859889/},
	urldate = {2019-10-25},
}

@article{surrey_genomic_nodate,
	title = {Genomic {Analysis} of {Dysembryoplastic} {Neuroepithelial} {Tumor} {Spectrum} {Reveals} a {Diversity} of {Molecular} {Alterations} {Dysregulating} the {MAPK} and {PI3K}/{mTOR} {Pathways}},
	url = {https://academic.oup.com/jnen/advance-article/doi/10.1093/jnen/nlz101/5588597},
	doi = {10.1093/jnen/nlz101},
	abstract = {Abstract.  Dysembryoplastic neuroepithelial tumors (DNT) lacking key diagnostic criteria are challenging to diagnose and sometimes fall into the broader categor},
	language = {en},
	urldate = {2019-10-23},
	journal = {Journal of Neuropathology \& Experimental Neurology},
	author = {Surrey, Lea F. and Jain, Payal and Zhang, Bo and Straka, Joshua and Zhao, Xiaonan and Harding, Brian N. and Resnick, Adam C. and Storm, Phillip B. and Buccoliero, Anna Maria and Genitori, Lorenzo and Li, Marilyn M. and Waanders, Angela J. and Santi, Mariarita},
}

@article{ison_community_nodate,
	title = {Community curation of bioinformatics software and data resources},
	url = {https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbz075/5560007},
	doi = {10.1093/bib/bbz075},
	abstract = {Abstract.  The corpus of bioinformatics resources is huge and expanding rapidly, presenting life scientists with a growing challenge in selecting tools that fit},
	language = {en},
	urldate = {2019-10-23},
	journal = {Briefings in Bioinformatics},
	author = {Ison, Jon and Ménager, Hervé and Brancotte, Bryan and Jaaniso, Erik and Salumets, Ahto and Raček, Tomáš and Lamprecht, Anna-Lena and Palmblad, Magnus and Kalaš, Matúš and Chmura, Piotr and Hancock, John M. and Schwämmle, Veit and Ienasescu, Hans-Ioan},
}

@article{vazquez_patient_2019,
	title = {Patient {Dossier}: {Healthcare} queries over distributed resources},
	volume = {15},
	issn = {1553-7358},
	shorttitle = {Patient {Dossier}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007291},
	doi = {10.1371/journal.pcbi.1007291},
	abstract = {As with many other aspects of the modern world, in healthcare, the explosion of data and resources opens new opportunities for the development of added-value services. Still, a number of specific conditions on this domain greatly hinders these developments, including ethical and legal issues, fragmentation of the relevant data in different locations, and a level of (meta)data complexity that requires great expertise across technical, clinical, and biological domains. We propose the Patient Dossier paradigm as a way to organize new innovative healthcare services that sorts the current limitations. The Patient Dossier conceptual framework identifies the different issues and suggests how they can be tackled in a safe, efficient, and responsible way while opening options for independent development for different players in the healthcare sector. An initial implementation of the Patient Dossier concepts in the Rbbt framework is available as open-source at https://github.com/mikisvaz and https://github.com/Rbbt-Workflows.},
	language = {en},
	number = {10},
	urldate = {2019-10-23},
	journal = {PLOS Computational Biology},
	author = {Vazquez, Miguel and Valencia, Alfonso},
	month = oct,
	year = {2019},
	keywords = {Bioinformatics, Computational pipelines, Data management, Data processing, Genome analysis, Genomic medicine, Genomics, Next-generation sequencing},
	pages = {e1007291},
}

@article{hey_fourth_2019,
	title = {The {Fourth} {Paradigm} 10 {Years} {On}},
	issn = {1432-122X},
	url = {https://doi.org/10.1007/s00287-019-01215-9},
	doi = {10.1007/s00287-019-01215-9},
	language = {en},
	urldate = {2019-10-23},
	journal = {Informatik Spektrum},
	author = {Hey, Tony and Trefethen, Anne},
	month = oct,
	year = {2019},
}

@misc{noauthor_fair_nodate,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship {\textbar} {Scientific} {Data}},
	url = {https://www.nature.com/articles/sdata201618},
	urldate = {2019-08-01},
}

@misc{noauthor_future_nodate,
	title = {The future of scientific workflows - {Ewa} {Deelman}, {Tom} {Peterka}, {Ilkay} {Altintas}, {Christopher} {D} {Carothers}, {Kerstin} {Kleese} van {Dam}, {Kenneth} {Moreland}, {Manish} {Parashar}, {Lavanya} {Ramakrishnan}, {Michela} {Taufer}, {Jeffrey} {Vetter}, 2018},
	url = {https://journals.sagepub.com/doi/10.1177/1094342017704893},
	urldate = {2019-08-01},
}

@book{crusoe_channeling_2014,
	title = {Channeling community contributions to scientific software: a hackathon experience},
	url = {http://doi.org/10.6084/M9.FIGSHARE.1112541.V2},
	author = {Crusoe, Michael R. and Crusoe, Michael R. and Brown, C. Titus},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.1112541.V2},
	note = {00002 },
}

@book{crusoe_khmer-protocols_2013,
	title = {khmer-protocols 0.8.4 documentation},
	url = {http://doi.org/10.6084/M9.FIGSHARE.878460.V2},
	author = {Crusoe, Michael R. and Brown, C. Titus and Scott, Camille and Crusoe, Michael R. and Sheneman, Leigh and Rosenthal, Josh and Howe, Adina},
	year = {2013},
	doi = {10.6084/M9.FIGSHARE.878460.V2},
	note = {00003 },
}

@article{da_veiga_leprevost_biocontainers:_2017,
	title = {{BioContainers}: an open-source and community-driven framework for software standardization},
	volume = {33},
	issn = {1367-4803},
	shorttitle = {{BioContainers}},
	url = {https://academic.oup.com/bioinformatics/article/33/16/2580/3096437},
	doi = {10.1093/bioinformatics/btx192},
	abstract = {AbstractMotivation.  BioContainers (biocontainers.pro) is an open-source and community-driven framework which provides platform independent executable environme},
	language = {en},
	number = {16},
	urldate = {2019-08-01},
	journal = {Bioinformatics},
	author = {da Veiga Leprevost, Felipe and Grüning, Björn A. and Alves Aflitos, Saulo and Röst, Hannes L. and Uszkoreit, Julian and Barsnes, Harald and Vaudel, Marc and Moreno, Pablo and Gatto, Laurent and Weber, Jonas and Bai, Mingze and Jimenez, Rafael C. and Sachsenberg, Timo and Pfeuffer, Julianus and Vera Alvarez, Roberto and Griss, Johannes and Nesvizhskii, Alexey I. and Perez-Riverol, Yasset},
	month = aug,
	year = {2017},
	pages = {2580--2582},
}

@article{alimena_searching_2019,
	title = {Searching for long-lived particles beyond the {Standard} {Model} at the {Large} {Hadron} {Collider}},
	url = {https://arxiv.org/abs/1903.04497v1},
	abstract = {Particles beyond the Standard Model (SM) can generically have lifetimes that
are long compared to SM particles at the weak scale. When produced at
experiments such as the Large Hadron Collider (LHC) at CERN, these long-lived
particles (LLPs) can decay far from the interaction vertex of the primary
proton-proton collision. Such LLP signatures are distinct from those of
promptly decaying particles that are targeted by the majority of searches for
new physics at the LHC, often requiring customized techniques to identify, for
example, significantly displaced decay vertices, tracks with atypical
properties, and short track segments. Given their non-standard nature, a
comprehensive overview of LLP signatures at the LHC is beneficial to ensure
that possible avenues of the discovery of new physics are not overlooked. Here
we report on the joint work of a community of theorists and experimentalists
with the ATLAS, CMS, and LHCb experiments --- as well as those working on
dedicated experiments such as MoEDAL, milliQan, MATHUSLA, CODEX-b, and FASER
--- to survey the current state of LLP searches at the LHC, and to chart a path
for the development of LLP searches into the future, both in the upcoming Run 3
and at the High-Luminosity LHC. The work is organized around the current and
future potential capabilities of LHC experiments to generally discover new
LLPs, and takes a signature-based approach to surveying classes of models that
give rise to LLPs rather than emphasizing any particular theory motivation. We
develop a set of simplified models; assess the coverage of current searches;
document known, often unexpected backgrounds; explore the capabilities of
proposed detector upgrades; provide recommendations for the presentation of
search results; and look towards the newest frontiers, namely high-multiplicity
"dark showers", highlighting opportunities for expanding the LHC reach for
these signals.},
	language = {en},
	urldate = {2019-03-19},
	author = {Alimena, Juliette and Beacham, James and Borsato, Martino and Cheng, Yangyang and Vidal, Xabier Cid and Cottin, Giovanna and De Roeck, Albert and Desai, Nishita and Curtin, David and Evans, Jared A. and Knapen, Simon and Kraml, Sabine and Lessa, Andre and Liu, Zhen and Mehlhase, Sascha and Ramsey-Musolf, Michael J. and Russell, Heather and Shelton, Jessie and Shuve, Brian and Verducci, Monica and Zurita, Jose and Adams, Todd and Adersberger, Michael and Alpigiani, Cristiano and Apresyan, Artur and Bainbridge, Robert John and Batozskaya, Varvara and Beauchesne, Hugues and Benato, Lisa and Berlendis, S. and Bhal, Eshwen and Blekman, Freya and Borovilou, Christina and Boyd, Jamie and Brau, Benjamin P. and Bryngemark, Lene and Buchmueller, Oliver and Buschmann, Malte and Buttinger, William and Campanelli, Mario and Cesarotti, Cari and Chen, Chunhui and Cheng, Hsin-Chia and Cheong, Sanha and Citron, Matthew and Coccaro, Andrea and Coco, V. and Conte, Eric and Cormier, Félix and Corpe, Louie D. and Craig, Nathaniel and Cui, Yanou and Dall'Occo, Elena and Dallapiccola, C. and Darwish, M. R. and Davoli, Alessandro and de Cosa, Annapaola and De Simone, Andrea and Rose, Luigi Delle and Deppisch, Frank F. and Dey, Biplab and Diamond, Miriam D. and Dienes, Keith R. and Dildick, Sven and Döbrich, Babette and Drewes, Marco and Eich, Melanie and ElSawy, M. and del Valle, Alberto Escalante and Facini, Gabriel and Farina, Marco and Feng, Jonathan L. and Fischer, Oliver and Flaecher, H. U. and Foldenauer, Patrick and Freytsis, Marat and Fuks, Benjamin and Galon, Iftah and Gershtein, Yuri and Giagu, Stefano and Giammanco, Andrea and Gligorov, Vladimir V. and Golling, Tobias and Grancagnolo, Sergio and Gustavino, Giuliano and Haas, Andrew and Hahn, Kristian and Hajer, Jan and Hammad, Ahmed and Heinrich, Lukas and Heisig, Jan and Helo, J. C. and Hesketh, Gavin and Hill, Christopher S. and Hirsch, Martin and Hohlmann, M. and Hulsbergen, W. and Huth, John and Ilten, Philip and Jacques, Thomas},
	month = mar,
	year = {2019},
	note = {00001},
}

@article{hillion_using_2017,
	title = {Using bio.tools to generate and annotate workbench tool descriptions},
	volume = {6},
	url = {https://doi.org/10.12688%2Ff1000research.12974.1},
	doi = {10.12688/f1000research.12974.1},
	journal = {F1000Research},
	author = {Hillion, Kenzo-Hugo and Kuzmin, Ivan and Khodak, Anton and Rasche, Eric and Crusoe, Michael and Peterson, Hedi and Ison, Jon and Ménager, Hervé},
	month = nov,
	year = {2017},
	note = {00001},
	pages = {2074},
}

@book{crusoe_khmer_2014,
	title = {The khmer software package: enabling efficient sequence analysis},
	url = {http://doi.org/10.6084/M9.FIGSHARE.979190.V1},
	author = {Crusoe, Michael R. and Brown, C. Titus and Crusoe, Michael R. and Edvenson, Greg and Fish, Jordan and Howe, Adina and McDonald, Eric and Nahum, Joshua and Nanlohy, Kaben and Ortiz-Zuazaga, Humberto and Pell, Jason and Simpson, Jared and Scott, Camille and Srinivasan, Ramakrishnan Rajaram and Zhang, Qingpeng},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.979190.V1},
	note = {00023 },
}

@book{crusoe_should_2019,
	title = {Should you cite this particular piece of software?},
	url = {http://doi.org/10.5281/zenodo.2559141},
	author = {Crusoe, Michael R. and Brown, Louise and Crusoe, Michael R. and Miļajevs, Dmitrijs and Romanowska, Iza},
	year = {2019},
	doi = {10.5281/zenodo.2559141},
	note = {00000 },
}

@article{maller_robust_2017,
	title = {Robust {Cross}-{Platform} {Workflows}: {How} {Technical} and {Scientific} {Communities} {Collaborate} to {Develop}, {Test} and {Share} {Best} {Practices} for {Data} {Analysis}},
	volume = {2},
	url = {https://doi.org/10.1007%2Fs41019-017-0050-4},
	doi = {10.1007/s41019-017-0050-4},
	number = {3},
	journal = {Data Science and Engineering},
	author = {MÃ¶ller, Steffen and Prescott, Stuart W. and Wirzenius, Lars and Reinholdtsen, Petter and Chapman, Brad and Prins, Pjotr and Soiland-Reyes, Stian and KlÃ¶tzl, Fabian and Bagnacani, Andrea and Kalaš, Matúš and Tille, Andreas and Crusoe, Michael R.},
	month = sep,
	year = {2017},
	note = {00000},
	pages = {232--244},
}

@book{crusoe_request_2017,
	title = {Request for {Community} partnership in data resource licensing planning},
	url = {http://doi.org/10.6084/M9.FIGSHARE.4972709},
	author = {Crusoe, Michael R. and Haendel, Melissa and Mungall, Chris and Su, Andrew and Robinson, Peter and Chute, Chris and Altman, Russ B. and Payne, Philip R. O. and Lawler, Mark and Oprea, Tudor I. and Willbanks, John and Srinivasan, Subha and Hunter, Lawrence and Sim, Ida and McDonald, Sean and Mooney, Sean and Smedley, Damian and Ganley, Emma and Kenall, Amye and Clark, Timothy and Goble, Carole and Dumontier, Michel and Holmes, Kristi and Diekans, Mark and Zell, Adrienne and Overby, Casey and Glusman, Gustavo and Carmody, Leigh and Jiang, Guoqian and Munos-Torres, Monica and Hoatlin, Maureen and Goecks, Jeremy and Jongeneel, Victor and Bittker, Joshua and Gourdine, Jean-Philippe and Brush, Matthew H. and Zhu, Richard L. and Mangravite, Lara and Tyler, Brett and Wilkinson, Mark D. and Crusoe, Michael R. and Mazumder, Raja and Tatonetti, Nicholas P. and D'Eustachio, Peter and Vasilevsky, Nicole and McMurry, Julie and Champieux, Robin},
	year = {2017},
	doi = {10.6084/M9.FIGSHARE.4972709},
	note = {00000 },
}

@inproceedings{tollis_mining_2014,
	title = {Mining the {Most} {Species}-{Rich} {Amniote} {Genus}: de novo {Sequencing} of {Three} {Anole} {Lizards} for {Comparative} {Genomic} {Analysis}},
	url = {https://pag.confex.com/pag/xxii/webprogram/Paper11290.html},
	booktitle = {International {Plant} and {Animal} {Genome} {XXII}},
	author = {Tollis, Marc and Hutchins, Elizabeth D and Eckalbar, Walter and Crusoe, Michael R and May, Catherine M. and Stapley, Jessica and Kulik, Elise and Huentelman, Matt J. and Fisher, Rebecca E and Kusumi, Kenro},
	year = {2014},
	note = {00000},
}

@book{crusoe_khmer-protocols_2013-1,
	title = {khmer-protocols 0.8.3 documentation},
	url = {http://doi.org/10.6084/M9.FIGSHARE.878460.V1},
	author = {Crusoe, Michael R. and Brown, C. Titus and Scott, Camille and Crusoe, Michael R. and Sheneman, Leigh and Rosenthal, Josh and Howe, Adina},
	year = {2013},
	doi = {10.6084/M9.FIGSHARE.878460.V1},
	note = {00000 },
}

@book{crusoe_galaxy_2016,
	title = {Galaxy {Planemo} 0.35.0},
	author = {Crusoe, Michael R. and Chilton, John and Cock, Peter and Rasche, Eric and Turaga, Nitesh and Cech, Martin and Grüning, Björn and Soranzo, Nicola and Bouvier, Dave and Blankenberg, Dan and Baker, Dannon and Ellrott, Kyle and Coraor, Nate and Beek, Marius van den and Parsons, Lance and Marenco, Remi and Freeberg, Mallory and Chambers, Matt and Heusden, Peter van and Stewart, Paul and Crusoe, Michael and Einon, Mark and Taylor, James and Corguillé, Gildas Le and Kuster, Greg Von},
	year = {2016},
	note = {00000},
}

@book{valencia_facial_2004,
	title = {Facial recognition using near infrared images to match visible images},
	copyright = {All rights reserved},
	author = {Valencia, Valorie S. and {Sandalphon} and Crusoe, Michael R.},
	year = {2004},
	note = {00000},
}

@article{alterovitz_enabling_2018,
	title = {Enabling precision medicine via standard communication of {HTS} provenance, analysis, and results},
	volume = {16},
	url = {https://doi.org/10.1371%2Fjournal.pbio.3000099},
	doi = {10.1371/journal.pbio.3000099},
	number = {12},
	journal = {PLOS Biology},
	author = {Alterovitz, Gil and Dean, Dennis and Goble, Carole and Crusoe, Michael R. and Soiland-Reyes, Stian and Bell, Amanda and Hayes, Anais and Suresh, Anita and Purkayastha, Anjan and King, Charles H. and Taylor, Dan and Johanson, Elaine and Thompson, Elaine E. and Donaldson, Eric and Morizono, Hiroki and Tsang, Hsinyi and Vora, Jeet K. and Goecks, Jeremy and Yao, Jianchao and Almeida, Jonas S. and Keeney, Jonathon and Addepalli, KanakaDurga and Krampis, Konstantinos and Smith, Krista M. and Guo, Lydia and Walderhaug, Mark and Schito, Marco and Ezewudo, Matthew and Guimera, Nuria and Walsh, Paul and Kahsay, Robel and Gottipati, Srikanth and Rodwell, Timothy C. and Bloom, Toby and Lai, Yuching and Simonyan, Vahan and Mazumder, Raja},
	month = dec,
	year = {2018},
	note = {00003},
	pages = {e3000099},
}

@book{crusoe_criteria-based_2015,
	title = {Criteria-based assessment of the khmer suite v1.0},
	url = {http://doi.org/10.6084/M9.FIGSHARE.1558322},
	author = {Crusoe, Michael R. and Crusoe, Michael R.},
	year = {2015},
	doi = {10.6084/M9.FIGSHARE.1558322},
	note = {00000 },
}

@book{crusoe_criteria-based_2015-1,
	title = {Criteria-based assessment of the khmer suite},
	url = {http://doi.org/10.6084/M9.FIGSHARE.1558321},
	author = {Crusoe, Michael R. and Crusoe, Michael R.},
	year = {2015},
	doi = {10.6084/M9.FIGSHARE.1558321},
	note = {00000 },
}

@article{tollis_comparative_2018,
	title = {Comparative genomics reveals accelerated evolution in conserved pathways during the diversification of anole lizards},
	volume = {10},
	copyright = {All rights reserved},
	number = {2},
	journal = {Genome biology and evolution},
	author = {Tollis, Marc and Hutchins, Elizabeth D and Stapley, Jessica and Rupp, Shawn M and Eckalbar, Walter L and Maayan, Inbar and Lasku, Eris and Infante, Carlos R and Dennis, Stuart R and Robertson, Joel A and {others}},
	year = {2018},
	note = {00006},
	pages = {489--506},
}

@book{crusoe_common-workflow-language/cwlprov:_2018,
	title = {Common-{Workflow}-{Language}/{Cwlprov}: {Cwlprov} 0.6.0},
	url = {http://doi.org/10.5281/zenodo.1471585},
	author = {Crusoe, Michael R. and Soiland-Reyes, Stian and Khan, Farah Zaib and Crusoe, Michael R.},
	year = {2018},
	doi = {10.5281/zenodo.1471585},
	note = {00000 },
}

@book{crusoe_finger_2004,
	title = {Finger minutiae-based biometric profile for seafarers' identity documents},
	isbn = {92-2-118217-7},
	author = {Crusoe, Michael R.},
	year = {2004},
	note = {00000},
}

@inproceedings{crusoe_modeling_2009,
	title = {Modeling {Malaria} {Pathogenesis}},
	booktitle = {Arizona {State} {University} {College} of {Liberal} {Arts} and {Sciences} {Undergraduate} {Research} {Symposium}},
	author = {Crusoe, Michael R.},
	year = {2009},
	note = {00000},
}

@book{crusoe_ged_2013,
	title = {{GED} submission to {First} {Workshop} on {Sustainable} {Software} for {Science}: {Practice} and {Experiences}},
	url = {http://doi.org/10.6084/M9.FIGSHARE.791567.V1},
	author = {Crusoe, Michael R. and Crusoe, Michael R. and Brown, C. Titus},
	year = {2013},
	doi = {10.6084/M9.FIGSHARE.791567.V1},
	note = {00000 },
}

@book{crusoe_khmer:_2014,
	title = {khmer: k-mer counting \& filtering {FTW}},
	url = {http://doi.org/10.6084/M9.FIGSHARE.963559.V1},
	author = {Crusoe, Michael R. and Crusoe, Michael R. and Edvenson, Greg and Fish, Jordan and Howe, Adina and McDonald, Eric and Nahum, Joshua and Nanlohy, Kaben and Pell, Jason and Simpson, Jared and Scott, Camille and Zhang, Qingpeng and Brown, C. Titus},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.963559.V1},
	note = {00004 },
}

@book{crusoe_better_2013,
	title = {Better science through superior software},
	author = {Crusoe, Michael R. and Crusoe, Michael R.},
	year = {2013},
	note = {00000},
}

@book{brown;_khmer:_2014,
	title = {khmer: k-mer counting \&amp; filtering {FTW}},
	url = {http://dx.doi.org/10.6084/M9.FIGSHARE.963559},
	publisher = {Figshare},
	author = {Brown;, Michael R. Crusoe; Greg Edvenson; Jordan Fish; Adina Howe; Eric McDonald; Joshua Nahum; Kaben Nanlohy; Jason Pell; Jared Simpson; Camille Scott; Qingpeng Zhang; C. Titus},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.963559},
	note = {00000 },
}

@book{crusoe;_khmer_2014,
	title = {The khmer software package: enabling efficient sequence analysis},
	url = {http://dx.doi.org/10.6084/M9.FIGSHARE.979190},
	publisher = {Figshare},
	author = {Crusoe;, C. Titus Brown; Michael R.},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.979190},
	note = {00023 },
}

@article{crusoe_channeling_2014-1,
	title = {Channeling community contributions to scientific software: a sprint experience},
	url = {10.6084/m9.figshare.1112541},
	author = {Crusoe, Michael R. and Brown, C. Titus},
	year = {2014},
	note = {00001},
}

@book{crusoe_channeling_2014-2,
	title = {Channeling community contributions to scientific software: a sprint experience},
	url = {http://doi.org/10.6084/M9.FIGSHARE.1112541.V3},
	author = {Crusoe, Michael R. and Crusoe, Michael R. and Brown, C. Titus},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.1112541.V3},
	note = {00001 },
}

@inproceedings{colbry_less_2014,
	title = {Less talking, more doing: crowd-sourcing the integration of {Galaxy} with a high-performance computing cluster},
	booktitle = {Galaxy {Community} {Conference}},
	author = {Colbry, Dirk and Crusoe, Michael R. and Keen, Andy and Mason, Greg and Muffett, Jason and Scholz, Matthew and Teal, Tracy K.},
	month = jul,
	year = {2014},
	note = {00000},
}

@article{crusoe_khmer_2015,
	title = {The khmer software package: enabling efficient nucleotide sequence analysis [version 1; referees: 2 approved, 1 approved with reservations]},
	url = {http://doi.org/10.12688/f1000research.6924.1},
	doi = {10.12688/f1000research.6924.1},
	author = {Crusoe, Michael R.},
	year = {2015},
	note = {00006},
}

@book{amstutz_beyond_2015,
	title = {Beyond {Galaxy}: portable workflows and tool definitions with the {CWL}},
	url = {https://cesgo.genouest.org/resources/129},
	author = {Amstutz, Peter and Tijanić, Nebojša and Soiland-Reyes, Stian and Kern, John and Stojanovic, Luka and Pierce, Tim and Chilton, John and Mikheev, Maxim and Lampa, Samuel and Ménager, Hervé and Frazer, Scott and Malladi, Venkat S. and Crusoe, Michael R.},
	month = jul,
	year = {2015},
	note = {00000},
}

@book{crusoe_common_2016,
	title = {Common workflow language v1.0 \& how it will affect you},
	url = {http://doi.org/10.7490/F1000RESEARCH.1112726.1},
	author = {Crusoe, Michael R. and Crusoe, Michael R.},
	year = {2016},
	doi = {10.7490/F1000RESEARCH.1112726.1},
	note = {00000 },
}

@article{crusoe_channeling_2016,
	title = {Channeling {Community} {Contributions} to {Scientific} {Software}: {A} {Sprint} {Experience}},
	volume = {4},
	url = {http://dx.doi.org/10.5334/jors.96},
	doi = {10.5334/jors.96},
	journal = {Journal of Open Research Software},
	author = {Crusoe, Michael R. and Brown, C. Titus},
	month = jul,
	year = {2016},
	note = {00001},
}

@book{crusoe_common_2016-1,
	title = {Common {Workflow} {Language}, draft 3},
	url = {http://doi.org/10.6084/M9.FIGSHARE.3115156.V1},
	author = {Crusoe, Michael R. and Amstutz, Peter and Andeer, Robin and Chapman, Brad and Chilton, John and Crusoe, Michael R. and Guimerà, Roman Valls and Hernandez, Guillermo Carrasco and Ivkovic, Sinisa and Kartashov, Andrey and Kern, John and Leehr, Dan and Ménager, Hervé and Mikheev, Maxim and Pierce, Tim and Randall, Josh and Soiland-Reyes, Stian and Stojanovic, Luka and Tijanić, Nebojša},
	year = {2016},
	doi = {10.6084/M9.FIGSHARE.3115156.V1},
	note = {00008 },
}

@book{crusoe_khmer_2017,
	title = {The {Khmer} {Project} {V2}.1},
	author = {Crusoe, Michael R. and Standage, Daniel and Aliyari, Ali and Cohen, Lisa J. and Crusoe, Michael R. and Head, Tim and Irber, Luiz and Joslin, Shannon EK and Kingsley, N. B. and Murray, Kevin D. and Neches, Russell and Scott, Camille and Shean, Ryan and Steinbiss, Sascha and Sydney, Cait and Brown, C. Titus},
	year = {2017},
	note = {00000},
}

@book{crusoe_common_2017,
	title = {Common {Workflow} {Language} {User} {Guide}},
	author = {Crusoe, Michael R. and Hodges, Toby and Crusoe, Michael R.},
	year = {2017},
	note = {00000},
}

@article{alterovitz_enabling_2017,
	title = {Enabling {Precision} {Medicine} via standard communication of {NGS} provenance, analysis, and results},
	url = {https://doi.org/10.1101%2F191783},
	doi = {10.1101/191783},
	author = {Alterovitz, Gil and Dean, Dennis A. and Goble, Carole and Crusoe, Michael R. and Soiland-Reyes, Stian and Bell, Amanda and Hayes, Anais and Suresh, Anita and King, Charles Hadley S. and Taylor, Dan and Addepalli, KanakaDurga and Johanson, Elaine and Thompson, Elaine E. and Donaldson, Eric and Morizono, Hiroki and Tsang, Hsinyi and Vora, Jeet K. and Goecks, Jeremy and Yao, Jianchao and Almeida, Jonas S. and Krampis, Konstantinos and Smith, Krista and Guo, Lydia and Walderhaug, Mark and Schito, Marco and Ezewudo, Matthew and Guimera, Nuria and Walsh, Paul and Kahsay, Robel and Gottipati, Srikanth and Rodwell, Timothy C. and Bloom, Toby and Lai, Yuching and Simonyan, Vahan and Mazumder, Raja},
	month = sep,
	year = {2017},
	note = {00002},
}

@book{crusoe_cwlprov_2018,
	title = {Cwlprov - {Interoperable} {Retrospective} {Provenance} {Capture} {And} {Its} {Challenges}},
	url = {http://doi.org/10.5281/ZENODO.1208477},
	author = {Crusoe, Michael R. and Khan, Farah Zaib and Soiland-Reyes, Stian and Crusoe, Michael R. and Lonie, Andrew and Sinnott, Richard O.},
	year = {2018},
	doi = {10.5281/ZENODO.1208477},
	note = {00001 },
}

@book{crusoe_cwlprov_2018-1,
	title = {Cwlprov - {Interoperable} {Retrospective} {Provenance} {Capture} {And} {Its} {Challenges}},
	url = {http://doi.org/10.5281/ZENODO.1215611},
	author = {Crusoe, Michael R. and Khan, Farah Zaib and Soiland-Reyes, Stian and Crusoe, Michael R. and Lonie, Andrew and Sinnott, Richard O.},
	year = {2018},
	doi = {10.5281/ZENODO.1215611},
	note = {00001 },
}

@article{crusoe_recommendations_2018,
	title = {Recommendations for the packaging and containerizing of bioinformatics software [version 1; referees: awaiting peer review]},
	issn = {2046-1402},
	url = {http://doi.org/10.12688/f1000research.15140.1},
	doi = {10.12688/f1000research.15140.1},
	author = {Crusoe, Michael R. and Gruening, Bjorn and Sallou, Olivier and Moreno, Pablo and Leprevost, Felipe da Veiga and Ménager, Hervé and Søndergaard, Dan and Röst, Hannes and Sachsenberg, Timo and O'Connor, Brian and Madeira, Fábio and Angel, Victoria Dominguez Del and Crusoe, Michael R. and Varma, Susheel and Blankenberg, Daniel and Jimenez, Rafael C. and Community, BioContainers and Perez-Riverol, Yasset},
	year = {2018},
	note = {00000},
}

@inproceedings{crusoe_cwlprov_2018-2,
	title = {Cwlprov - {Interoperable} {Retrospective} {Provenance} {Capture} {And} {Its} {Challenges}},
	url = {http://doi.org/10.5281/ZENODO.1208478},
	doi = {10.5281/ZENODO.1208478},
	booktitle = {Zenodo},
	author = {Crusoe, Michael R. and Khan, Farah Zaib and Soiland-Reyes, Stian and Crusoe, Michael R. and Lonie, Andrew and Sinnott, Richard O.},
	year = {2018},
	note = {00001},
}

@article{crusoe_organizing_2018,
	title = {Organizing and running bioinformatics hackathons within {Africa}: {The} {H3ABioNet} cloud computing experience [version 1; referees: awaiting peer review]},
	issn = {2515-9321},
	url = {http://doi.org/10.12688/aasopenres.12847.1},
	doi = {10.12688/aasopenres.12847.1},
	author = {Crusoe, Michael R. and Ahmed, Azza E. and Mpangase, Phelelani T. and Panji, Sumir and Baichoo, Shakuntala and Botha, Gerrit and Fadlelmola, Faisal M. and Hazelhurst, Scott and Heusden, Peter Van and Jongeneel, C. Victor and Joubert, Fourie and Mainzer, Liudmila Sergeevna and Meintjes, Ayton and Armstrong, Don and Crusoe, Michael R. and O'connor, Brian D. and Souilmi, Yassine and Alghali, Mustafa and Aron, Shaun and Bendou, Hocine and Beste, Eugene De and Mbiyavanga, Mamana and Souiai, Oussema and Yi, Long and Zermeno, Jennie and Mulder, Nicola},
	year = {2018},
	note = {00000},
}

@book{crusoe_cwlprov:_2018,
	title = {Cwlprov: {Interoperable} {Retrospective} {Provenance} {Capture} {And} {Computational} {Analysis} {Sharing}},
	url = {http://doi.org/10.5281/zenodo.1473157},
	author = {Crusoe, Michael R. and Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O. and Lonie, Andrew and Goble, Carole and Crusoe, Michael R.},
	year = {2018},
	doi = {10.5281/zenodo.1473157},
	note = {00000 },
}

@inproceedings{crusoe_capturing_2018,
	title = {Capturing {Interoperable} {Reproducible} {Workflows} {With} {Common} {Workflow} {Language}},
	url = {http://doi.org/10.5281/ZENODO.1312622},
	doi = {10.5281/ZENODO.1312622},
	booktitle = {Zenodo},
	author = {Crusoe, Michael R. and Soiland-Reyes, Stian and Khan, Farah Zaib and Sinnott, Richard O. and Lonie, Andrew and Crusoe, Michael R. and Goble, Carole},
	year = {2018},
	note = {00000},
}

@inproceedings{crusoe_capturing_2018-1,
	title = {Capturing {Interoperable} {Reproducible} {Workflows} {With} {Common} {Workflow} {Language}},
	url = {http://doi.org/10.5281/ZENODO.1312623},
	doi = {10.5281/ZENODO.1312623},
	booktitle = {Zenodo},
	author = {Crusoe, Michael R. and Soiland-Reyes, Stian and Khan, Farah Zaib and Sinnott, Richard O. and Lonie, Andrew and Crusoe, Michael R. and Goble, Carole},
	year = {2018},
	note = {00000},
}

@book{crusoe_capturing_2018-2,
	title = {Capturing {Interoperable} {Reproducible} {Workflows} {With} {Common} {Workflow} {Language}},
	url = {http://doi.org/10.5281/zenodo.1484496},
	author = {Crusoe, Michael R. and Soiland-Reyes, Stian and Khan, Farah Zaib and Sinnott, Richard O. and Lonie, Andrew and Crusoe, Michael R. and Goble, Carole},
	year = {2018},
	doi = {10.5281/zenodo.1484496},
	note = {00000 },
}

@book{crusoe_sharing_2018,
	title = {Sharing interoperable workflow provenance: {A} review of best practices and their practical application in {CWLProv}},
	url = {http://doi.org/10.5281/zenodo.1966881},
	author = {Crusoe, Michael R. and Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O. and Lonie, Andrew and Goble, Carole and Crusoe, Michael R.},
	year = {2018},
	doi = {10.5281/zenodo.1966881},
	note = {00000 },
}

@book{crusoe_capturing_2018,
	title = {Capturing {Interoperable} {Reproducible} {Workflows} {With} {Common} {Workflow} {Language}},
	url = {http://doi.org/10.5281/zenodo.1484495},
	author = {Crusoe, Michael R. and Soiland-Reyes, Stian and Khan, Farah Zaib and Sinnott, Richard O. and Lonie, Andrew and Crusoe, Michael R. and Goble, Carole},
	year = {2018},
	doi = {10.5281/zenodo.1484495},
	note = {00000 },
}

@article{baichoo_developing_2018,
	title = {Developing reproducible bioinformatics analysis workflows for heterogeneous computing environments to support {African} genomics},
	volume = {19},
	copyright = {All rights reserved},
	number = {1},
	journal = {BMC bioinformatics},
	author = {Baichoo, Shakuntala and Souilmi, Yassine and Panji, Sumir and Botha, Gerrit and Meintjes, Ayton and Hazelhurst, Scott and Bendou, Hocine and de Beste, Eugene and Mpangase, Phelelani T and Souiai, Oussema and {others}},
	year = {2018},
	note = {00000},
	pages = {457},
}

@inproceedings{simko_search_2018,
	title = {Search for computational workflow synergies in reproducible research data analyses in particle physics and life sciences},
	copyright = {All rights reserved},
	booktitle = {2018 {IEEE} 14th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Šimko, Tibor and Cranmer, Kyle and Crusoe, Michael R and Heinrich, Lukas and Khodak, Anton and Kousidis, Dinos and Rodríguez, Diego},
	year = {2018},
	note = {00000},
	pages = {403--404},
}

@article{gruening_bioinformatics_2018,
	title = {bioinformatics software [version 1; referees: 2 approved with},
	copyright = {All rights reserved},
	author = {Gruening, Bjorn and Sallou, Olivier and Moreno, Pablo and da Veiga Leprevost, Felipe and Ménager, Hervé and Søndergaard, Dan and Röst, Hannes and Sachsenberg, Timo and O'Connor, Brian and Madeira, Fábio and {others}},
	year = {2018},
	note = {00000},
}

@article{gruening_recommendations_2018,
	title = {Recommendations for the packaging and containerizing of bioinformatics software},
	volume = {7},
	copyright = {All rights reserved},
	journal = {F1000Research},
	author = {Gruening, Bjorn and Sallou, Olivier and Moreno, Pablo and da Veiga Leprevost, Felipe and Ménager, Hervé and Søndergaard, Dan and Röst, Hannes and Sachsenberg, Timo and O'Connor, Brian and Madeira, Fábio and {others}},
	year = {2018},
	note = {00002},
}

@article{ahmed_organizing_2018,
	title = {Organizing and running bioinformatics hackathons within {Africa}: {The} {H3ABioNet} cloud computing experience},
	volume = {1},
	copyright = {All rights reserved},
	journal = {AAS Open Research},
	author = {Ahmed, Azza E and Mpangase, Phelelani T and Panji, Sumir and Baichoo, Shakuntala and Botha, Gerrit and Fadlelmola, Faisal M and Hazelhurst, Scott and Van Heusden, Peter and Jongeneel, C Victor and Joubert, Fourie and {others}},
	year = {2018},
	note = {00001},
}

@article{michael_r._crusoe_channeling_2016,
	title = {Channeling {Community} {Contributions} to {Scientific} {Software}: {A} {Sprint} {Experience}},
	volume = {4},
	copyright = {All rights reserved},
	journal = {Journal of Open Research Software},
	author = {Michael R. Crusoe, C. Titus Brown},
	year = {2016},
	note = {00001},
}

@article{crusoe_khmer_2015,
	title = {The khmer software package: enabling efficient nucleotide sequence analysis},
	volume = {4},
	copyright = {All rights reserved},
	journal = {F1000Research},
	author = {Crusoe, Michael R and Alameldin, Hussien F and Awad, Sherine and Boucher, Elmar and Caldwell, Adam and Cartwright, Reed and Charbonneau, Amanda and Constantinides, Bede and Edvenson, Greg and Fay, Scott and {others}},
	year = {2015},
	note = {00127},
}

@inproceedings{austerman_modeling_2009,
	title = {Modeling {Malaria} {Pathogenesis}: {The} {Double}-{Edged} {Sword} of {Nitric} {Oxide}},
	copyright = {All rights reserved},
	booktitle = {Arizona {State} {University} {College} of {Liberal} {Arts} and {Sciences} {Undergraduate} {Research} {Symposium}},
	author = {Austerman, R. J. and Crusoe, Michael R and Malenica, I. and Lindsay, J. I and Salmon, P. and McCaughan, M. and Farahani, T. and Kuang, Y. and Nagy, John D.},
	year = {2009},
	note = {00000},
}

@article{musselman_sid-0002_2006,
	title = {{SID}-0002 {Finger} minutiae-based biometric profile for seafarers' identity documents: {The} standard for the biometric template required by the {Convention} {No}. 185},
	copyright = {All rights reserved},
	author = {Musselman, Cynthia L. and Valencia, Valorie S. and Crusoe, Michael R.},
	year = {2006},
	note = {00000},
}

@inproceedings{crusoe_using_2005,
	title = {Using {Your} {Body} for {Authentication}: {A} {Biometrics} {Guide} for {System} {Administrators}.},
	copyright = {All rights reserved},
	booktitle = {{LISA}},
	author = {Crusoe, Michael R},
	year = {2005},
}
