
@article{sroka_formal_2010,
	series = {Special {Issue}: {Scientific} {Workflow} 2009},
	title = {A formal semantics for the {Taverna} 2 workflow model},
	volume = {76},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S0022000009001251},
	doi = {10.1016/j.jcss.2009.11.009},
	abstract = {This paper presents a formal semantics for the Taverna 2 scientific workflow system. Taverna 2 is a successor to Taverna, an open-source workflow system broadly adopted within the e-science community worldwide. The new version improves upon the existing model in two main ways: (i) by adding support for data pipelining, which in turns enables input streams of indefinite length to be processed efficiently; and (ii) by providing new extensibility points that make it possible to add new operators to the workflow model. Consistent with previous work by some of the authors, we use trace semantics to describe the effect of workflow computations, and we show how they can be used to describe the new features in the Taverna 2 model.},
	language = {en},
	number = {6},
	urldate = {2020-01-21},
	journal = {Journal of Computer and System Sciences},
	author = {Sroka, Jacek and Hidders, Jan and Missier, Paolo and Goble, Carole},
	month = sep,
	year = {2010},
	keywords = {Formal semantics, Scientific workflows, Taverna 2, Trace semantics},
	pages = {490--508},
	file = {ScienceDirect Full Text PDF:/home/michael/Zotero/storage/X7VISYM9/Sroka et al. - 2010 - A formal semantics for the Taverna 2 workflow mode.pdf:application/pdf;ScienceDirect Snapshot:/home/michael/Zotero/storage/R9Z759QT/S0022000009001251.html:text/html}
}

@article{mons_cloudy_2017,
	title = {Cloudy, increasingly {FAIR}; revisiting the {FAIR} {Data} guiding principles for the {European} {Open} {Science} {Cloud}},
	volume = {37},
	issn = {0167-5265},
	url = {http://content.iospress.com/articles/information-services-and-use/isu824},
	doi = {10.3233/ISU-170824},
	abstract = {The FAIR Data Principles propose that all scholarly output should be Findable, Accessible, Interoperable, and Reusable. As a set of guiding principles, expressing only the kinds of behaviours that researchers should expect from contemporary data reso},
	language = {en},
	number = {1},
	urldate = {2020-07-15},
	journal = {Information Services \& Use},
	author = {Mons, Barend and Neylon, Cameron and Velterop, Jan and Dumontier, Michel and da Silva Santos, Luiz Olavo Bonino and Wilkinson, Mark D.},
	month = jan,
	year = {2017},
	note = {Publisher: IOS Press},
	pages = {49--56},
	file = {Full Text PDF:/home/michael/Zotero/storage/3L5DCVE6/Mons et al. - 2017 - Cloudy, increasingly FAIR\; revisiting the FAIR Dat.pdf:application/pdf;Snapshot:/home/michael/Zotero/storage/QPDCZ37A/isu824.html:text/html}
}

@misc{eoscpilot_ewatercycle,
	title = {{EOSCpilot} {Science} {Demonstrator}: {eWaterCycle} \& {SWITCH}-{ON} - {FAIR} data for hydrology {\textbar} {EOSC} {Portal}},
	url = {https://www.eosc-portal.eu/eoscpilot-science-demonstrator-ewatercycle-switch-fair-data-hydrology},
	urldate = {2021-01-13},
	file = {EOSCpilot Science Demonstrator\: eWaterCycle & SWITCH-ON - FAIR data for hydrology | EOSC Portal:/home/michael/Zotero/storage/HXQIYU4V/eoscpilot-science-demonstrator-ewatercycle-switch-fair-data-hydrology.html:text/html}
}

@article{feitelson_repeatability_2015,
	title = {From {Repeatability} to {Reproducibility} and {Corroboration}},
	volume = {49},
	issn = {0163-5980},
	url = {https://doi.org/10.1145/2723872.2723875},
	doi = {10.1145/2723872.2723875},
	abstract = {Being able to repeat experiments is considered a hallmark of the scientific method, used to confirm or refute hypotheses and previously obtained results. But this can take many forms, from precise repetition using the original experimental artifacts, to conceptual reproduction of the main experimental idea using new artifacts. Furthermore, the conclusions from previous work can also be corroborated using a different experimental methodology altogether. In order to promote a better understanding and use of such methodologies we propose precise definitions for different terms, and suggest when and why each should be used.},
	number = {1},
	urldate = {2021-01-13},
	journal = {ACM SIGOPS Operating Systems Review},
	author = {Feitelson, Dror G.},
	month = jan,
	year = {2015},
	pages = {3--11}
}

@article{de_la_garza_desktop_2016,
	title = {From the desktop to the grid: scalable bioinformatics via workflow conversion},
	volume = {17},
	issn = {1471-2105},
	shorttitle = {From the desktop to the grid},
	url = {https://doi.org/10.1186/s12859-016-0978-9},
	doi = {10.1186/s12859-016-0978-9},
	abstract = {Reproducibility is one of the tenets of the scientific method. Scientific experiments often comprise complex data flows, selection of adequate parameters, and analysis and visualization of intermediate and end results. Breaking down the complexity of such experiments into the joint collaboration of small, repeatable, well defined tasks, each with well defined inputs, parameters, and outputs, offers the immediate benefit of identifying bottlenecks, pinpoint sections which could benefit from parallelization, among others. Workflows rest upon the notion of splitting complex work into the joint effort of several manageable tasks.},
	number = {1},
	urldate = {2020-11-11},
	journal = {BMC Bioinformatics},
	author = {de la Garza, Luis and Veit, Johannes and Szolek, Andras and Röttig, Marc and Aiche, Stephan and Gesing, Sandra and Reinert, Knut and Kohlbacher, Oliver},
	month = mar,
	year = {2016},
	pages = {127},
	file = {Snapshot:/home/michael/Zotero/storage/R9ABPQFL/s12859-016-0978-9.html:text/html;Snapshot:/home/michael/Zotero/storage/83QSB7WC/s12859-016-0978-9.html:text/html}
}

@article{tange_gnu_2011,
	title = {{GNU} {Parallel} - {The} {Command}-{Line} {Power} {Tool}},
	volume = {36},
	issn = {1044-6397},
	url = {https://www.usenix.org/system/files/login/articles/105438-Tange.pdf},
	journal = {;login: The USENIX Magazine},
	author = {Tange, O},
	month = feb,
	year = {2011},
	pages = {42--47}
}

@misc{eoscpilot-lofar,
	title = {{LOFAR} and the radio astronomy community {\textbar} {EOSC} {Portal}},
	url = {https://www.eosc-portal.eu/lofar-and-radio-astronomy-community},
	urldate = {2021-01-13},
	file = {LOFAR and the radio astronomy community | EOSC Portal:/home/michael/Zotero/storage/4MZC9PCE/lofar-and-radio-astronomy-community.html:text/html}
}

@inproceedings{babuji_parsl_2019,
	address = {New York, NY, USA},
	series = {{HPDC} '19},
	title = {Parsl: {Pervasive} {Parallel} {Programming} in {Python}},
	isbn = {978-1-4503-6670-0},
	shorttitle = {Parsl},
	url = {https://doi.org/10.1145/3307681.3325400},
	doi = {10.1145/3307681.3325400},
	abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
	urldate = {2020-12-13},
	booktitle = {Proceedings of the 28th {International} {Symposium} on {High}-{Performance} {Parallel} and {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
	month = jun,
	year = {2019},
	keywords = {parallel programming, parsl, python},
	pages = {25--36}
}

@techreport{the_austin_group_posix1-2008_2008,
	title = {{POSIX}.1-2008 ({IEEE} {Std} 1003.1™-2008 and {The} {Open} {Group} {Technical} {Standard} {Base} {Specifications}, {Issue} 7)},
	url = {https://pubs.opengroup.org/onlinepubs/9699919799.2008edition/},
	author = {The Austin Group},
	year = {2008}
}

@article{tejedor_pycompss_2017,
	title = {{PyCOMPSs}: {Parallel} computational workflows in {Python}},
	volume = {31},
	issn = {1094-3420},
	shorttitle = {{PyCOMPSs}},
	url = {https://doi.org/10.1177/1094342015594678},
	doi = {10.1177/1094342015594678},
	abstract = {The use of the Python programming language for scientific computing has been gaining momentum in the last years. The fact that it is compact and readable and its complete set of scientific libraries are two important characteristics that favour its adoption. Nevertheless, Python still lacks a solution for easily parallelizing generic scripts on distributed infrastructures, since the current alternatives mostly require the use of APIs for message passing or are restricted to embarrassingly parallel computations. In that sense, this paper presents PyCOMPSs, a framework that facilitates the development of parallel computational workflows in Python. In this approach, the user programs her script in a sequential fashion and decorates the functions to be run as asynchronous parallel tasks. A runtime system is in charge of exploiting the inherent concurrency of the script, detecting the data dependencies between tasks and spawning them to the available resources. Furthermore, we show how this programming model can be built on top of a Big Data storage architecture, where the data stored in the backend is abstracted and accessed from the application in the form of persistent objects.},
	language = {en},
	number = {1},
	urldate = {2020-12-13},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Tejedor, Enric and Becerra, Yolanda and Alomar, Guillem and Queralt, Anna and Badia, Rosa M and Torres, Jordi and Cortes, Toni and Labarta, Jesús},
	month = jan,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Python, Big Data storage, parallel programming models, Scientic computing},
	pages = {66--82},
	file = {SAGE PDF Full Text:/home/michael/Zotero/storage/SZ8YZZ5Z/Tejedor et al. - 2017 - PyCOMPSs Parallel computational workflows in Pyth.pdf:application/pdf}
}

@article{ivie_reproducibility_2018,
	title = {Reproducibility in {Scientific} {Computing}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3186266},
	doi = {10.1145/3186266},
	abstract = {Reproducibility is widely considered to be an essential requirement of the scientific process. However, a number of serious concerns have been raised recently, questioning whether today’s computational work is adequately reproducible. In principle, it should be possible to specify a computation to sufficient detail that anyone should be able to reproduce it exactly. But in practice, there are fundamental, technical, and social barriers to doing so. The many objectives and meanings of reproducibility are discussed within the context of scientific computing. Technical barriers to reproducibility are described, extant approaches surveyed, and open areas of research are identified.},
	number = {3},
	urldate = {2021-01-13},
	journal = {ACM Computing Surveys},
	author = {Ivie, Peter and Thain, Douglas},
	month = jul,
	year = {2018},
	keywords = {workflow, scientific workflows, workflows, Reproducibility, scientific workflow, computational science, replicability, reproducible, scientific computing},
	pages = {63:1--63:36}
}

@article{cuevas-vicenttin_scientific_2012,
	title = {Scientific {Workflows} and {Provenance}: {Introduction} and {Research} {Opportunities}},
	volume = {12},
	issn = {1610-1995},
	shorttitle = {Scientific {Workflows} and {Provenance}},
	url = {https://doi.org/10.1007/s13222-012-0100-z},
	doi = {10.1007/s13222-012-0100-z},
	abstract = {Scientific workflows are becoming increasingly popular for compute-intensive and data-intensive scientific applications. The vision and promise of scientific workflows includes rapid, easy workflow design, reuse, scalable execution, and other advantages, e.g., to facilitate “reproducible science” through provenance (e.g., data lineage) support. However, as described in the paper, important research challenges remain. While the database community has studied (business) workflow technologies extensively in the past, most current work in scientific workflows seems to be done outside of the database community, e.g., by practitioners and researchers in the computational sciences and eScience. We provide a brief introduction to scientific workflows and provenance, and identify areas and problems that suggest new opportunities for database research.},
	language = {en},
	number = {3},
	urldate = {2020-07-03},
	journal = {Datenbank-Spektrum},
	author = {Cuevas-Vicenttín, Víctor and Dey, Saumen and Köhler, Sven and Riddle, Sean and Ludäscher, Bertram},
	month = nov,
	year = {2012},
	note = {ASAP},
	pages = {193--203},
	file = {Springer Full Text PDF:/home/michael/Zotero/storage/E4J9A7L6/Cuevas-Vicenttín et al. - 2012 - Scientific Workflows and Provenance Introduction .pdf:application/pdf}
}

@incollection{hutchison_taverna_2010,
	address = {Berlin, Heidelberg},
	title = {Taverna, {Reloaded}},
	volume = {6187},
	isbn = {978-3-642-13817-1 978-3-642-13818-8},
	url = {http://link.springer.com/10.1007/978-3-642-13818-8_33},
	abstract = {The Taverna workﬂow management system is an open source project with a history of widespread adoption within multiple experimental science communities, and a longterm ambition of eﬀectively supporting the evolving need of those communities for complex, data-intensive, service-based experimental pipelines. This paper describes how the recently overhauled technical architecture of Taverna addresses issues of eﬃciency, scalability, and extensibility, and presents performance results based on a collection of synthetic workﬂows, as well as a concrete case study involving a production workﬂow in the area of cancer research.},
	language = {en},
	urldate = {2020-01-21},
	booktitle = {Scientific and {Statistical} {Database} {Management}},
	publisher = {Springer Berlin Heidelberg},
	author = {Missier, Paolo and Soiland-Reyes, Stian and Owen, Stuart and Tan, Wei and Nenadic, Alexandra and Dunlop, Ian and Williams, Alan and Oinn, Tom and Goble, Carole},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gertz, Michael and Ludäscher, Bertram},
	year = {2010},
	doi = {10.1007/978-3-642-13818-8_33},
	pages = {471--481},
	file = {Missier et al. - 2010 - Taverna, Reloaded.pdf:/home/michael/Zotero/storage/588GXYZV/Missier et al. - 2010 - Taverna, Reloaded.pdf:application/pdf}
}

@article{kunze_bagit_2018,
	title = {The {BagIt} {File} {Packaging} {Format} ({V1}.0)},
	issn = {2070-1721},
	url = {https://www.rfc-editor.org/info/rfc8493},
	doi = {10.17487/RFC8493},
	urldate = {2021-01-13},
	author = {Kunze, J. and Littman, J. and Madden, E. and Scancella, J. and Adams, C.},
	year = {2018},
	note = {Number: RFC 8493},
	file = {Snapshot:/home/michael/Zotero/storage/IR55Z8XV/rfc8493.html:text/html}
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	language = {en},
	number = {1},
	urldate = {2020-07-15},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {160018},
	file = {Full Text PDF:/home/michael/Zotero/storage/FFFIJ8ZX/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf:application/pdf;Snapshot:/home/michael/Zotero/storage/KGTK7SP3/sdata201618.html:text/html}
}

@article{deelman_future_2017,
	title = {The future of scientific workflows},
	volume = {32},
	issn = {1094-3420},
	url = {http://journals.sagepub.com/doi/10.1177/1094342017704893},
	doi = {10.1177/1094342017704893},
	abstract = {Today’s computational, experimental, and observational sciences rely on computations that involve many related tasks. The success of a scientific mission often hinges on the computer automation of these workflows. In April 2015, the US Department of Energy (DOE) invited a diverse group of domain and computer scientists from national laboratories supported by the Office of Science, the National Nuclear Security Administration, from industry, and from academia to review the workflow requirements of DOE’s science and national security missions, to assess the current state of the art in science workflows, to understand the impact of emerging extreme-scale computing systems on those workflows, and to develop requirements for automated workflow management in future and existing environments. This article is a summary of the opinions of over 50 leading researchers attending this workshop. We highlight use cases, computing systems, workflow needs and conclude by summarizing the remaining challenges this community...},
	number = {1},
	urldate = {2020-01-20},
	journal = {The international journal of high performance computing applications},
	author = {Deelman, Ewa and Peterka, Tom and Altintas, Ilkay and Carothers, Christopher D and Kleese van Dam, Kerstin and Moreland, Kenneth and Parashar, Manish and Ramakrishnan, Lavanya and Taufer, Michela and Vetter, Jeffrey},
	month = apr,
	year = {2017},
	pages = {109434201770489},
	file = {Full Text:/home/michael/Zotero/storage/PTT7AN6V/Deelman et al. - 2017 - The future of scientific workflows.pdf:application/pdf;Snapshot:/home/michael/Zotero/storage/HXZNENXJ/1094342017704893.html:text/html}
}

@inproceedings{missier_w3c_2013,
	address = {New York, NY, USA},
	series = {{EDBT} '13},
	title = {The {W3C} {PROV} family of specifications for modelling provenance metadata},
	isbn = {978-1-4503-1597-5},
	url = {https://doi.org/10.1145/2452376.2452478},
	doi = {10.1145/2452376.2452478},
	abstract = {Provenance, a form of structured metadata designed to record the origin or source of information, can be instrumental in deciding whether information is to be trusted, how it can be integrated with other diverse information sources, and how to establish attribution of information to authors throughout its history. The PROV set of specifications, produced by the World Wide Web Consortium (W3C), is designed to promote the publication of provenance information on the Web, and offers a basis for interoperability across diverse provenance management systems. The PROV provenance model is deliberately generic and domain-agnostic, but extension mechanisms are available and can be exploited for modelling specific domains. This tutorial provides an account of these specifications. Starting from intuitive and informal examples that present idiomatic provenance patterns, it progressively introduces the relational model of provenance along with the constraints model for validation of provenance documents, and concludes with example applications that show the extension points in use.},
	urldate = {2021-01-13},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Extending} {Database} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Missier, Paolo and Belhajjame, Khalid and Cheney, James},
	month = mar,
	year = {2013},
	pages = {773--776}
}

@article{belhajjame_using_2015,
	title = {Using a suite of ontologies for preserving workflow-centric research objects},
	volume = {32},
	issn = {1570-8268},
	url = {http://www.sciencedirect.com/science/article/pii/S1570826815000049},
	doi = {10.1016/j.websem.2015.01.003},
	abstract = {Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions. In this article, we present a novel approach to the preservation of scientific workflows through the application of research objects—aggregations of data and metadata that enrich the workflow specifications. Our approach is realised as a suite of ontologies that support the creation of workflow-centric research objects. Their design was guided by requirements elicited from previous empirical analyses of workflow decay and repair. The ontologies developed make use of and extend existing well known ontologies, namely the Object Reuse and Exchange (ORE) vocabulary, the Annotation Ontology (AO) and the W3C PROV ontology (PROVO). We illustrate the application of the ontologies for building Workflow Research Objects with a case-study that investigates Huntington’s disease, performed in collaboration with a team from the Leiden University Medial Centre (HG-LUMC). Finally we present a number of tools developed for creating and managing workflow-centric research objects.},
	language = {en},
	urldate = {2021-01-13},
	journal = {Journal of Web Semantics},
	author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Gamble, Matthew and Hettne, Kristina and Palma, Raul and Mina, Eleni and Corcho, Oscar and Gómez-Pérez, José Manuel and Bechhofer, Sean and Klyne, Graham and Goble, Carole},
	month = may,
	year = {2015},
	keywords = {Annotation, Ontologies, Scientific workflow, Preservation, Provenance, Research object},
	pages = {16--42},
	file = {ScienceDirect Snapshot:/home/michael/Zotero/storage/PMETXNEM/S1570826815000049.html:text/html}
}

@article{brandies_ten_2021,
	title = {Ten simple rules for getting started with command-line bioinformatics},
	volume = {17},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008645},
	doi = {10.1371/journal.pcbi.1008645},
	language = {en},
	number = {2},
	urldate = {2021-03-02},
	journal = {PLOS Computational Biology},
	author = {Brandies, Parice A. and Hogg, Carolyn J.},
	month = feb,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Bioinformatics, Cloud computing, Software tools, Computational pipelines, Computer software, Operating systems, Programming languages, Source code},
	pages = {e1008645},
	file = {Snapshot:/home/michael/Zotero/storage/BQ2UM2ZJ/article.html:text/html}
}

@article{seemann_ten_2013,
	title = {Ten recommendations for creating usable bioinformatics command line software},
	volume = {2},
	issn = {2047-217X},
	url = {https://doi.org/10.1186/2047-217X-2-15},
	doi = {10.1186/2047-217X-2-15},
	abstract = {Bioinformatics software varies greatly in quality. In terms of usability, the command line interface is the first experience a user will have of a tool. Unfortunately, this is often also the last time a tool will be used. Here I present ten recommendations for command line software author’s tools to follow, which I believe would greatly improve the uptake and usability of their products, waste less user’s time, and improve the quality of scientific analyses.},
	number = {2047-217X-2-15},
	urldate = {2021-03-02},
	journal = {GigaScience},
	author = {Seemann, Torsten},
	month = dec,
	year = {2013},
	file = {Snapshot:/home/michael/Zotero/storage/Q4K4LXU9/2656133.html:text/html}
}

@article{georgeson_bionitio_2019,
	title = {Bionitio: demonstrating and facilitating best practices for bioinformatics command-line software},
	volume = {8},
	issn = {2047-217X},
	shorttitle = {Bionitio},
	url = {https://doi.org/10.1093/gigascience/giz109},
	doi = {10.1093/gigascience/giz109},
	abstract = {Bioinformatics software tools are often created ad hoc, frequently by people without extensive training in software development. In particular, for beginners, the barrier to entry in bioinformatics software development is high, especially if they want to adopt good programming practices. Even experienced developers do not always follow best practices. This results in the proliferation of poorer-quality bioinformatics software, leading to limited scalability and inefficient use of resources; lack of reproducibility, usability, adaptability, and interoperability; and erroneous or inaccurate results.We have developed Bionitio, a tool that automates the process of starting new bioinformatics software projects following recommended best practices. With a single command, the user can create a new well-structured project in 1 of 12 programming languages. The resulting software is functional, carrying out a prototypical bioinformatics task, and thus serves as both a working example and a template for building new tools. Key features include command-line argument parsing, error handling, progress logging, defined exit status values, a test suite, a version number, standardized building and packaging, user documentation, code documentation, a standard open source software license, software revision control, and containerization.Bionitio serves as a learning aid for beginner-to-intermediate bioinformatics programmers and provides an excellent starting point for new projects. This helps developers adopt good programming practices from the beginning of a project and encourages high-quality tools to be developed more rapidly. This also benefits users because tools are more easily installed and consistent in their usage. Bionitio is released as open source software under the MIT License and is available at https://github.com/bionitio-team/bionitio.},
	number = {giz109},
	urldate = {2021-03-02},
	journal = {GigaScience},
	author = {Georgeson, Peter and Syme, Anna and Sloggett, Clare and Chung, Jessica and Dashnow, Harriet and Milton, Michael and Lonsdale, Andrew and Powell, David and Seemann, Torsten and Pope, Bernard},
	month = sep,
	year = {2019},
	file = {Snapshot:/home/michael/Zotero/storage/Z8LRIDW8/5572530.html:text/html}
}

@article{berthold_knime_2009,
	title = {{KNIME} - the {Konstanz} information miner: version 2.0 and beyond},
	volume = {11},
	issn = {1931-0145},
	shorttitle = {{KNIME} - the {Konstanz} information miner},
	url = {https://doi.org/10.1145/1656274.1656280},
	doi = {10.1145/1656274.1656280},
	abstract = {The Konstanz Information Miner is a modular environment, which enables easy visual assembly and interactive execution of a data pipeline. It is designed as a teaching, research and collaboration platform, which enables simple integration of new algorithms and tools as well as data manipulation or visualization methods in the form of new modules or nodes. In this paper we describe some of the design aspects of the underlying architecture, briey sketch how new nodes can be incorporated, and highlight some of the new features of version 2.0.},
	number = {1},
	urldate = {2021-03-02},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Berthold, Michael R. and Cebron, Nicolas and Dill, Fabian and Gabriel, Thomas R. and Kötter, Tobias and Meinl, Thorsten and Ohl, Peter and Thiel, Kilian and Wiswedel, Bernd},
	month = nov,
	year = {2009},
	pages = {26--31},
	file = {Full Text PDF:/home/michael/Zotero/storage/NBEMIGHB/Berthold et al. - 2009 - KNIME - the Konstanz information miner version 2..pdf:application/pdf}
}

@article{perkel_workflow_2019,
	title = {Workflow systems turn raw data into scientific knowledge},
	volume = {573},
	copyright = {2019 Nature},
	url = {http://www.nature.com/articles/d41586-019-02619-z},
	doi = {10.1038/d41586-019-02619-z},
	abstract = {How workflow tools can make your computational methods portable, maintainable, reproducible and shareable.},
	language = {en},
	urldate = {2019-09-03},
	journal = {Nature},
	author = {Perkel, Jeffrey M.},
	month = sep,
	year = {2019},
	pages = {149--150}
}

@article{afgan_galaxy_2018,
	title = {The {Galaxy} platform for accessible, reproducible and collaborative biomedical analyses: 2018 update},
	volume = {46},
	issn = {0305-1048},
	shorttitle = {The {Galaxy} platform for accessible, reproducible and collaborative biomedical analyses},
	url = {https://doi.org/10.1093/nar/gky379},
	doi = {10.1093/nar/gky379},
	abstract = {Galaxy (homepage: https://galaxyproject.org, main public server: https://usegalaxy.org) is a web-based scientific analysis platform used by tens of thousands of scientists across the world to analyze large biomedical datasets such as those found in genomics, proteomics, metabolomics and imaging. Started in 2005, Galaxy continues to focus on three key challenges of data-driven biomedical science: making analyses accessible to all researchers, ensuring analyses are completely reproducible, and making it simple to communicate analyses so that they can be reused and extended. During the last two years, the Galaxy team and the open-source community around Galaxy have made substantial improvements to Galaxy's core framework, user interface, tools, and training materials. Framework and user interface improvements now enable Galaxy to be used for analyzing tens of thousands of datasets, and \&gt;5500 tools are now available from the Galaxy ToolShed. The Galaxy community has led an effort to create numerous high-quality tutorials focused on common types of genomic analyses. The Galaxy developer and user communities continue to grow and be integral to Galaxy's development. The number of Galaxy public servers, developers contributing to the Galaxy framework and its tools, and users of the main Galaxy server have all increased substantially.},
	number = {W1},
	urldate = {2021-03-02},
	journal = {Nucleic Acids Research},
	author = {Afgan, Enis and Baker, Dannon and Batut, Bérénice and van den Beek, Marius and Bouvier, Dave and Čech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Grüning, Björn A and Guerler, Aysam and Hillman-Jackson, Jennifer and Hiltemann, Saskia and Jalili, Vahid and Rasche, Helena and Soranzo, Nicola and Goecks, Jeremy and Taylor, James and Nekrutenko, Anton and Blankenberg, Daniel},
	month = jul,
	year = {2018},
	pages = {W537--W544},
	file = {Snapshot:/home/michael/Zotero/storage/9TN9XNR3/5001157.html:text/html}
}

@article{deelman_pegasus_2015,
	title = {Pegasus, a workflow management system for science automation},
	volume = {46},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X14002015},
	doi = {10.1016/j.future.2014.10.008},
	abstract = {Modern science often requires the execution of large-scale, multi-stage simulation and data analysis pipelines to enable the study of complex systems. The amount of computation and data involved in these pipelines requires scalable workflow management systems that are able to reliably and efficiently coordinate and automate data movement and task execution on distributed computational resources: campus clusters, national cyberinfrastructures, and commercial and academic clouds. This paper describes the design, development and evolution of the Pegasus Workflow Management System, which maps abstract workflow descriptions onto distributed computing infrastructures. Pegasus has been used for more than twelve years by scientists in a wide variety of domains, including astronomy, seismology, bioinformatics, physics and others. This paper provides an integrated view of the Pegasus system, showing its capabilities that have been developed over time in response to application needs and to the evolution of the scientific computing platforms. The paper describes how Pegasus achieves reliable, scalable workflow execution across a wide variety of computing infrastructures.},
	language = {en},
	urldate = {2021-03-02},
	journal = {Future Generation Computer Systems},
	author = {Deelman, Ewa and Vahi, Karan and Juve, Gideon and Rynge, Mats and Callaghan, Scott and Maechling, Philip J. and Mayani, Rajiv and Chen, Weiwei and Ferreira da Silva, Rafael and Livny, Miron and Wenger, Kent},
	month = may,
	year = {2015},
	keywords = {Scientific workflows, Workflow management system, Pegasus},
	pages = {17--35},
	file = {ScienceDirect Full Text PDF:/home/michael/Zotero/storage/KW6CA2L7/Deelman et al. - 2015 - Pegasus, a workflow management system for science .pdf:application/pdf;ScienceDirect Snapshot:/home/michael/Zotero/storage/H9L4LA59/S0167739X14002015.html:text/html}
}

@incollection{couvares_workflow_2007,
	address = {London},
	title = {Workflow {Management} in {Condor}},
	isbn = {978-1-84628-757-2},
	url = {https://doi.org/10.1007/978-1-84628-757-2_22},
	abstract = {The Condor project began in 1988 and has evolved into a feature-rich batch system that targets high-throughput computing; that is, Condor ([262], [414]) focuses on providing reliable access to computing over long periods of time instead of highly tuned, high-performance computing for short periods of time or a small number of applications.},
	language = {en},
	urldate = {2021-03-02},
	booktitle = {Workflows for e-{Science}: {Scientific} {Workflows} for {Grids}},
	publisher = {Springer},
	author = {Couvares, Peter and Kosar, Tevfik and Roy, Alain and Weber, Jeff and Wenger, Kent},
	editor = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
	year = {2007},
	doi = {10.1007/978-1-84628-757-2_22},
	keywords = {Basic Local Alignment Search Tool, Batch System, Data Placement, Grid Environment, State Diagram},
	pages = {357--375},
	file = {Springer Full Text PDF:/home/michael/Zotero/storage/XA9P3NNC/Couvares et al. - 2007 - Workflow Management in Condor.pdf:application/pdf}
}
